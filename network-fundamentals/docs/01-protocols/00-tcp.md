# TCP（Transmission Control Protocol）

> TCPは信頼性のある通信を実現するプロトコル。3-wayハンドシェイク、シーケンス番号、フロー制御、輻輳制御の仕組みを理解し、なぜWebの通信基盤なのかを学ぶ。

## この章で学ぶこと

- [ ] TCPの3-wayハンドシェイクを理解する
- [ ] フロー制御と輻輳制御の仕組みを把握する
- [ ] TCPのヘッダー構造と主要フラグを学ぶ

---

## 1. TCPの基本特性

```
TCP = コネクション指向の信頼性あるプロトコル

特性:
  ✓ 信頼性（Reliability）: データの到達を保証
  ✓ 順序保証（Ordering）: 送信順に受信
  ✓ エラー検出: チェックサムで破損検知
  ✓ フロー制御: 受信側の処理能力に合わせる
  ✓ 輻輳制御: ネットワークの混雑を回避

  ✗ オーバーヘッドが大きい（ヘッダー20バイト〜）
  ✗ 接続確立に時間がかかる（1.5 RTT）
  ✗ Head-of-Line Blocking

用途:
  HTTP/HTTPS, FTP, SMTP, SSH, データベース接続
  → 「データを確実に届けたい」ときに使う
```

---

## 2. 3-wayハンドシェイク

```
TCP接続の確立:

  クライアント                サーバー
       │                        │
       │── SYN (seq=100) ──→   │  ① 接続要求
       │                        │
       │←── SYN-ACK ──────    │  ② 接続許可 + 応答
       │   (seq=300, ack=101)   │
       │                        │
       │── ACK (ack=301) ──→   │  ③ 確認応答
       │                        │
       │  ← 接続確立 →          │

  ①SYN: 「接続したい。seq=100から始めるよ」
  ②SYN-ACK: 「OK。101を待ってるよ。こっちはseq=300から」
  ③ACK: 「了解。301を待つよ」

  所要時間: 1.5 RTT（Round Trip Time）
  → 東京-US間: 約150ms（RTT 100ms × 1.5）

なぜ3-wayか:
  → 2-wayだと古いSYNパケットで偽の接続ができてしまう
  → 3-wayで双方のシーケンス番号を確認し、確実に同期
```

---

## 3. データ転送

```
シーケンス番号とACK:

  クライアント                    サーバー
       │                            │
       │── データ (seq=101, 100B) ──→│  データ送信
       │                            │
       │←── ACK (ack=201) ─────── │  「201バイト目を待ってる」
       │                            │
       │── データ (seq=201, 100B) ──→│  次のデータ
       │                            │
       │←── ACK (ack=301) ─────── │
       │                            │

  seq = このデータの先頭バイト番号
  ack = 次に期待するバイト番号

再送メカニズム:
  タイムアウト再送:
    → ACKが一定時間内に届かない → データを再送
    → RTO（Retransmission Timeout）= RTTの推定値に基づく

  高速再送:
    → 同じACKが3回連続（Duplicate ACK ×3）
    → タイムアウトを待たず即座に再送

  パケットロスの検出:
  クライアント                    サーバー
       │── seq=1 ──→              │  ✓ 受信
       │── seq=2 ──→   ✗（ロス） │
       │── seq=3 ──→              │  ack=2（2が欲しい）
       │── seq=4 ──→              │  ack=2（2が欲しい）← 重複ACK
       │── seq=5 ──→              │  ack=2（2が欲しい）← 重複ACK×3
       │── seq=2 ──→（再送）       │  ✓ ack=6（全部届いた）
```

---

## 4. フロー制御（Flow Control）

```
フロー制御 = 受信側のバッファ溢れを防ぐ

スライディングウィンドウ:
  受信側がウィンドウサイズ（受信可能バイト数）を通知

  受信バッファ:
  ┌──────────────────────────────────────────────┐
  │■■■■■■■■■■│□□□□□□□□□□│                      │
  │処理済み      │受信済み      │空き（ウィンドウ）│
  └──────────────────────────────────────────────┘
                                 ← rwnd = 16KB →

  ウィンドウの変化:
  1. 初期: rwnd = 64KB → 「64KBまで送ってOK」
  2. 受信: rwnd = 32KB → 「残り32KB分OK」（バッファが埋まった）
  3. 処理: rwnd = 64KB → 「処理完了、また64KB OK」

  ゼロウィンドウ:
  → rwnd = 0 になると送信停止
  → 送信側はZero Window Probeを定期的に送信
  → 受信側がバッファを空けたらWindow Updateを通知
```

---

## 5. 輻輳制御（Congestion Control）

```
輻輳制御 = ネットワーク全体の混雑を回避

  フロー制御: 受信側の能力に合わせる（エンドツーエンド）
  輻輳制御:   ネットワークの状態に合わせる（ネットワーク全体）

4つのフェーズ:

  ① スロースタート（Slow Start）:
     cwnd = 1 MSS → 2 → 4 → 8 → 16 ...
     → ACKごとにcwndを倍増（指数的増加）
     → ssthresh（閾値）に達するまで

  ② 輻輳回避（Congestion Avoidance）:
     cwnd = 16 → 17 → 18 → 19 ...
     → RTTごとにcwndを1 MSS増加（線形増加）

  ③ 高速再送（Fast Retransmit）:
     → 重複ACK ×3 で即座に再送

  ④ 高速回復（Fast Recovery）:
     → ssthresh = cwnd / 2
     → cwnd = ssthresh + 3
     → 輻輳回避フェーズに移行

  cwnd の変化:
  cwnd
   ↑
   │        ②線形増加
   │       /        ＼ パケットロス
   │      /          ↓ ssthresh = cwnd/2
   │     /    ②線形  /
   │    /    増加   /
   │   / ①        /
   │  / 指数     /
   │ / 増加    /
   │/________/______→ 時間

主な輻輳制御アルゴリズム:
  Reno:    古典的、高速再送+回復
  CUBIC:   Linux標準、cwndを3次関数で制御
  BBR:     Google開発、帯域×遅延を最大化
           → YouTube, Google Cloudで使用
```

---

## 6. TCP接続の切断（4-way Handshake）

```
TCP切断:

  クライアント                サーバー
       │                        │
       │── FIN ──→              │  ① 「送信完了」
       │                        │
       │←── ACK ────            │  ② 「了解」
       │                        │
       │        （サーバーの残りデータ送信）
       │                        │
       │←── FIN ────            │  ③ 「こっちも送信完了」
       │                        │
       │── ACK ──→              │  ④ 「了解」
       │                        │
       │  TIME_WAIT             │
       │  (2 × MSL)            │
       │                        │

  TIME_WAIT:
  → 最後のACKが失われた場合に備える
  → MSL（Maximum Segment Lifetime）= 通常60秒
  → TIME_WAIT = 120秒
  → 同じポートをすぐに再利用できない（SO_REUSEADDR で回避）

  問題: 大量のTIME_WAITが蓄積
  → 短寿命接続が多いサーバーで問題になる
  → 対策: Keep-Alive, 接続プーリング, SO_REUSEADDR
```

---

## 7. TCPヘッダー

```
TCP ヘッダー構造（20バイト〜60バイト）:

  0                   1                   2                   3
  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
  ┌─────────────────────────┬─────────────────────────┐
  │     送信元ポート (16)    │     宛先ポート (16)      │
  ├─────────────────────────────────────────────────────┤
  │                シーケンス番号 (32)                    │
  ├─────────────────────────────────────────────────────┤
  │                 ACK番号 (32)                          │
  ├────┬──────┬─┬─┬─┬─┬─┬─┬─────────────────────────┤
  │オフ│予約  │U│A│P│R│S│F│   ウィンドウサイズ (16)  │
  │(4) │(6)   │R│C│S│S│Y│I│                          │
  │    │      │G│K│H│T│N│N│                          │
  ├─────────────────────────┬─────────────────────────┤
  │   チェックサム (16)      │   緊急ポインタ (16)      │
  ├─────────────────────────┴─────────────────────────┤
  │              オプション（0〜40バイト）               │
  └─────────────────────────────────────────────────────┘

  主要フラグ:
  SYN: 接続開始
  ACK: 確認応答
  FIN: 接続終了
  RST: 接続リセット（異常終了）
  PSH: バッファリングせず即座に配信
  URG: 緊急データあり

  重要なオプション:
  MSS: Maximum Segment Size（通常1460バイト）
  Window Scale: ウィンドウサイズの拡張（最大1GB）
  SACK: Selective ACK（部分的な再送を可能に）
  Timestamp: RTT測定用
```

---

## まとめ

| 概念 | ポイント |
|------|---------|
| 3-way Handshake | SYN → SYN-ACK → ACK で接続確立 |
| シーケンス番号 | バイト単位でデータを管理 |
| フロー制御 | rwndで受信側のバッファを保護 |
| 輻輳制御 | cwndでネットワーク混雑を回避 |
| 切断 | 4-way Handshake + TIME_WAIT |

---

## 次に読むべきガイド
→ [[01-udp.md]] — UDP

---

## 参考文献
1. RFC 9293. "Transmission Control Protocol (TCP)." IETF, 2022.
2. RFC 5681. "TCP Congestion Control." IETF, 2009.
3. Cardwell, N. "BBR: Congestion-Based Congestion Control." ACM Queue, 2016.
