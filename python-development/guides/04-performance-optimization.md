# âš¡ Python Performance & Optimization Guide

> **ç›®çš„**: Pythonã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã®å®Ÿè·µçš„ãªæœ€é©åŒ–æ‰‹æ³•ã€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æŠ€è¡“ã‚’ç¿’å¾—ã™ã‚‹

## ğŸ“š ç›®æ¬¡

1. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š](#ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š)
2. [ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°](#ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°)
3. [ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®æœ€é©åŒ–](#ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®æœ€é©åŒ–)
4. [ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ€é©åŒ–](#ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ€é©åŒ–)
5. [ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–](#ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–)
6. [ä¸¦åˆ—å‡¦ç†ãƒ»éåŒæœŸå‡¦ç†](#ä¸¦åˆ—å‡¦ç†éåŒæœŸå‡¦ç†)
7. [NumPy/Pandas æœ€é©åŒ–](#numpypandas-æœ€é©åŒ–)
8. [ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥](#ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥)
9. [ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–](#ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–)
10. [Cythonãƒ»JIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«](#cythonjit-ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«)
11. [å®Ÿè·µçš„ãªæœ€é©åŒ–äº‹ä¾‹](#å®Ÿè·µçš„ãªæœ€é©åŒ–äº‹ä¾‹)

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š

### time ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

**åŸºæœ¬çš„ãªæ™‚é–“è¨ˆæ¸¬**:
```python
import time

# é–¢æ•°ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆæ¸¬
start = time.time()
result = some_function()
end = time.time()
print(f"Execution time: {end - start:.4f} seconds")

# ã‚ˆã‚Šç²¾å¯†ãªè¨ˆæ¸¬ï¼ˆtime.perf_counterï¼‰
start = time.perf_counter()
result = some_function()
end = time.perf_counter()
print(f"Execution time: {end - start:.6f} seconds")
```

**ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§è¨ˆæ¸¬**:
```python
import time
from functools import wraps
from typing import Callable, Any


def timeit(func: Callable) -> Callable:
    """é–¢æ•°ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆæ¸¬ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    @wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        start = time.perf_counter()
        result = func(*args, **kwargs)
        end = time.perf_counter()
        print(f"{func.__name__} took {end - start:.6f} seconds")
        return result
    return wrapper


@timeit
def process_data(data: list[int]) -> int:
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°"""
    return sum(x ** 2 for x in data)


# ä½¿ç”¨ä¾‹
result = process_data(list(range(1000000)))
# process_data took 0.234567 seconds
```

### timeit ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

**ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**:
```python
import timeit

# å˜ç´”ãªè¨ˆæ¸¬
execution_time = timeit.timeit(
    stmt='sum(range(100))',
    number=10000
)
print(f"Time: {execution_time:.6f} seconds")

# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚³ãƒ¼ãƒ‰ä»˜ã
execution_time = timeit.timeit(
    stmt='result = [x ** 2 for x in data]',
    setup='data = list(range(1000))',
    number=10000
)
print(f"Time: {execution_time:.6f} seconds")

# é–¢æ•°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
def my_function():
    return sum(x ** 2 for x in range(1000))

execution_time = timeit.timeit(
    stmt='my_function()',
    globals=globals(),
    number=10000
)
print(f"Time: {execution_time:.6f} seconds")
```

**è¤‡æ•°ã®å®Ÿè£…ã‚’æ¯”è¼ƒ**:
```python
import timeit

def compare_implementations():
    """è¤‡æ•°ã®å®Ÿè£…ã‚’æ¯”è¼ƒ"""

    # ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜
    time1 = timeit.timeit(
        stmt='[x ** 2 for x in range(1000)]',
        number=10000
    )

    # map + lambda
    time2 = timeit.timeit(
        stmt='list(map(lambda x: x ** 2, range(1000)))',
        number=10000
    )

    # for ãƒ«ãƒ¼ãƒ—
    time3 = timeit.timeit(
        stmt='''
result = []
for x in range(1000):
    result.append(x ** 2)
''',
        number=10000
    )

    print(f"List comprehension: {time1:.6f}s")
    print(f"Map + lambda:       {time2:.6f}s")
    print(f"For loop:           {time3:.6f}s")
    print(f"Fastest: List comprehension ({time1:.6f}s)")


compare_implementations()
# List comprehension: 0.456789s  â† æœ€é€Ÿ
# Map + lambda:       0.567890s
# For loop:           0.678901s
```

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

```python
import time
from typing import Callable, Any
from dataclasses import dataclass
from statistics import mean, stdev


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""
    name: str
    mean_time: float
    std_dev: float
    min_time: float
    max_time: float
    iterations: int

    def __str__(self) -> str:
        return (
            f"{self.name}:\n"
            f"  Mean: {self.mean_time:.6f}s\n"
            f"  Std Dev: {self.std_dev:.6f}s\n"
            f"  Min: {self.min_time:.6f}s\n"
            f"  Max: {self.max_time:.6f}s\n"
            f"  Iterations: {self.iterations}"
        )


def benchmark(
    func: Callable,
    *args: Any,
    iterations: int = 100,
    warmup: int = 10,
    **kwargs: Any
) -> BenchmarkResult:
    """é–¢æ•°ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""

    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆJIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãªã©ã®ãŸã‚ï¼‰
    for _ in range(warmup):
        func(*args, **kwargs)

    # è¨ˆæ¸¬
    times = []
    for _ in range(iterations):
        start = time.perf_counter()
        func(*args, **kwargs)
        end = time.perf_counter()
        times.append(end - start)

    return BenchmarkResult(
        name=func.__name__,
        mean_time=mean(times),
        std_dev=stdev(times) if len(times) > 1 else 0.0,
        min_time=min(times),
        max_time=max(times),
        iterations=iterations
    )


# ä½¿ç”¨ä¾‹
def process_list_comprehension(n: int) -> list[int]:
    return [x ** 2 for x in range(n)]

def process_map(n: int) -> list[int]:
    return list(map(lambda x: x ** 2, range(n)))

result1 = benchmark(process_list_comprehension, 10000, iterations=1000)
result2 = benchmark(process_map, 10000, iterations=1000)

print(result1)
print("\n")
print(result2)
```

---

## ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

### cProfile

**åŸºæœ¬çš„ãªä½¿ã„æ–¹**:
```python
import cProfile
import pstats
from io import StringIO


def expensive_function():
    """é‡ã„å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
    total = 0
    for i in range(1000000):
        total += i ** 2
    return total


def main():
    result = expensive_function()
    # ãã®ä»–ã®å‡¦ç†...


# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ
if __name__ == "__main__":
    profiler = cProfile.Profile()
    profiler.enable()

    main()

    profiler.disable()

    # çµæœã‚’å‡ºåŠ›
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)  # ä¸Šä½10ä»¶ã‚’è¡¨ç¤º
```

**ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰å®Ÿè¡Œ**:
```bash
# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ
python -m cProfile -o output.prof script.py

# çµæœã‚’è¡¨ç¤º
python -m pstats output.prof
# stats> sort cumulative
# stats> stats 10
```

**ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°**:
```python
import cProfile
import pstats
from functools import wraps
from typing import Callable


def profile(output_file: str | None = None):
    """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            profiler = cProfile.Profile()
            profiler.enable()

            result = func(*args, **kwargs)

            profiler.disable()

            if output_file:
                profiler.dump_stats(output_file)
            else:
                stats = pstats.Stats(profiler)
                stats.sort_stats('cumulative')
                stats.print_stats(20)

            return result
        return wrapper
    return decorator


@profile(output_file="my_function.prof")
def my_function():
    # å‡¦ç†...
    pass
```

### line_profiler

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**:
```bash
pip install line-profiler
```

**ä½¿ç”¨æ–¹æ³•**:
```python
# script.py
@profile  # line_profiler ã®ãƒã‚¸ãƒƒã‚¯ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def process_data(data: list[int]) -> list[int]:
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†"""
    result = []
    for item in data:
        # å„è¡Œã®å®Ÿè¡Œæ™‚é–“ãŒè¨ˆæ¸¬ã•ã‚Œã‚‹
        squared = item ** 2
        if squared > 100:
            result.append(squared)
    return result


def main():
    data = list(range(10000))
    result = process_data(data)


if __name__ == "__main__":
    main()
```

**å®Ÿè¡Œ**:
```bash
# è¡Œå˜ä½ã§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
kernprof -l -v script.py

# å‡ºåŠ›ä¾‹:
# Line #      Hits         Time  Per Hit   % Time  Line Contents
# ==============================================================
#      1                                           @profile
#      2                                           def process_data(data):
#      3         1          2.0      2.0      0.0      result = []
#      4     10000       5234.0      0.5     45.2      for item in data:
#      5     10000       3456.0      0.3     29.8          squared = item ** 2
#      6     10000       2345.0      0.2     20.2          if squared > 100:
#      7      9900        567.0      0.1      4.8              result.append(squared)
#      8         1          0.0      0.0      0.0      return result
```

### memory_profiler

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**:
```bash
pip install memory-profiler
```

**ä½¿ç”¨æ–¹æ³•**:
```python
from memory_profiler import profile


@profile
def memory_intensive_function():
    """ãƒ¡ãƒ¢ãƒªã‚’å¤§é‡ã«ä½¿ã†é–¢æ•°"""
    # å¤§ããªãƒªã‚¹ãƒˆã‚’ä½œæˆ
    data = [i for i in range(1000000)]

    # ã•ã‚‰ã«åŠ å·¥
    squared = [x ** 2 for x in data]

    # è¾æ›¸ã«å¤‰æ›
    result = {i: x for i, x in enumerate(squared)}

    return result


if __name__ == "__main__":
    memory_intensive_function()
```

**å®Ÿè¡Œ**:
```bash
# ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
python -m memory_profiler script.py

# å‡ºåŠ›ä¾‹:
# Line #    Mem usage    Increment  Occurrences   Line Contents
# =============================================================
#      1     45.2 MiB     45.2 MiB           1   @profile
#      2                                         def memory_intensive_function():
#      3     83.5 MiB     38.3 MiB           1       data = [i for i in range(1000000)]
#      4    121.8 MiB     38.3 MiB           1       squared = [x ** 2 for x in data]
#      5    198.4 MiB     76.6 MiB           1       result = {i: x for i, x in enumerate(squared)}
#      6    198.4 MiB      0.0 MiB           1       return result
```

### pyinstrument

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**:
```bash
pip install pyinstrument
```

**ä½¿ç”¨æ–¹æ³•**:
```python
from pyinstrument import Profiler


def main():
    # å‡¦ç†...
    pass


if __name__ == "__main__":
    profiler = Profiler()
    profiler.start()

    main()

    profiler.stop()

    # çµæœã‚’å‡ºåŠ›
    profiler.print()

    # HTML ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    with open("profile_report.html", "w") as f:
        f.write(profiler.output_html())
```

**ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰å®Ÿè¡Œ**:
```bash
# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ
pyinstrument script.py

# HTML ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
pyinstrument -o report.html script.py
```

---

## ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®æœ€é©åŒ–

### ãƒªã‚¹ãƒˆ vs ã‚¿ãƒ—ãƒ« vs ã‚»ãƒƒãƒˆ vs è¾æ›¸

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ**:
```python
import timeit


def compare_data_structures():
    """ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ"""

    # ãƒªã‚¹ãƒˆ
    list_creation = timeit.timeit(
        stmt='[i for i in range(1000)]',
        number=10000
    )

    list_lookup = timeit.timeit(
        stmt='999 in data',
        setup='data = list(range(1000))',
        number=10000
    )

    # ã‚¿ãƒ—ãƒ«
    tuple_creation = timeit.timeit(
        stmt='tuple(i for i in range(1000))',
        number=10000
    )

    tuple_lookup = timeit.timeit(
        stmt='999 in data',
        setup='data = tuple(range(1000))',
        number=10000
    )

    # ã‚»ãƒƒãƒˆ
    set_creation = timeit.timeit(
        stmt='{i for i in range(1000)}',
        number=10000
    )

    set_lookup = timeit.timeit(
        stmt='999 in data',
        setup='data = set(range(1000))',
        number=10000
    )

    # è¾æ›¸
    dict_creation = timeit.timeit(
        stmt='{i: i for i in range(1000)}',
        number=10000
    )

    dict_lookup = timeit.timeit(
        stmt='999 in data',
        setup='data = {i: i for i in range(1000)}',
        number=10000
    )

    print("Creation times:")
    print(f"  List:  {list_creation:.6f}s")
    print(f"  Tuple: {tuple_creation:.6f}s")
    print(f"  Set:   {set_creation:.6f}s")
    print(f"  Dict:  {dict_creation:.6f}s")

    print("\nLookup times:")
    print(f"  List:  {list_lookup:.6f}s")  # O(n) - é…ã„
    print(f"  Tuple: {tuple_lookup:.6f}s")  # O(n) - é…ã„
    print(f"  Set:   {set_lookup:.6f}s")    # O(1) - é€Ÿã„!
    print(f"  Dict:  {dict_lookup:.6f}s")   # O(1) - é€Ÿã„!


compare_data_structures()
```

**æœ€é©ãªé¸æŠ**:
```python
# âŒ é…ã„: ãƒªã‚¹ãƒˆã§è¦ç´ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
def slow_check(items: list[int], target: int) -> bool:
    return target in items  # O(n)


# âœ… é€Ÿã„: ã‚»ãƒƒãƒˆã§è¦ç´ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
def fast_check(items: set[int], target: int) -> bool:
    return target in items  # O(1)


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
import timeit

data_list = list(range(100000))
data_set = set(range(100000))

time_list = timeit.timeit(
    stmt='99999 in data',
    setup='from __main__ import data_list as data',
    number=10000
)

time_set = timeit.timeit(
    stmt='99999 in data',
    setup='from __main__ import data_set as data',
    number=10000
)

print(f"List lookup: {time_list:.6f}s")  # ç´„ 0.5s
print(f"Set lookup:  {time_set:.6f}s")   # ç´„ 0.0001s (5000å€é€Ÿã„!)
```

### collections ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

**defaultdict**:
```python
from collections import defaultdict


# âŒ é…ã„: é€šå¸¸ã®è¾æ›¸
def group_by_category_slow(items: list[dict]) -> dict[str, list[dict]]:
    result = {}
    for item in items:
        category = item['category']
        if category not in result:
            result[category] = []
        result[category].append(item)
    return result


# âœ… é€Ÿã„: defaultdict
def group_by_category_fast(items: list[dict]) -> dict[str, list[dict]]:
    result = defaultdict(list)
    for item in items:
        result[item['category']].append(item)
    return result
```

**Counter**:
```python
from collections import Counter


# âŒ é…ã„: æ‰‹å‹•ã§ã‚«ã‚¦ãƒ³ãƒˆ
def count_words_slow(words: list[str]) -> dict[str, int]:
    counts = {}
    for word in words:
        if word in counts:
            counts[word] += 1
        else:
            counts[word] = 1
    return counts


# âœ… é€Ÿã„: Counter
def count_words_fast(words: list[str]) -> dict[str, int]:
    return Counter(words)


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
words = ["apple", "banana", "apple", "cherry", "banana", "apple"] * 10000

import timeit

time_slow = timeit.timeit(
    stmt='count_words_slow(words)',
    setup='from __main__ import count_words_slow, words',
    number=100
)

time_fast = timeit.timeit(
    stmt='count_words_fast(words)',
    setup='from __main__ import count_words_fast, words',
    number=100
)

print(f"Manual count: {time_slow:.6f}s")
print(f"Counter:      {time_fast:.6f}s")  # ç´„2å€é€Ÿã„
```

**dequeï¼ˆä¸¡ç«¯ã‚­ãƒ¥ãƒ¼ï¼‰**:
```python
from collections import deque
import timeit


# ãƒªã‚¹ãƒˆã®å…ˆé ­æŒ¿å…¥ã¯é…ã„ï¼ˆO(n)ï¼‰
list_insert = timeit.timeit(
    stmt='data.insert(0, 1)',
    setup='data = list(range(10000))',
    number=1000
)

# deque ã®å…ˆé ­æŒ¿å…¥ã¯é€Ÿã„ï¼ˆO(1)ï¼‰
deque_insert = timeit.timeit(
    stmt='data.appendleft(1)',
    setup='from collections import deque; data = deque(range(10000))',
    number=1000
)

print(f"List insert at front:  {list_insert:.6f}s")  # é…ã„
print(f"Deque insert at front: {deque_insert:.6f}s")  # é€Ÿã„ (100å€ä»¥ä¸Šé€Ÿã„!)
```

---

## ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ€é©åŒ–

### è¨ˆç®—é‡ã®æ”¹å–„

**O(nÂ²) â†’ O(n) ã¸ã®æœ€é©åŒ–**:
```python
# âŒ O(nÂ²): ãƒã‚¹ãƒˆãƒ«ãƒ¼ãƒ—
def find_duplicates_slow(nums: list[int]) -> list[int]:
    duplicates = []
    for i in range(len(nums)):
        for j in range(i + 1, len(nums)):
            if nums[i] == nums[j] and nums[i] not in duplicates:
                duplicates.append(nums[i])
    return duplicates


# âœ… O(n): ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
def find_duplicates_fast(nums: list[int]) -> list[int]:
    seen = set()
    duplicates = set()
    for num in nums:
        if num in seen:
            duplicates.add(num)
        else:
            seen.add(num)
    return list(duplicates)


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
import timeit

data = list(range(1000)) * 2

time_slow = timeit.timeit(
    stmt='find_duplicates_slow(data)',
    setup='from __main__ import find_duplicates_slow, data',
    number=10
)

time_fast = timeit.timeit(
    stmt='find_duplicates_fast(data)',
    setup='from __main__ import find_duplicates_fast, data',
    number=10
)

print(f"O(nÂ²): {time_slow:.6f}s")
print(f"O(n):  {time_fast:.6f}s")  # 1000å€ä»¥ä¸Šé€Ÿã„!
```

**ã‚½ãƒ¼ãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠ**:
```python
import timeit
import random


def compare_sorting_algorithms(data: list[int]):
    """ã‚½ãƒ¼ãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒ"""

    # çµ„ã¿è¾¼ã¿ã‚½ãƒ¼ãƒˆï¼ˆTimsort - O(n log n)ï¼‰
    time_builtin = timeit.timeit(
        stmt='sorted(data)',
        setup=f'data = {data}',
        number=1000
    )

    # ãƒãƒ–ãƒ«ã‚½ãƒ¼ãƒˆï¼ˆO(nÂ²) - é…ã„ï¼‰
    def bubble_sort(arr):
        n = len(arr)
        for i in range(n):
            for j in range(0, n - i - 1):
                if arr[j] > arr[j + 1]:
                    arr[j], arr[j + 1] = arr[j + 1], arr[j]
        return arr

    time_bubble = timeit.timeit(
        stmt='bubble_sort(data.copy())',
        setup=f'from __main__ import bubble_sort; data = {data}',
        number=100
    )

    print(f"Built-in sort (Timsort): {time_builtin:.6f}s")
    print(f"Bubble sort:             {time_bubble:.6f}s")
    print(f"Speed ratio: {time_bubble / time_builtin:.1f}x slower")


# ãƒ†ã‚¹ãƒˆ
data = [random.randint(1, 100) for _ in range(100)]
compare_sorting_algorithms(data)
```

### ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã§é…å»¶è©•ä¾¡

**ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå‡¦ç†**:
```python
# âŒ é…ã„: ã™ã¹ã¦ã‚’ãƒ¡ãƒ¢ãƒªã«å±•é–‹
def process_all_at_once(n: int) -> int:
    squares = [x ** 2 for x in range(n)]
    evens = [x for x in squares if x % 2 == 0]
    return sum(evens)


# âœ… é€Ÿã„: ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã§é…å»¶è©•ä¾¡
def process_with_generator(n: int) -> int:
    squares = (x ** 2 for x in range(n))
    evens = (x for x in squares if x % 2 == 0)
    return sum(evens)


# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ
import sys
import timeit

n = 1000000

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
list_comp = [x ** 2 for x in range(n)]
gen_comp = (x ** 2 for x in range(n))

print(f"List size: {sys.getsizeof(list_comp):,} bytes")  # ç´„8MB
print(f"Gen size:  {sys.getsizeof(gen_comp):,} bytes")   # ç´„200 bytes

# å®Ÿè¡Œæ™‚é–“æ¯”è¼ƒ
time_list = timeit.timeit(
    stmt='process_all_at_once(1000000)',
    setup='from __main__ import process_all_at_once',
    number=10
)

time_gen = timeit.timeit(
    stmt='process_with_generator(1000000)',
    setup='from __main__ import process_with_generator',
    number=10
)

print(f"\nExecution time:")
print(f"List comprehension: {time_list:.6f}s")
print(f"Generator:          {time_gen:.6f}s")  # ç´„2å€é€Ÿã„
```

### itertools ã§åŠ¹ç‡çš„ãªå‡¦ç†

```python
from itertools import islice, chain, groupby


# âŒ é…ã„: ãƒªã‚¹ãƒˆã®ã‚¹ãƒ©ã‚¤ã‚¹
def get_first_n_slow(data: list, n: int) -> list:
    return data[:n]


# âœ… é€Ÿã„: isliceï¼ˆã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã®ã‚¹ãƒ©ã‚¤ã‚¹ï¼‰
def get_first_n_fast(data, n: int):
    return list(islice(data, n))


# ãƒã‚§ã‚¤ãƒ³ã§è¤‡æ•°ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’çµåˆ
def combine_iterators_slow(*iterators):
    result = []
    for iterator in iterators:
        result.extend(list(iterator))
    return result


def combine_iterators_fast(*iterators):
    return list(chain(*iterators))


# ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
from operator import itemgetter

data = [
    {"category": "A", "value": 10},
    {"category": "A", "value": 20},
    {"category": "B", "value": 30},
    {"category": "B", "value": 40},
]

# ã‚½ãƒ¼ãƒˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
data.sort(key=itemgetter("category"))

for category, group in groupby(data, key=itemgetter("category")):
    items = list(group)
    total = sum(item["value"] for item in items)
    print(f"{category}: {total}")
```

---

## ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¸¬å®š

```python
import sys
from typing import Any


def get_size(obj: Any) -> int:
    """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºã‚’å–å¾—"""
    return sys.getsizeof(obj)


# å„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º
data = list(range(1000))

print(f"List:  {get_size(data):,} bytes")
print(f"Tuple: {get_size(tuple(data)):,} bytes")
print(f"Set:   {get_size(set(data)):,} bytes")
print(f"Dict:  {get_size({i: i for i in data}):,} bytes")

# æ–‡å­—åˆ—ã®ãƒ¡ãƒ¢ãƒª
text = "Hello" * 1000
print(f"String: {get_size(text):,} bytes")
```

### __slots__ ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›

```python
import sys


# âŒ é€šå¸¸ã®ã‚¯ãƒ©ã‚¹ï¼ˆ__dict__ ã‚’æŒã¤ï¼‰
class NormalUser:
    def __init__(self, name: str, age: int, email: str):
        self.name = name
        self.age = age
        self.email = email


# âœ… __slots__ ä½¿ç”¨ï¼ˆ__dict__ ãªã—ï¼‰
class OptimizedUser:
    __slots__ = ['name', 'age', 'email']

    def __init__(self, name: str, age: int, email: str):
        self.name = name
        self.age = age
        self.email = email


# ãƒ¡ãƒ¢ãƒªæ¯”è¼ƒ
normal = NormalUser("Alice", 25, "alice@example.com")
optimized = OptimizedUser("Alice", 25, "alice@example.com")

print(f"Normal class:    {sys.getsizeof(normal)} bytes")
print(f"Optimized class: {sys.getsizeof(optimized)} bytes")
print(f"Memory saved:    {sys.getsizeof(normal) - sys.getsizeof(optimized)} bytes")

# å¤§é‡ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
normal_users = [NormalUser(f"User{i}", 25, f"user{i}@example.com") for i in range(10000)]
optimized_users = [OptimizedUser(f"User{i}", 25, f"user{i}@example.com") for i in range(10000)]

normal_total = sum(sys.getsizeof(u) for u in normal_users)
optimized_total = sum(sys.getsizeof(u) for u in optimized_users)

print(f"\n10,000 instances:")
print(f"Normal:    {normal_total:,} bytes")
print(f"Optimized: {optimized_total:,} bytes")
print(f"Saved:     {normal_total - optimized_total:,} bytes ({(1 - optimized_total/normal_total)*100:.1f}%)")
```

### ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–

```python
# âŒ ãƒ¡ãƒ¢ãƒªã‚’å¤§é‡ã«ä½¿ç”¨
def read_large_file_slow(file_path: str) -> list[str]:
    with open(file_path) as f:
        return f.readlines()  # ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿


# âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„
def read_large_file_fast(file_path: str):
    with open(file_path) as f:
        for line in f:  # 1è¡Œãšã¤å‡¦ç†
            yield line.strip()


# ä½¿ç”¨ä¾‹
for line in read_large_file_fast("large_file.txt"):
    process_line(line)  # 1è¡Œãšã¤å‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒä¸€å®šï¼‰
```

---

## ä¸¦åˆ—å‡¦ç†ãƒ»éåŒæœŸå‡¦ç†

### multiprocessing ã§ CPU ãƒã‚¦ãƒ³ãƒ‰å‡¦ç†

```python
from multiprocessing import Pool, cpu_count
import time


def cpu_bound_task(n: int) -> int:
    """CPU ã‚’ä½¿ã†é‡ã„å‡¦ç†"""
    return sum(i * i for i in range(n))


def sequential_processing(tasks: list[int]) -> list[int]:
    """é€æ¬¡å‡¦ç†"""
    return [cpu_bound_task(task) for task in tasks]


def parallel_processing(tasks: list[int], workers: int = None) -> list[int]:
    """ä¸¦åˆ—å‡¦ç†"""
    if workers is None:
        workers = cpu_count()

    with Pool(processes=workers) as pool:
        return pool.map(cpu_bound_task, tasks)


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
tasks = [10000000] * 8

start = time.time()
results_seq = sequential_processing(tasks)
time_seq = time.time() - start

start = time.time()
results_par = parallel_processing(tasks, workers=4)
time_par = time.time() - start

print(f"Sequential: {time_seq:.2f}s")
print(f"Parallel:   {time_par:.2f}s")
print(f"Speedup:    {time_seq / time_par:.2f}x")
```

### concurrent.futures

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import time


# I/O ãƒã‚¦ãƒ³ãƒ‰å‡¦ç†ï¼ˆThreadPoolExecutorï¼‰
def io_bound_task(url: str) -> dict:
    """I/O ã‚’å¾…ã¤å‡¦ç†ï¼ˆAPI ãƒªã‚¯ã‚¨ã‚¹ãƒˆãªã©ï¼‰"""
    time.sleep(0.1)  # I/O å¾…æ©Ÿã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    return {"url": url, "status": 200}


def process_urls_parallel(urls: list[str], max_workers: int = 10) -> list[dict]:
    """URL ã‚’ä¸¦åˆ—ã§å‡¦ç†"""
    results = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {executor.submit(io_bound_task, url): url for url in urls}

        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as exc:
                print(f"{url} generated an exception: {exc}")

    return results


# CPU ãƒã‚¦ãƒ³ãƒ‰å‡¦ç†ï¼ˆProcessPoolExecutorï¼‰
def cpu_bound_task(n: int) -> int:
    """CPU ã‚’ä½¿ã†å‡¦ç†"""
    return sum(i * i for i in range(n))


def process_cpu_bound_parallel(numbers: list[int], max_workers: int = 4) -> list[int]:
    """CPU ãƒã‚¦ãƒ³ãƒ‰å‡¦ç†ã‚’ä¸¦åˆ—åŒ–"""
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        return list(executor.map(cpu_bound_task, numbers))


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
urls = [f"https://example.com/page{i}" for i in range(50)]

start = time.time()
results_seq = [io_bound_task(url) for url in urls]
time_seq = time.time() - start

start = time.time()
results_par = process_urls_parallel(urls, max_workers=10)
time_par = time.time() - start

print(f"Sequential (I/O): {time_seq:.2f}s")
print(f"Parallel (I/O):   {time_par:.2f}s")
print(f"Speedup:          {time_seq / time_par:.2f}x")
```

### asyncio ã§éåŒæœŸå‡¦ç†

```python
import asyncio
import aiohttp
import time


async def fetch_url_async(session: aiohttp.ClientSession, url: str) -> dict:
    """éåŒæœŸã§ URL ã‚’å–å¾—"""
    async with session.get(url) as response:
        return {"url": url, "status": response.status}


async def fetch_all_urls_async(urls: list[str]) -> list[dict]:
    """è¤‡æ•° URL ã‚’éåŒæœŸã§å–å¾—"""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url_async(session, url) for url in urls]
        return await asyncio.gather(*tasks)


# åŒæœŸç‰ˆï¼ˆæ¯”è¼ƒç”¨ï¼‰
import requests


def fetch_url_sync(url: str) -> dict:
    """åŒæœŸã§ URL ã‚’å–å¾—"""
    response = requests.get(url)
    return {"url": url, "status": response.status_code}


def fetch_all_urls_sync(urls: list[str]) -> list[dict]:
    """è¤‡æ•° URL ã‚’åŒæœŸã§å–å¾—"""
    return [fetch_url_sync(url) for url in urls]


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
async def benchmark_async():
    urls = ["https://httpbin.org/delay/1"] * 10

    # éåŒæœŸç‰ˆ
    start = time.time()
    results = await fetch_all_urls_async(urls)
    time_async = time.time() - start

    print(f"Async:  {time_async:.2f}s")
    print(f"Result: {len(results)} URLs fetched")


# å®Ÿè¡Œ
asyncio.run(benchmark_async())
# Async:  1.2s (10å€‹ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸¦è¡Œå®Ÿè¡Œ)
# åŒæœŸç‰ˆã ã¨ 10s ã‹ã‹ã‚‹
```

---

## NumPy/Pandas æœ€é©åŒ–

### NumPy ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–

```python
import numpy as np
import timeit


# âŒ é…ã„: Python ã®ãƒ«ãƒ¼ãƒ—
def sum_of_squares_python(arr: list[float]) -> float:
    total = 0
    for x in arr:
        total += x ** 2
    return total


# âœ… é€Ÿã„: NumPy ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
def sum_of_squares_numpy(arr: np.ndarray) -> float:
    return np.sum(arr ** 2)


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
data_list = list(range(1000000))
data_numpy = np.array(data_list)

time_python = timeit.timeit(
    stmt='sum_of_squares_python(data)',
    setup='from __main__ import sum_of_squares_python, data_list as data',
    number=10
)

time_numpy = timeit.timeit(
    stmt='sum_of_squares_numpy(data)',
    setup='from __main__ import sum_of_squares_numpy, data_numpy as data',
    number=10
)

print(f"Python loop: {time_python:.6f}s")
print(f"NumPy:       {time_numpy:.6f}s")
print(f"Speedup:     {time_python / time_numpy:.1f}x")  # 100å€ä»¥ä¸Šé€Ÿã„!
```

**NumPy æœ€é©åŒ–ã®ã‚³ãƒ„**:
```python
import numpy as np


# âŒ é…ã„: ãƒ«ãƒ¼ãƒ—ã§è¦ç´ ã«ã‚¢ã‚¯ã‚»ã‚¹
def slow_processing(arr: np.ndarray) -> np.ndarray:
    result = np.zeros_like(arr)
    for i in range(len(arr)):
        result[i] = arr[i] ** 2 + 2 * arr[i] + 1
    return result


# âœ… é€Ÿã„: ãƒ™ã‚¯ãƒˆãƒ«åŒ–
def fast_processing(arr: np.ndarray) -> np.ndarray:
    return arr ** 2 + 2 * arr + 1


# âœ… ã•ã‚‰ã«é€Ÿã„: in-place æ¼”ç®—
def fastest_processing(arr: np.ndarray) -> np.ndarray:
    result = arr.copy()
    result **= 2
    result += 2 * arr
    result += 1
    return result


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
data = np.arange(1000000, dtype=np.float64)

time_slow = timeit.timeit(
    stmt='slow_processing(data)',
    setup='from __main__ import slow_processing, data',
    number=10
)

time_fast = timeit.timeit(
    stmt='fast_processing(data)',
    setup='from __main__ import fast_processing, data',
    number=10
)

time_fastest = timeit.timeit(
    stmt='fastest_processing(data)',
    setup='from __main__ import fastest_processing, data',
    number=10
)

print(f"Loop:        {time_slow:.6f}s")
print(f"Vectorized:  {time_fast:.6f}s")
print(f"In-place:    {time_fastest:.6f}s")
```

### Pandas æœ€é©åŒ–

**iterrows() ã‚’é¿ã‘ã‚‹**:
```python
import pandas as pd
import numpy as np
import timeit


# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
df = pd.DataFrame({
    'A': np.random.rand(100000),
    'B': np.random.rand(100000),
    'C': np.random.rand(100000),
})


# âŒ æœ€ã‚‚é…ã„: iterrows()
def process_with_iterrows(df: pd.DataFrame) -> pd.Series:
    results = []
    for index, row in df.iterrows():
        results.append(row['A'] + row['B'] * row['C'])
    return pd.Series(results)


# âš ï¸ é…ã„: apply()
def process_with_apply(df: pd.DataFrame) -> pd.Series:
    return df.apply(lambda row: row['A'] + row['B'] * row['C'], axis=1)


# âœ… é€Ÿã„: ãƒ™ã‚¯ãƒˆãƒ«åŒ–
def process_vectorized(df: pd.DataFrame) -> pd.Series:
    return df['A'] + df['B'] * df['C']


# âœ… æœ€é€Ÿ: NumPy
def process_numpy(df: pd.DataFrame) -> pd.Series:
    return pd.Series(df['A'].values + df['B'].values * df['C'].values)


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
time_iterrows = timeit.timeit(
    stmt='process_with_iterrows(df)',
    setup='from __main__ import process_with_iterrows, df',
    number=10
)

time_apply = timeit.timeit(
    stmt='process_with_apply(df)',
    setup='from __main__ import process_with_apply, df',
    number=10
)

time_vectorized = timeit.timeit(
    stmt='process_vectorized(df)',
    setup='from __main__ import process_vectorized, df',
    number=10
)

time_numpy = timeit.timeit(
    stmt='process_numpy(df)',
    setup='from __main__ import process_numpy, df',
    number=10
)

print(f"iterrows():   {time_iterrows:.6f}s")
print(f"apply():      {time_apply:.6f}s")
print(f"Vectorized:   {time_vectorized:.6f}s")
print(f"NumPy:        {time_numpy:.6f}s")
print(f"\nSpeedup (iterrows vs NumPy): {time_iterrows / time_numpy:.1f}x")
```

**ã‚«ãƒ†ã‚´ãƒªå‹ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**:
```python
import pandas as pd


# æ–‡å­—åˆ—ã‚«ãƒ©ãƒ 
df = pd.DataFrame({
    'category': ['A', 'B', 'C', 'A', 'B'] * 100000
})

print(f"String type: {df.memory_usage(deep=True)['category']:,} bytes")

# ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›
df['category'] = df['category'].astype('category')

print(f"Category type: {df.memory_usage(deep=True)['category']:,} bytes")
print(f"Memory saved: {100 - (df.memory_usage(deep=True)['category'] / 28000000 * 100):.1f}%")
```

**ãƒãƒ£ãƒ³ã‚¯ã§å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†**:
```python
import pandas as pd


# âŒ ãƒ¡ãƒ¢ãƒªä¸è¶³: ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚’èª­ã¿è¾¼ã¿
# df = pd.read_csv("huge_file.csv")  # ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼!


# âœ… ãƒãƒ£ãƒ³ã‚¯ã§å‡¦ç†
def process_large_file(file_path: str, chunk_size: int = 10000):
    """å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯ã§å‡¦ç†"""
    results = []

    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‡¦ç†
        processed = chunk[chunk['value'] > 100]
        results.append(processed)

    # çµæœã‚’çµåˆ
    return pd.concat(results, ignore_index=True)


# ä½¿ç”¨ä¾‹
# result = process_large_file("huge_file.csv")
```

---

## ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

### functools.lru_cache

```python
from functools import lru_cache
import timeit


# âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
def fibonacci_no_cache(n: int) -> int:
    if n < 2:
        return n
    return fibonacci_no_cache(n - 1) + fibonacci_no_cache(n - 2)


# âœ… lru_cache ã§ãƒ¡ãƒ¢åŒ–
@lru_cache(maxsize=128)
def fibonacci_cached(n: int) -> int:
    if n < 2:
        return n
    return fibonacci_cached(n - 1) + fibonacci_cached(n - 2)


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
time_no_cache = timeit.timeit(
    stmt='fibonacci_no_cache(30)',
    setup='from __main__ import fibonacci_no_cache',
    number=1
)

time_cached = timeit.timeit(
    stmt='fibonacci_cached(30)',
    setup='from __main__ import fibonacci_cached',
    number=1
)

print(f"No cache: {time_no_cache:.6f}s")
print(f"Cached:   {time_cached:.6f}s")
print(f"Speedup:  {time_no_cache / time_cached:.0f}x")  # 100,000å€ä»¥ä¸Šé€Ÿã„!

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
print(f"\nCache stats: {fibonacci_cached.cache_info()}")
# CacheInfo(hits=28, misses=31, maxsize=128, currsize=31)
```

**ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿**:
```python
from functools import wraps
from typing import Callable, Any
import time


def timed_cache(expiry_seconds: int = 60):
    """æœŸé™ä»˜ãã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func: Callable) -> Callable:
        cache = {}

        @wraps(func)
        def wrapper(*args, **kwargs):
            key = str(args) + str(kwargs)
            now = time.time()

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if key in cache:
                result, timestamp = cache[key]
                if now - timestamp < expiry_seconds:
                    print(f"Cache hit for {func.__name__}")
                    return result

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ - é–¢æ•°å®Ÿè¡Œ
            print(f"Cache miss for {func.__name__}")
            result = func(*args, **kwargs)
            cache[key] = (result, now)

            return result

        return wrapper
    return decorator


@timed_cache(expiry_seconds=5)
def expensive_computation(x: int) -> int:
    """é‡ã„è¨ˆç®—"""
    time.sleep(2)
    return x ** 2


# ä½¿ç”¨ä¾‹
print(expensive_computation(10))  # Cache miss - 2ç§’å¾…æ©Ÿ
print(expensive_computation(10))  # Cache hit - å³åº§ã«è¿”ã™
time.sleep(6)
print(expensive_computation(10))  # Cache miss - æœŸé™åˆ‡ã‚Œ
```

### Redis ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥

```bash
pip install redis
```

```python
import redis
import json
import time
from functools import wraps
from typing import Callable, Any


# Redis æ¥ç¶š
redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)


def redis_cache(expiry_seconds: int = 3600):
    """Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
            cache_key = f"{func.__name__}:{str(args)}:{str(kwargs)}"

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cached_value = redis_client.get(cache_key)
            if cached_value:
                print(f"Redis cache hit for {func.__name__}")
                return json.loads(cached_value)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ - é–¢æ•°å®Ÿè¡Œ
            print(f"Redis cache miss for {func.__name__}")
            result = func(*args, **kwargs)

            # Redis ã«ä¿å­˜
            redis_client.setex(
                cache_key,
                expiry_seconds,
                json.dumps(result)
            )

            return result

        return wrapper
    return decorator


@redis_cache(expiry_seconds=60)
def get_user_data(user_id: int) -> dict:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆé‡ã„å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰"""
    time.sleep(2)
    return {
        "id": user_id,
        "name": f"User {user_id}",
        "email": f"user{user_id}@example.com"
    }


# ä½¿ç”¨ä¾‹
print(get_user_data(1))  # Cache miss - 2ç§’å¾…æ©Ÿ
print(get_user_data(1))  # Cache hit - å³åº§ã«è¿”ã™
```

---

## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–

### SQLAlchemy æœ€é©åŒ–

**N+1 å•é¡Œã®è§£æ±º**:
```python
from sqlalchemy import create_engine, Column, Integer, String, ForeignKey
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship, joinedload
import time

Base = declarative_base()


class Author(Base):
    __tablename__ = 'authors'
    id = Column(Integer, primary_key=True)
    name = Column(String)
    books = relationship('Book', back_populates='author')


class Book(Base):
    __tablename__ = 'books'
    id = Column(Integer, primary_key=True)
    title = Column(String)
    author_id = Column(Integer, ForeignKey('authors.id'))
    author = relationship('Author', back_populates='books')


# âŒ N+1 å•é¡Œ
def get_books_slow(session):
    """å„æ›¸ç±ã”ã¨ã«è‘—è€…ã‚’å–å¾—ï¼ˆN+1ã‚¯ã‚¨ãƒªï¼‰"""
    books = session.query(Book).all()

    for book in books:
        print(f"{book.title} by {book.author.name}")  # å„ãƒ«ãƒ¼ãƒ—ã§ã‚¯ã‚¨ãƒªå®Ÿè¡Œ


# âœ… joinedload ã§æœ€é©åŒ–
def get_books_fast(session):
    """1ã‚¯ã‚¨ãƒªã§æ›¸ç±ã¨è‘—è€…ã‚’å–å¾—"""
    books = session.query(Book).options(joinedload(Book.author)).all()

    for book in books:
        print(f"{book.title} by {book.author.name}")  # ã‚¯ã‚¨ãƒªãªã—


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
engine = create_engine('sqlite:///books.db')
Session = sessionmaker(bind=engine)
session = Session()

start = time.time()
get_books_slow(session)
time_slow = time.time() - start

start = time.time()
get_books_fast(session)
time_fast = time.time() - start

print(f"\nN+1 queries: {time_slow:.6f}s")
print(f"Optimized:   {time_fast:.6f}s")
print(f"Speedup:     {time_slow / time_fast:.1f}x")
```

**ãƒãƒ«ã‚¯æŒ¿å…¥**:
```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker


# âŒ é…ã„: 1ä»¶ãšã¤ã‚³ãƒŸãƒƒãƒˆ
def insert_one_by_one(session, data: list[dict]):
    for item in data:
        user = User(**item)
        session.add(user)
        session.commit()  # å„æŒ¿å…¥ã§ã‚³ãƒŸãƒƒãƒˆ


# âœ… é€Ÿã„: ãƒãƒ«ã‚¯æŒ¿å…¥
def bulk_insert(session, data: list[dict]):
    session.bulk_insert_mappings(User, data)
    session.commit()  # æœ€å¾Œã«1å›ã ã‘ã‚³ãƒŸãƒƒãƒˆ


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
data = [{"name": f"User{i}", "email": f"user{i}@example.com"} for i in range(1000)]

start = time.time()
insert_one_by_one(session, data)
time_slow = time.time() - start

start = time.time()
bulk_insert(session, data)
time_fast = time.time() - start

print(f"One by one: {time_slow:.6f}s")
print(f"Bulk:       {time_fast:.6f}s")
print(f"Speedup:    {time_slow / time_fast:.1f}x")  # 100å€ä»¥ä¸Šé€Ÿã„!
```

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–

```python
from sqlalchemy import Index


class User(Base):
    __tablename__ = 'users'

    id = Column(Integer, primary_key=True)
    email = Column(String, unique=True, index=True)  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ 
    name = Column(String, index=True)  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ 
    created_at = Column(DateTime, index=True)

    # è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    __table_args__ = (
        Index('idx_email_name', 'email', 'name'),
        Index('idx_created_at_desc', 'created_at desc'),
    )
```

---

## Cythonãƒ»JIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

### Cython ã§é«˜é€ŸåŒ–

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**:
```bash
pip install cython
```

**Cython ã‚³ãƒ¼ãƒ‰ï¼ˆfast_math.pyxï¼‰**:
```python
# cython: language_level=3
# cython: boundscheck=False
# cython: wraparound=False

def sum_of_squares_cython(int n):
    """å¹³æ–¹å’Œã‚’Cythonã§è¨ˆç®—"""
    cdef long long total = 0
    cdef int i

    for i in range(n):
        total += i * i

    return total
```

**setup.py**:
```python
from setuptools import setup
from Cython.Build import cythonize

setup(
    ext_modules=cythonize("fast_math.pyx")
)
```

**ãƒ“ãƒ«ãƒ‰**:
```bash
python setup.py build_ext --inplace
```

**ä½¿ç”¨ä¾‹**:
```python
import timeit
from fast_math import sum_of_squares_cython


# Python ç‰ˆ
def sum_of_squares_python(n: int) -> int:
    total = 0
    for i in range(n):
        total += i * i
    return total


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
n = 10000000

time_python = timeit.timeit(
    stmt='sum_of_squares_python(n)',
    setup='from __main__ import sum_of_squares_python, n',
    number=10
)

time_cython = timeit.timeit(
    stmt='sum_of_squares_cython(n)',
    setup='from fast_math import sum_of_squares_cython; from __main__ import n',
    number=10
)

print(f"Python:  {time_python:.6f}s")
print(f"Cython:  {time_cython:.6f}s")
print(f"Speedup: {time_python / time_cython:.1f}x")  # 10-100å€é€Ÿã„!
```

### Numba ã§ JIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**:
```bash
pip install numba
```

**ä½¿ç”¨ä¾‹**:
```python
from numba import jit
import numpy as np
import timeit


# âŒ é€šå¸¸ã® Python
def sum_of_squares_python(arr):
    total = 0
    for i in range(len(arr)):
        total += arr[i] ** 2
    return total


# âœ… Numba JIT
@jit(nopython=True)
def sum_of_squares_numba(arr):
    total = 0
    for i in range(len(arr)):
        total += arr[i] ** 2
    return total


# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
arr = np.arange(10000000, dtype=np.int64)

# ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆJIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼‰
sum_of_squares_numba(arr)

time_python = timeit.timeit(
    stmt='sum_of_squares_python(arr)',
    setup='from __main__ import sum_of_squares_python, arr',
    number=10
)

time_numba = timeit.timeit(
    stmt='sum_of_squares_numba(arr)',
    setup='from __main__ import sum_of_squares_numba, arr',
    number=10
)

print(f"Python: {time_python:.6f}s")
print(f"Numba:  {time_numba:.6f}s")
print(f"Speedup: {time_python / time_numba:.1f}x")  # 100å€ä»¥ä¸Šé€Ÿã„!
```

### PyPy

**PyPy ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**:
```bash
# PyPy ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# https://www.pypy.org/download.html
```

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**:
```python
# benchmark.py
def fibonacci(n):
    if n < 2:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)

result = fibonacci(35)
print(f"Result: {result}")
```

**å®Ÿè¡Œæ¯”è¼ƒ**:
```bash
# CPython
time python benchmark.py
# real: 3.5s

# PyPy
time pypy3 benchmark.py
# real: 0.3s  (ç´„10å€é€Ÿã„!)
```

---

## å®Ÿè·µçš„ãªæœ€é©åŒ–äº‹ä¾‹

### ã‚±ãƒ¼ã‚¹1: API ãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ€é©åŒ–

**Beforeï¼ˆé…ã„ï¼‰**:
```python
from fastapi import FastAPI
from sqlalchemy.orm import Session

app = FastAPI()


@app.get("/users")
def get_users(db: Session = Depends(get_db)):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§å–å¾—ï¼ˆæœ€é©åŒ–å‰ï¼‰"""
    users = db.query(User).all()

    # N+1 å•é¡Œ: å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿æ•°ã‚’å–å¾—
    result = []
    for user in users:
        result.append({
            "id": user.id,
            "name": user.name,
            "posts_count": len(user.posts)  # å„ãƒ«ãƒ¼ãƒ—ã§ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
        })

    return result
```

**Afterï¼ˆé€Ÿã„ï¼‰**:
```python
from fastapi import FastAPI
from sqlalchemy import func
from sqlalchemy.orm import Session, joinedload

app = FastAPI()


@app.get("/users")
def get_users(db: Session = Depends(get_db)):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§å–å¾—ï¼ˆæœ€é©åŒ–å¾Œï¼‰"""
    # 1ã‚¯ã‚¨ãƒªã§é›†è¨ˆ
    users = (
        db.query(
            User.id,
            User.name,
            func.count(Post.id).label('posts_count')
        )
        .outerjoin(Post)
        .group_by(User.id, User.name)
        .all()
    )

    return [
        {
            "id": user.id,
            "name": user.name,
            "posts_count": user.posts_count
        }
        for user in users
    ]


# ã•ã‚‰ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¿½åŠ 
from functools import lru_cache

@lru_cache(maxsize=1)
def get_users_cached(db: Session):
    # ...åŒã˜å‡¦ç†
    pass
```

### ã‚±ãƒ¼ã‚¹2: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–

**Beforeï¼ˆé…ã„ï¼‰**:
```python
import pandas as pd


def process_sales_data_slow(file_path: str) -> pd.DataFrame:
    """å£²ä¸Šãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆæœ€é©åŒ–å‰ï¼‰"""
    # ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚’èª­ã¿è¾¼ã¿
    df = pd.read_csv(file_path)

    # iterrows ã§å‡¦ç†ï¼ˆé…ã„ï¼‰
    results = []
    for index, row in df.iterrows():
        if row['amount'] > 1000:
            results.append({
                'date': row['date'],
                'total': row['amount'] * row['quantity'],
                'category': row['category']
            })

    return pd.DataFrame(results)
```

**Afterï¼ˆé€Ÿã„ï¼‰**:
```python
import pandas as pd


def process_sales_data_fast(file_path: str) -> pd.DataFrame:
    """å£²ä¸Šãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆæœ€é©åŒ–å¾Œï¼‰"""
    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿èª­ã¿è¾¼ã¿ + å‹æŒ‡å®š
    df = pd.read_csv(
        file_path,
        usecols=['date', 'amount', 'quantity', 'category'],
        dtype={
            'amount': 'float32',
            'quantity': 'int32',
            'category': 'category'
        },
        parse_dates=['date']
    )

    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—
    df = df[df['amount'] > 1000].copy()
    df['total'] = df['amount'] * df['quantity']

    return df[['date', 'total', 'category']]


# ã•ã‚‰ã«ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚’è¿½åŠ 
def process_sales_data_chunks(file_path: str, chunk_size: int = 10000) -> pd.DataFrame:
    """å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ£ãƒ³ã‚¯ã§å‡¦ç†"""
    chunks = []

    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‡¦ç†
        processed = chunk[chunk['amount'] > 1000].copy()
        processed['total'] = processed['amount'] * processed['quantity']
        chunks.append(processed[['date', 'total', 'category']])

    return pd.concat(chunks, ignore_index=True)
```

### ã‚±ãƒ¼ã‚¹3: ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

**Beforeï¼ˆé…ã„ï¼‰**:
```python
import requests
from bs4 import BeautifulSoup


def scrape_pages_slow(urls: list[str]) -> list[dict]:
    """ãƒšãƒ¼ã‚¸ã‚’é€æ¬¡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    results = []

    for url in urls:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.find('h1').text
        results.append({'url': url, 'title': title})

    return results


# 100ãƒšãƒ¼ã‚¸ã§ç´„100ç§’
```

**Afterï¼ˆé€Ÿã„ï¼‰**:
```python
import asyncio
import aiohttp
from bs4 import BeautifulSoup


async def scrape_page(session: aiohttp.ClientSession, url: str) -> dict:
    """ãƒšãƒ¼ã‚¸ã‚’éåŒæœŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    async with session.get(url) as response:
        content = await response.text()
        soup = BeautifulSoup(content, 'html.parser')
        title = soup.find('h1').text if soup.find('h1') else ''
        return {'url': url, 'title': title}


async def scrape_pages_fast(urls: list[str]) -> list[dict]:
    """è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚’ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    async with aiohttp.ClientSession() as session:
        tasks = [scrape_page(session, url) for url in urls]
        return await asyncio.gather(*tasks)


# 100ãƒšãƒ¼ã‚¸ã§ç´„2ç§’ï¼ˆ50å€é€Ÿã„!ï¼‰
```

---

## ã¾ã¨ã‚

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**æ¸¬å®šãƒ»ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°**:
- [ ] time/timeit ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
- [ ] cProfile ã§é–¢æ•°ãƒ¬ãƒ™ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
- [ ] line_profiler ã§è¡Œãƒ¬ãƒ™ãƒ«ã®åˆ†æ
- [ ] memory_profiler ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç¢ºèª

**ãƒ‡ãƒ¼ã‚¿æ§‹é€ **:
- [ ] æ¤œç´¢ã¯ set/dict ã‚’ä½¿ç”¨ï¼ˆO(1)ï¼‰
- [ ] å¤§é‡ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ __slots__ ä½¿ç”¨
- [ ] collections ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ´»ç”¨
- [ ] ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:
- [ ] è¨ˆç®—é‡ã‚’æ„è­˜ï¼ˆO(nÂ²) â†’ O(n)ï¼‰
- [ ] ã‚½ãƒ¼ãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯çµ„ã¿è¾¼ã¿ä½¿ç”¨
- [ ] ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã§é…å»¶è©•ä¾¡
- [ ] itertools ã§åŠ¹ç‡çš„ãªå‡¦ç†

**NumPy/Pandas**:
- [ ] ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆãƒ«ãƒ¼ãƒ—ã‚’é¿ã‘ã‚‹ï¼‰
- [ ] iterrows() ã‚’é¿ã‘ã‚‹
- [ ] ã‚«ãƒ†ã‚´ãƒªå‹ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- [ ] ãƒãƒ£ãƒ³ã‚¯ã§å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†

**ä¸¦åˆ—ãƒ»éåŒæœŸ**:
- [ ] CPU ãƒã‚¦ãƒ³ãƒ‰: multiprocessing
- [ ] I/O ãƒã‚¦ãƒ³ãƒ‰: asyncio/ThreadPoolExecutor
- [ ] concurrent.futures ã§ç°¡å˜ã«ä¸¦åˆ—åŒ–

**ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°**:
- [ ] functools.lru_cache ã§ãƒ¡ãƒ¢åŒ–
- [ ] Redis ã§åˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ¥
- [ ] é©åˆ‡ãªæœ‰åŠ¹æœŸé™è¨­å®š

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**:
- [ ] N+1 å•é¡Œã‚’è§£æ±ºï¼ˆjoinedloadï¼‰
- [ ] ãƒãƒ«ã‚¯æŒ¿å…¥
- [ ] ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ 
- [ ] ã‚¯ã‚¨ãƒªæœ€é©åŒ–

**é«˜åº¦ãªæœ€é©åŒ–**:
- [ ] Cython ã§ C æ‹¡å¼µ
- [ ] Numba ã§ JIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
- [ ] PyPy ã§å®Ÿè¡Œé€Ÿåº¦å‘ä¸Š

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. è¨ˆæ¸¬ã—ã¦ã‹ã‚‰æœ€é©åŒ–

```python
# ã¾ãšè¨ˆæ¸¬
import cProfile

profiler = cProfile.Profile()
profiler.enable()
slow_function()
profiler.disable()

# ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ã¦ã‹ã‚‰æœ€é©åŒ–
```

### 2. æ®µéšçš„ã«æœ€é©åŒ–

1. **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: ã¾ãšè¨ˆç®—é‡ã‚’æ”¹å–„
2. **ãƒ‡ãƒ¼ã‚¿æ§‹é€ **: é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’é¸æŠ
3. **ãƒ™ã‚¯ãƒˆãƒ«åŒ–**: NumPy/Pandas ã§æœ€é©åŒ–
4. **ä¸¦åˆ—åŒ–**: CPU/I/O ãƒã‚¦ãƒ³ãƒ‰ã«å¿œã˜ã¦ä¸¦åˆ—åŒ–
5. **ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«**: Cython/Numba ã§æœ€å¾Œã®æœ€é©åŒ–

### 3. å¯èª­æ€§ã¨ã®ãƒãƒ©ãƒ³ã‚¹

```python
# âŒ éåº¦ãªæœ€é©åŒ–
result = sum(map(lambda x: x**2, filter(lambda x: x%2==0, data)))

# âœ… èª­ã¿ã‚„ã™ãã€ãã‚Œãªã‚Šã«é€Ÿã„
result = sum(x**2 for x in data if x % 2 == 0)
```

---

*è¨ˆæ¸¬ãƒ»åˆ†æãƒ»æœ€é©åŒ–ã®ã‚µã‚¤ã‚¯ãƒ«ã§é«˜é€ŸãªPythonã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ã€‚*
