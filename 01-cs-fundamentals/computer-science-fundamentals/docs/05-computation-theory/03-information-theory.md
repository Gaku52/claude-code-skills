# 情報理論

> シャノンの情報理論は、データ圧縮の限界と通信の信頼性の限界を数学的に明らかにした。

## この章で学ぶこと

- [ ] 情報量（ビット）の数学的定義を理解する
- [ ] エントロピーとデータ圧縮の関係を説明できる
- [ ] 通信路容量の概念を知る

---

## 1. 情報量とエントロピー

```
情報量: 事象の「驚き」の度合い

  I(x) = -log₂(P(x))

  例:
  公平なコイン: I(表) = -log₂(0.5) = 1 ビット
  確実な事象:   I(必ず) = -log₂(1) = 0 ビット
  稀な事象:     I(P=0.01) = -log₂(0.01) ≈ 6.64 ビット

  エントロピー: 情報量の期待値（平均的な「驚き」）
  H(X) = -Σ P(x) × log₂(P(x))

  例: 英語テキスト
  各文字の出現頻度は均一ではない（eが最頻）
  H ≈ 4.7 ビット/文字（理論値）
  → ASCII 8ビットから4.7ビットまで圧縮可能
  → 実際のテキスト圧縮率は文脈を利用してさらに向上

  エントロピーの性質:
  - 均一分布の時に最大（最もランダム、最も圧縮困難）
  - 1つの事象が確実な時に0（完全に予測可能、圧縮可能）
```

---

## 2. データ圧縮の限界

```
シャノンの符号化定理（第1定理）:

  「エントロピーH以下の平均符号長で可逆圧縮することは不可能」

  つまり:
  - エントロピーが圧縮の理論的限界
  - ハフマン符号はこの限界に近づく最適前置符号
  - 算術符号はさらに限界に近づく

  エントロピー符号化の進化:
  ハフマン符号 → 算術符号 → ANS（Asymmetric Numeral Systems）
  → 圧縮率はシャノン限界に漸近
  → ANSはzstdで使用（Facebookが開発）
```

---

## 3. 通信路符号化

```
シャノンの通信路符号化定理（第2定理）:

  通信路容量: C = max_{p(x)} I(X;Y)

  「通信速度がC以下であれば、誤り率を任意に小さくできる」
  「通信速度がCを超えると、誤り率は0にできない」

  実務: 誤り訂正符号
  - ハミング符号: 1ビットの誤り訂正
  - リードソロモン符号: バースト誤り訂正（CD, DVD, QRコード）
  - LDPC/Turbo符号: シャノン限界に近い性能（5G, Wi-Fi 6）
  - Polar符号: 理論的にシャノン限界を達成（5G制御チャネル）
```

---

## 4. 実務での応用

```
情報理論の実務応用:

  1. データ圧縮: エントロピーが圧縮限界を決める
  2. 暗号: エントロピーが鍵の安全性を決める
     → 128ビットAES鍵 = 128ビットのエントロピー
  3. 機械学習: 交差エントロピー損失関数
     → 予測分布と正解分布のKLダイバージェンス最小化
  4. 通信: 5G, Wi-Fiの誤り訂正符号設計
  5. 乱数生成: エントロピーソース（/dev/random）
```

---

## まとめ

| 概念 | ポイント |
|------|---------|
| 情報量 | -log₂(P(x))。稀な事象ほど情報量が大きい |
| エントロピー | 情報量の期待値。圧縮の理論的限界 |
| 第1定理 | エントロピー以下には圧縮不可能 |
| 第2定理 | 通信路容量以下なら信頼性を保証可能 |

---

## 次に読むべきガイド
→ [[04-cryptography-basics.md]] — 暗号の基礎

---

## 参考文献
1. Shannon, C. E. "A Mathematical Theory of Communication." 1948.
2. Cover, T. M. & Thomas, J. A. "Elements of Information Theory." Wiley, 2006.
