# 浮動小数点数 — IEEE 754

> 0.1 + 0.2 ≠ 0.3 という事実は、浮動小数点数の本質を理解しなければ永遠にプログラマを悩ませ続ける。

## この章で学ぶこと

- [ ] IEEE 754 の構造（符号・指数部・仮数部）を説明できる
- [ ] なぜ 0.1 + 0.2 ≠ 0.3 なのかを論理的に説明できる
- [ ] 浮動小数点の特殊値（NaN, Infinity, -0）を理解する
- [ ] 精度問題の実務的な対策を実装できる

## 前提知識

- 2進数 → 参照: [[00-binary-and-number-systems.md]]
- 整数表現 → 参照: [[02-integer-representation.md]]

---

## 1. なぜ浮動小数点が必要か

### 1.1 固定小数点の限界

```
科学技術計算で扱う数値の範囲:

  電子の質量:   9.109 × 10^(-31) kg
  太陽の質量:   1.989 × 10^(30) kg
  比率:         約10^61倍の差

  固定小数点（32ビット）で表現すると:
  - 整数部16ビット + 小数部16ビット
  - 範囲: 0 〜 65535.9999...
  - 電子の質量も太陽の質量も表現不可能

  → 指数表現が必要
  → 「浮動」小数点: 小数点の位置を動的に変える
```

### 1.2 科学的記数法との対応

```
科学的記数法（10進数）:
  1.989 × 10^30
  ↑       ↑  ↑
  仮数    基数 指数

IEEE 754（2進数）:
  (-1)^s × 1.M × 2^(E-bias)
     ↑      ↑       ↑
   符号   仮数部   指数部

  → 2進数版の科学的記数法
```

---

## 2. IEEE 754 の構造

### 2.1 単精度（32ビット）と倍精度（64ビット）

```
IEEE 754 フォーマット:

  単精度 (float, 32ビット):
  ┌───┬──────────┬───────────────────────┐
  │ S │ 指数部E  │     仮数部M           │
  │1b │  8ビット  │     23ビット           │
  └───┴──────────┴───────────────────────┘
  bit: 31  30...23    22...0

  倍精度 (double, 64ビット):
  ┌───┬─────────────┬────────────────────────────────────────┐
  │ S │  指数部E    │           仮数部M                      │
  │1b │  11ビット    │           52ビット                      │
  └───┴─────────────┴────────────────────────────────────────┘
  bit: 63  62...52       51...0

  比較:
  ┌──────────┬────────┬────────┬──────────────────────────┐
  │ 型       │ 精度   │ 指数   │ 範囲                     │
  ├──────────┼────────┼────────┼──────────────────────────┤
  │ float16  │ 10ビット│ 5ビット│ ±6.55 × 10^4            │
  │ float32  │ 23ビット│ 8ビット│ ±3.4 × 10^38            │
  │ float64  │ 52ビット│ 11ビット│ ±1.8 × 10^308           │
  │ float128 │ 112ビット│ 15ビット│ ±1.2 × 10^4932          │
  └──────────┴────────┴────────┴──────────────────────────┘

  有効桁数（10進数換算）:
  float32:  約7桁
  float64:  約15-16桁
  float128: 約33-34桁
```

### 2.2 値の計算式

```
正規化数の値:
  値 = (-1)^S × (1 + M/2^仮数ビット数) × 2^(E - bias)

  bias（ゲタ履き）:
    float32: bias = 127  (E: 1〜254 → 指数: -126〜+127)
    float64: bias = 1023 (E: 1〜2046 → 指数: -1022〜+1023)

  例: float32 で 6.5 を表現
  ─────────────────────────
  6.5 (10進) = 110.1 (2進)
  正規化: 1.101 × 2^2

  S = 0 (正)
  E = 2 + 127 = 129 = 1000 0001
  M = 101 0000 0000 0000 0000 0000 (1.の後の部分)

  ビット列: 0 10000001 10100000000000000000000
  16進数: 0x40D00000

  検証 (Python):
  import struct
  print(struct.pack('>f', 6.5).hex())  # '40d00000' ✓
```

### 2.3 具体的な変換例

```
float32 で -12.375 を表現:

  ステップ1: 絶対値を2進数に変換
    12 = 1100 (2進)
    0.375:
      0.375 × 2 = 0.75  → 0
      0.75  × 2 = 1.5   → 1
      0.5   × 2 = 1.0   → 1
    0.375 = 0.011 (2進)
    12.375 = 1100.011 (2進)

  ステップ2: 正規化
    1100.011 = 1.100011 × 2^3

  ステップ3: 各フィールドを決定
    S = 1 (負)
    E = 3 + 127 = 130 = 1000 0010
    M = 100 0110 0000 0000 0000 0000

  結果: 1 10000010 10001100000000000000000
  16進数: 0xC1460000
```

---

## 3. 特殊値

### 3.1 ゼロ、無限大、NaN

```
IEEE 754 の特殊値（float32）:

  指数部(E)    仮数部(M)    意味
  ─────────────────────────────────
  0000 0000    0...0        ±0（符号付きゼロ）
  0000 0000    非ゼロ       非正規化数（denormalized）
  1111 1111    0...0        ±∞（無限大）
  1111 1111    非ゼロ       NaN（非数）

  ±0:
    +0.0 == -0.0  → True（比較では等しい）
    1/+0.0 → +Inf
    1/-0.0 → -Inf  （符号が区別される場面がある）

  ±∞:
    1.0 / 0.0 → +Inf
    -1.0 / 0.0 → -Inf
    Inf + Inf → Inf
    Inf - Inf → NaN
    Inf × 0 → NaN

  NaN (Not a Number):
    0.0 / 0.0 → NaN
    sqrt(-1) → NaN
    NaN == NaN → False!  ← 最大の罠
    NaN != NaN → True!
    → NaN判定は専用関数を使う
```

### 3.2 NaN の扱い

```python
import math

x = float('nan')

# NaN の性質
print(x == x)        # False! (NaN は自分自身と等しくない)
print(x != x)        # True!
print(x > 0)         # False
print(x < 0)         # False
print(x == 0)        # False
# → NaN はあらゆる比較で False（!= を除く）

# NaN 判定の正しい方法
print(math.isnan(x))      # True ✓
print(x != x)              # True (イディオム、あまり推奨しない)

# NaN の伝播
print(x + 1)         # nan
print(x * 0)         # nan
print(max(x, 5))     # nan (Python)
# → NaN が1つでも混入すると計算結果が全て NaN になる（毒）

# JavaScript での注意
# typeof NaN === 'number'  ← Number型なのにNot a Number!
# Number.isNaN(NaN) === true
# isNaN("hello") === true  ← グローバルisNaN は型変換する(バグの元)
# Number.isNaN("hello") === false ← こちらを使う
```

---

## 4. 精度問題

### 4.1 なぜ 0.1 + 0.2 ≠ 0.3 なのか

```
0.1 を2進数に変換:

  0.1 × 2 = 0.2 → 0
  0.2 × 2 = 0.4 → 0
  0.4 × 2 = 0.8 → 0
  0.8 × 2 = 1.6 → 1
  0.6 × 2 = 1.2 → 1
  0.2 × 2 = 0.4 → 0  ← ここから繰り返し
  0.4 × 2 = 0.8 → 0
  0.8 × 2 = 1.6 → 1
  ...

  0.1 (10進) = 0.0001100110011001100110011... (2進, 無限循環)

  → 10進数の 0.1 は2進数では正確に表現できない！
  → float64 でも近似値に丸められる

  float64 での 0.1:
  0.1000000000000000055511151231257827021181583404541015625

  float64 での 0.2:
  0.200000000000000011102230246251565404236316680908203125

  0.1 + 0.2 =
  0.3000000000000000444089209850062616169452667236328125

  0.3 =
  0.299999999999999988897769753748434595763683319091796875

  → 0.1 + 0.2 > 0.3 !
```

### 4.2 丸め誤差の蓄積

```python
# 誤差の蓄積例
total = 0.0
for i in range(1000):
    total += 0.1
print(total)        # 99.9999999999986 (100ではない!)
print(total == 100)  # False

# 改善策1: math.fsum（補償加算）
import math
total = math.fsum([0.1] * 1000)
print(total)  # 100.00000000000007 (まだ誤差あり)

# 改善策2: Kahan summation algorithm
def kahan_sum(values):
    total = 0.0
    compensation = 0.0
    for v in values:
        y = v - compensation
        t = total + y
        compensation = (t - total) - y
        total = t
    return total

print(kahan_sum([0.1] * 1000))  # より正確
```

### 4.3 実務での対策

```python
# 1. 浮動小数点の比較: イプシロン比較
import math

def float_equal(a, b, rel_tol=1e-9, abs_tol=1e-12):
    return math.isclose(a, b, rel_tol=rel_tol, abs_tol=abs_tol)

print(float_equal(0.1 + 0.2, 0.3))  # True ✓

# 2. 金融計算: Decimal型
from decimal import Decimal, ROUND_HALF_UP

price = Decimal('19.99')
tax_rate = Decimal('0.08')
tax = (price * tax_rate).quantize(Decimal('0.01'), rounding=ROUND_HALF_UP)
total = price + tax
print(f"税込: {total}")  # 税込: 21.58

# 3. 科学計算: 相対誤差の管理
# NumPyの allclose
import numpy as np
a = np.array([0.1 + 0.2])
b = np.array([0.3])
print(np.allclose(a, b))  # True

# 4. 整数への変換
# 通貨をセント単位の整数で管理
price_cents = 1999  # $19.99
tax_cents = round(price_cents * 0.08)  # 160
total_cents = price_cents + tax_cents  # 2159 = $21.59
```

---

## 5. 浮動小数点の密度分布

### 5.1 数直線上の分布

```
浮動小数点数は均等に分布していない:

  0付近: 非常に密（多くの表現可能な値）
  大きな数: 非常に疎（隣接する値の間隔が大きい）

  float32 の隣接値の間隔（ULP: Unit in the Last Place）:

  値の範囲          ULP（隣接値の差）
  ──────────────────────────────
  1.0 付近          ~1.2 × 10^-7
  1000.0 付近       ~6.1 × 10^-5
  1,000,000 付近    ~0.0625
  16,777,216        1.0 ← ここから整数の精度が失われる！
  10^30 付近        ~10^23

  float32 で正確に表現できる最大の整数:
  2^24 = 16,777,216
  → これ以上の整数は隣接値が2以上離れる
  → 16,777,217 は float32 で正確に表現できない！

  float64 で正確に表現できる最大の整数:
  2^53 = 9,007,199,254,740,992
  → JavaScript の Number.MAX_SAFE_INTEGER
```

### 5.2 非正規化数（Denormalized Numbers）

```
正規化数の最小値付近の問題:

  正規化数: 1.M × 2^E
  → 仮数部は常に 1.xxx... (暗黙の1)
  → 最小の正規化数: 1.000...0 × 2^(-126) ≈ 1.18 × 10^(-38) [float32]

  非正規化数（指数部 = 0 のとき）:
  → 暗黙の1がなくなる: 0.M × 2^(-126)
  → 最小の非正規化数: 0.000...1 × 2^(-126) = 2^(-149) ≈ 1.4 × 10^(-45)

  目的: 0への段階的アンダーフロー（gradual underflow）
  → ゼロとの間に「隙間」を作らない
  → a - b == 0 ↔ a == b が保証される

  ┌──────┬────────────────┬──────────────────┬──────┐
  │ -∞   │  負の正規化数   │ 非正規化数(疎→密) │  0   │
  └──────┴────────────────┴──────────────────┴──────┘
                                               ↕
  ┌──────┬──────────────────┬────────────────┬──────┐
  │  0   │ 非正規化数(密→疎) │  正の正規化数   │  +∞  │
  └──────┴──────────────────┴────────────────┴──────┘
```

---

## 6. AI/GPU と低精度浮動小数点

### 6.1 各精度フォーマット

```
AI時代の浮動小数点フォーマット:

  FP64 (double):     1 + 11 + 52 = 64ビット  科学計算
  FP32 (float):      1 +  8 + 23 = 32ビット  汎用
  TF32 (TensorFloat): 1 +  8 + 10 = 19ビット  NVIDIA A100以降
  BF16 (bfloat16):   1 +  8 +  7 = 16ビット  Google TPU / PyTorch
  FP16 (half):       1 +  5 + 10 = 16ビット  推論、モバイルAI
  FP8 (E4M3):        1 +  4 +  3 =  8ビット  学習（H100以降）
  FP8 (E5M2):        1 +  5 +  2 =  8ビット  勾配（H100以降）
  INT8:              8ビット整数              量子化推論
  INT4:              4ビット整数              極限量子化

  なぜ低精度で学習できるのか:
  - ニューラルネットワークはノイズに強い
  - 勾配の「方向」が合っていれば精度は粗くてよい
  - メモリ帯域幅がボトルネック → 小さいデータ → 高速
  - FP16で学習 → FP32比で2倍のバッチサイズ → 学習高速化

  混合精度学習（Mixed Precision Training）:
  - 重み: FP32で保持（マスターコピー）
  - 順伝播: FP16で計算（高速）
  - 勾配: FP16で計算
  - 更新: FP32に変換して加算（精度維持）
```

---

## 7. 実践演習

### 演習1: IEEE 754 変換（基礎）
以下の値の float32 ビット表現を手計算で求めよ:
1. -0.75
2. 100.0
3. 0.1（近似値でよい）

### 演習2: 精度限界の体験（応用）
Pythonで以下を検証せよ:
1. `float(2**53) == float(2**53 + 1)` の結果と理由
2. `1e20 + 1 - 1e20` と `1e20 - 1e20 + 1` の結果の違い
3. `0.1 + 0.1 + 0.1 - 0.3` と `0.3 - 0.1 - 0.1 - 0.1` の結果の違い

### 演習3: 安全な浮動小数点比較（発展）
相対誤差と絶対誤差の両方を考慮した浮動小数点比較関数を実装し、エッジケース（NaN, Inf, -0, 非常に小さい値）を全て正しく処理せよ。

---

## FAQ

### Q1: なぜ float と double の2つがあるのですか？
**A**: 精度とメモリ/速度のトレードオフ。float(32bit)は約7桁の精度でメモリ効率が良い。double(64bit)は約15桁の精度で科学計算に十分。GPU/AIではfloat16やbfloat16も使用。ほとんどの汎用プログラミングではdoubleがデフォルト。

### Q2: 銀行家の丸め（Banker's Rounding）とは？
**A**: 0.5の丸めを「最近接偶数」に丸める方法。IEEE 754のデフォルト丸めモード。0.5→0, 1.5→2, 2.5→2, 3.5→4。統計的な偏りを防ぐ。通常の四捨五入は+方向に偏る。

### Q3: JavaScriptにはなぜ整数型がないのですか？
**A**: 設計当初（1995年）にシンプルさを優先した結果。全ての数値がfloat64。2^53までの整数は正確に表現できるが、それ以上はBigIntを使用。TypeScriptでもnumber型は内部的にfloat64。

---

## まとめ

| 概念 | ポイント |
|------|---------|
| IEEE 754 | 符号 + 指数部 + 仮数部。32/64ビット |
| 精度問題 | 0.1は2進数で無限循環。比較にはイプシロン使用 |
| 特殊値 | NaN ≠ NaN、±0、±∞。判定には専用関数を使う |
| 密度 | 0付近が密、大きな数は疎。float32は2^24まで整数精度 |
| AI精度 | BF16/FP16/FP8で高速化。混合精度学習が主流 |

---

## 次に読むべきガイド
→ [[04-compression-algorithms.md]] — 圧縮アルゴリズム

---

## 参考文献
1. Goldberg, D. "What Every Computer Scientist Should Know About Floating-Point Arithmetic." ACM Computing Surveys, 1991.
2. IEEE. "IEEE 754-2019 Standard for Floating-Point Arithmetic."
3. Kahan, W. "How Java's Floating-Point Hurts Everyone Everywhere." 1998.
4. Micikevicius, P. et al. "Mixed Precision Training." ICLR 2018.
