# 2進数と数値表現

> コンピュータの世界では全てが0と1で表現される。この「制約」こそが、デジタル技術の信頼性と普遍性を生み出す源泉である。

## この章で学ぶこと

- [ ] 2進数、8進数、16進数の変換ができる
- [ ] ビット演算（AND, OR, XOR, NOT, シフト）を使いこなせる
- [ ] なぜコンピュータが2進数を採用しているか説明できる

## 前提知識

- 基本的な数学（四則演算）

---

## 1. なぜ2進数か

### 1.1 物理的な理由

```
トランジスタ = スイッチ:

  ON  = 電圧高い = 1
  OFF = 電圧低い = 0

  2つの状態しかないので「2進数」が自然

  もし3進数を使おうとすると:
  - 電圧を3段階に正確に区別する必要がある
  - ノイズに弱くなる（境界が2箇所）
  - 回路が複雑化

  2進数の利点:
  - ノイズに強い（しきい値が1つだけ）
  - 回路が単純（ON/OFFだけ）
  - 論理演算とビット演算が直接対応
```

### 1.2 基数変換

```
10進数 ⇔ 2進数 ⇔ 16進数:

  10進数  2進数          16進数   8進数
  ──────────────────────────────────────
     0    0000 0000      0x00     000
     1    0000 0001      0x01     001
    10    0000 1010      0x0A     012
    42    0010 1010      0x2A     052
   127    0111 1111      0x7F     177
   128    1000 0000      0x80     200
   255    1111 1111      0xFF     377
   256    1 0000 0000    0x100    400

  変換方法（10進→2進）: 2で割り続けて余りを逆から並べる
  42 ÷ 2 = 21 余り 0
  21 ÷ 2 = 10 余り 1
  10 ÷ 2 =  5 余り 0
   5 ÷ 2 =  2 余り 1
   2 ÷ 2 =  1 余り 0
   1 ÷ 2 =  0 余り 1
  → 101010 (2) = 42 (10) ✓

  16進数は2進数の4ビットまとめ:
  0010 1010 → 2  A → 0x2A ✓
```

---

## 2. ビット演算

### 2.1 基本演算

```python
# Pythonでのビット演算

# AND: 両方1なら1
a = 0b1100  # 12
b = 0b1010  # 10
print(bin(a & b))   # 0b1000 = 8

# OR: どちらか1なら1
print(bin(a | b))   # 0b1110 = 14

# XOR: 異なれば1
print(bin(a ^ b))   # 0b0110 = 6

# NOT: ビット反転
print(bin(~a & 0xFF))  # 0b11110011 = 243 (8ビットの場合)

# 左シフト: 2倍（各ビットを左にずらす）
print(bin(a << 1))  # 0b11000 = 24 (12 × 2)
print(bin(a << 3))  # 0b1100000 = 96 (12 × 8)

# 右シフト: 半分（各ビットを右にずらす）
print(bin(a >> 1))  # 0b110 = 6 (12 ÷ 2)
```

### 2.2 ビット演算の実務応用

```python
# ビット演算の実務的なユースケース

# 1. フラグ管理（ビットフィールド）
READ    = 0b001  # 1
WRITE   = 0b010  # 2
EXECUTE = 0b100  # 4

permissions = READ | WRITE  # 0b011 = 3
has_read = bool(permissions & READ)     # True
has_execute = bool(permissions & EXECUTE)  # False

# ファイルパーミッション: chmod 755 = rwxr-xr-x
# 7 = 111, 5 = 101, 5 = 101

# 2. 偶奇判定（最下位ビット）
is_even = (n & 1) == 0  # n % 2 == 0 と同じだが高速

# 3. 2の冪乗判定
is_power_of_2 = n > 0 and (n & (n - 1)) == 0
# 8 = 1000, 7 = 0111 → 1000 & 0111 = 0000 → True
# 6 = 0110, 5 = 0101 → 0110 & 0101 = 0100 → False

# 4. XORスワップ（一時変数なしで交換）
a ^= b
b ^= a
a ^= b
# 理論的に面白いが、実用では temp = a; a = b; b = temp が読みやすい

# 5. ビットマスク
ip_address = 0xC0A80164  # 192.168.1.100
subnet_mask = 0xFFFFFF00  # 255.255.255.0
network = ip_address & subnet_mask  # 192.168.1.0
```

---

## 3. データサイズの単位

```
ビットとバイトの階層:

  1 bit    = 0 or 1
  1 nibble = 4 bits   = 16進数1桁
  1 byte   = 8 bits   = 256通りの値（0-255）
  1 word   = 32 or 64 bits（CPU依存）

  ストレージ単位:
  ┌────────────┬──────────────┬───────────────────────┐
  │ 単位       │ 10進（SI）   │ 2進（IEC）             │
  ├────────────┼──────────────┼───────────────────────┤
  │ キロ (K)   │ 1,000        │ 1,024 (2^10) = KiB    │
  │ メガ (M)   │ 1,000,000    │ 1,048,576 (2^20) = MiB│
  │ ギガ (G)   │ 10^9         │ 2^30 = GiB            │
  │ テラ (T)   │ 10^12        │ 2^40 = TiB            │
  │ ペタ (P)   │ 10^15        │ 2^50 = PiB            │
  │ エクサ (E) │ 10^18        │ 2^60 = EiB            │
  └────────────┴──────────────┴───────────────────────┘

  注意: HDD/SSDメーカーはSI単位を使い、OSはIEC単位を使う
  → 1TB SSD がOS上で 931GB と表示される
  → 1,000,000,000,000 / 1,073,741,824 ≈ 931 GiB
```

---

## 4. 各プログラミング言語での数値リテラル

```python
# Python: 接頭辞で基数を指定
decimal = 42        # 10進数
binary  = 0b101010  # 2進数
octal   = 0o52      # 8進数
hexadec = 0x2A      # 16進数
# 全て42
```

```javascript
// JavaScript: 同様の接頭辞
const decimal = 42;
const binary  = 0b101010;
const octal   = 0o52;
const hexadec = 0x2A;
// BigInt: 大きな整数
const big = 9007199254740993n;  // 'n' サフィックス
```

```rust
// Rust: 型アノテーション + アンダースコア区切り
let decimal: u32 = 42;
let binary: u32  = 0b0010_1010;  // アンダースコアで視認性向上
let octal: u32   = 0o52;
let hexadec: u32 = 0x2A;
let byte: u8     = b'A';  // ASCII値 (65)
```

---

## 5. 実践演習

### 演習1: 基数変換（基礎）
以下の変換を手計算で行え:
1. 10進数 `173` → 2進数 → 16進数
2. 16進数 `0xDEAD` → 10進数
3. 2進数 `1011 0110` → 10進数 → 8進数

### 演習2: ビット演算パズル（応用）
ビット演算のみを使って以下を実装せよ（算術演算禁止）:
1. 2つの整数の加算
2. 整数のセットビット数（1の数）をカウント

### 演習3: IPアドレス計算（発展）
IPv4アドレス `192.168.10.50` とサブネットマスク `255.255.255.0 (/24)` から、ビット演算でネットワークアドレスとブロードキャストアドレスを求めよ。

---

## FAQ

### Q1: 16進数はなぜプログラミングで多用されるのですか？
**A**: 2進数の4ビットが16進数の1桁に正確に対応するため。`0xFF` は `1111 1111` と即座に分かるが、`255` からは直感的に分からない。メモリアドレス、色コード (#FF0000)、バイト列の表現に便利。

### Q2: 3進法コンピュータは存在しましたか？
**A**: ソ連のSetun（1958年）が三進法コンピュータとして有名。理論的には3進法は情報効率が最も高い（自然対数の底eに最も近い整数が3）。しかし、実用的にはトランジスタの2状態スイッチングの信頼性が圧倒的に高く、2進法が標準となった。

### Q3: ビット演算は実務でどのくらい使いますか？
**A**: 分野による。Web開発ではほぼ使わない。システムプログラミング、ネットワーク（IPマスク）、暗号、ゲーム開発、組み込みでは頻繁に使う。「使わない」場合でも、原理を理解していることで、パフォーマンス最適化やバグ解析に役立つ。

---

## まとめ

| 概念 | ポイント |
|------|---------|
| 2進数 | トランジスタのON/OFFに直結。ノイズ耐性が高い |
| 16進数 | 2進数の4ビット=1桁。メモリ/バイト列の表記に標準 |
| ビット演算 | AND, OR, XOR, NOT, シフト。フラグ管理、最適化に使用 |
| 単位 | SI(KB=1000)とIEC(KiB=1024)の2系統がある |

---

## 次に読むべきガイド
→ [[01-character-encoding.md]] — 文字コードとUnicode

---

## 参考文献
1. Petzold, C. "Code: The Hidden Language of Computer Hardware and Software." 2nd Edition, Microsoft Press, 2022.
2. Warren, H. S. "Hacker's Delight." 2nd Edition, Addison-Wesley, 2012.
3. Bryant, R. E. & O'Hallaron, D. R. "Computer Systems: A Programmer's Perspective." Chapter 2.
