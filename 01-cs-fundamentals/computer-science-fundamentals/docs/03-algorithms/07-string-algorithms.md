# æ–‡å­—åˆ—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

> ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã¯æœ€ã‚‚èº«è¿‘ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®1ã¤ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®Ctrl+Fã€grepã‚³ãƒãƒ³ãƒ‰ã€IDEã®æ¤œç´¢â€”â€”å…¨ã¦ãŒæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã«ä¾å­˜ã—ã¦ã„ã‚‹ã€‚

## ã“ã®ç« ã§å­¦ã¶ã“ã¨

- [ ] ãƒŠã‚¤ãƒ¼ãƒ–ãªæ–‡å­—åˆ—æ¤œç´¢ã®å•é¡Œç‚¹ã‚’ç†è§£ã™ã‚‹
- [ ] KMPæ³•ã®åŸç†ã‚’èª¬æ˜ã§ãã‚‹
- [ ] Rabin-Karpæ³•ã®ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒãƒƒã‚·ãƒ¥ã‚’ç†è§£ã™ã‚‹
- [ ] Boyer-Mooreæ³•ã®ã‚¹ã‚­ãƒƒãƒ—æˆ¦ç•¥ã‚’ç†è§£ã™ã‚‹
- [ ] Trieã€ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—ãªã©ã®æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’å®Ÿè£…ã§ãã‚‹
- [ ] æ­£è¦è¡¨ç¾ã¨ã‚ªãƒ¼ãƒˆãƒãƒˆãƒ³ã®é–¢ä¿‚ã‚’ç†è§£ã™ã‚‹
- [ ] å®Ÿå‹™ã§ã®æ–‡å­—åˆ—å‡¦ç†ã®æœ€é©åŒ–æ‰‹æ³•ã‚’ç¿’å¾—ã™ã‚‹

## å‰æçŸ¥è­˜

- è¨ˆç®—é‡è§£æ â†’ å‚ç…§: [[01-complexity-analysis.md]]
- æ–‡å­—ã‚³ãƒ¼ãƒ‰ â†’ å‚ç…§: [[../02-data-representation/01-character-encoding.md]]

---

## 1. æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°

### 1.1 ãƒŠã‚¤ãƒ¼ãƒ–æ³•

```python
def naive_search(text, pattern):
    """ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ºç¾ä½ç½®ã‚’å…¨ã¦è¿”ã™"""
    n, m = len(text), len(pattern)
    positions = []
    for i in range(n - m + 1):
        if text[i:i+m] == pattern:
            positions.append(i)
    return positions

# è¨ˆç®—é‡: O(n Ã— m) â€” æœ€æ‚ªã‚±ãƒ¼ã‚¹
# ä¾‹: text="AAAAAB", pattern="AAB"
# æ¯å›ã»ã¼mæ–‡å­—æ¯”è¼ƒã—ã¦ã‹ã‚‰ä¸ä¸€è‡´ã‚’æ¤œå‡º

# æœ€æ‚ªã‚±ãƒ¼ã‚¹ã®ä¾‹:
# text = "A" * 1000000 + "B"
# pattern = "A" * 999 + "B"
# â†’ ç´„100ä¸‡å› Ã— 1000æ–‡å­—æ¯”è¼ƒ = 10å„„å›ã®æ¯”è¼ƒ
```

### 1.2 KMPæ³•ï¼ˆKnuth-Morris-Prattï¼‰

```python
def kmp_search(text, pattern):
    """å¤±æ•—é–¢æ•°ã‚’ä½¿ã£ã¦ç„¡é§„ãªæ¯”è¼ƒã‚’çœã"""
    # å¤±æ•—é–¢æ•°ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã®æ§‹ç¯‰
    m = len(pattern)
    failure = [0] * m
    j = 0
    for i in range(1, m):
        while j > 0 and pattern[i] != pattern[j]:
            j = failure[j - 1]
        if pattern[i] == pattern[j]:
            j += 1
        failure[i] = j

    # æ¤œç´¢
    positions = []
    j = 0
    for i in range(len(text)):
        while j > 0 and text[i] != pattern[j]:
            j = failure[j - 1]  # å¤±æ•—é–¢æ•°ã§ã‚¸ãƒ£ãƒ³ãƒ—
        if text[i] == pattern[j]:
            j += 1
        if j == m:
            positions.append(i - m + 1)
            j = failure[j - 1]

    return positions

# è¨ˆç®—é‡: O(n + m) â€” ç·šå½¢æ™‚é–“ï¼
# å¤±æ•—é–¢æ•°ã®æ§‹ç¯‰: O(m)
# æ¤œç´¢: O(n)

# KMPã®æ ¸å¿ƒ:
# ä¸ä¸€è‡´ãŒèµ·ããŸæ™‚ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã€Œæ¥é ­è¾ã¨æ¥å°¾è¾ã®ä¸€è‡´ã€ã‚’åˆ©ç”¨ã—ã¦
# ãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚¤ãƒ³ã‚¿ã‚’æˆ»ã•ãšã«ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚¤ãƒ³ã‚¿ã ã‘ã‚’èª¿æ•´ã™ã‚‹
```

#### KMPã®å¤±æ•—é–¢æ•°ã®è©³ç´°è§£èª¬

```python
def build_failure_function(pattern):
    """å¤±æ•—é–¢æ•°ã‚’æ§‹ç¯‰ã—ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’å¯è¦–åŒ–"""
    m = len(pattern)
    failure = [0] * m
    j = 0

    print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern}")
    print(f"{'i':>3} {'pattern[i]':>10} {'j':>3} {'ä¸€è‡´?':>5} {'failure[i]':>10}")
    print("-" * 40)

    for i in range(1, m):
        while j > 0 and pattern[i] != pattern[j]:
            j = failure[j - 1]

        match = pattern[i] == pattern[j]
        if match:
            j += 1

        failure[i] = j
        print(f"{i:>3} {pattern[i]:>10} {j:>3} {'YES' if match else 'NO':>5} {failure[i]:>10}")

    return failure

# ä¾‹: ãƒ‘ã‚¿ãƒ¼ãƒ³ "ABABAC"
# failure = [0, 0, 1, 2, 3, 0]
#
# è§£é‡ˆ:
# failure[i] = pattern[0:i+1] ã®æœ€é•·ã®ã€Œä¸€è‡´ã™ã‚‹æ¥é ­è¾ã¨æ¥å°¾è¾ã€ã®é•·ã•
#
# i=0: "A"       â†’ 0 (è‡ªæ˜)
# i=1: "AB"      â†’ 0 (æ¥é ­è¾ "A" â‰  æ¥å°¾è¾ "B")
# i=2: "ABA"     â†’ 1 (æ¥é ­è¾ "A" = æ¥å°¾è¾ "A")
# i=3: "ABAB"    â†’ 2 (æ¥é ­è¾ "AB" = æ¥å°¾è¾ "AB")
# i=4: "ABABA"   â†’ 3 (æ¥é ­è¾ "ABA" = æ¥å°¾è¾ "ABA")
# i=5: "ABABAC"  â†’ 0 (ä¸€è‡´ãªã—)
```

#### KMPæ³•ã®å‹•ä½œå¯è¦–åŒ–

```python
def kmp_search_verbose(text, pattern):
    """KMPæ³•ã®å‹•ä½œã‚’å¯è¦–åŒ–"""
    m = len(pattern)
    failure = [0] * m
    j = 0
    for i in range(1, m):
        while j > 0 and pattern[i] != pattern[j]:
            j = failure[j - 1]
        if pattern[i] == pattern[j]:
            j += 1
        failure[i] = j

    print(f"å¤±æ•—é–¢æ•°: {failure}")
    print(f"ãƒ†ã‚­ã‚¹ãƒˆ:   {text}")
    print()

    positions = []
    j = 0
    comparisons = 0

    for i in range(len(text)):
        while j > 0 and text[i] != pattern[j]:
            print(f"  ä¸ä¸€è‡´ text[{i}]='{text[i]}' vs pattern[{j}]='{pattern[j]}'"
                  f" â†’ j ã‚’ {failure[j-1]} ã«å·»ãæˆ»ã—")
            j = failure[j - 1]
            comparisons += 1

        comparisons += 1
        if text[i] == pattern[j]:
            j += 1
            if j == m:
                positions.append(i - m + 1)
                print(f"  â˜… ãƒãƒƒãƒï¼ ä½ç½® {i - m + 1}")
                j = failure[j - 1]
        else:
            pass

    print(f"\næ¯”è¼ƒå›æ•°: {comparisons}")
    print(f"ãƒŠã‚¤ãƒ¼ãƒ–æ³•ã®æœ€æ‚ª: {len(text) * len(pattern)}")
    return positions

# ä½¿ç”¨ä¾‹
kmp_search_verbose("ABABDABABABABAC", "ABABAC")
```

### 1.3 Rabin-Karpæ³•

```python
def rabin_karp(text, pattern):
    """ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒãƒƒã‚·ãƒ¥ã§é«˜é€Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
    n, m = len(text), len(pattern)
    base, mod = 256, 10**9 + 7
    positions = []

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒãƒƒã‚·ãƒ¥å€¤
    pattern_hash = 0
    text_hash = 0
    h = pow(base, m - 1, mod)

    for i in range(m):
        pattern_hash = (pattern_hash * base + ord(pattern[i])) % mod
        text_hash = (text_hash * base + ord(text[i])) % mod

    for i in range(n - m + 1):
        if text_hash == pattern_hash:
            if text[i:i+m] == pattern:  # ãƒãƒƒã‚·ãƒ¥è¡çªãƒã‚§ãƒƒã‚¯
                positions.append(i)
        if i < n - m:
            text_hash = ((text_hash - ord(text[i]) * h) * base
                        + ord(text[i + m])) % mod

    return positions

# è¨ˆç®—é‡: O(n + m) æœŸå¾… / O(nm) æœ€æ‚ªï¼ˆãƒãƒƒã‚·ãƒ¥è¡çªæ™‚ï¼‰
# åˆ©ç‚¹: è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åŒæ™‚æ¤œç´¢ã«å¼·ã„
```

#### è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åŒæ™‚æ¤œç´¢

```python
def rabin_karp_multi(text, patterns):
    """è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŒæ™‚ã«æ¤œç´¢"""
    n = len(text)
    base, mod = 256, 10**9 + 7
    results = {p: [] for p in patterns}

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é•·ã•ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    by_length = {}
    for p in patterns:
        m = len(p)
        if m not in by_length:
            by_length[m] = {}
        # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—
        h = 0
        for c in p:
            h = (h * base + ord(c)) % mod
        by_length[m][h] = by_length[m].get(h, []) + [p]

    # å„é•·ã•ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«æ¤œç´¢
    for m, hash_to_patterns in by_length.items():
        if m > n:
            continue

        h_pow = pow(base, m - 1, mod)

        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤
        text_hash = 0
        for i in range(m):
            text_hash = (text_hash * base + ord(text[i])) % mod

        for i in range(n - m + 1):
            if text_hash in hash_to_patterns:
                for p in hash_to_patterns[text_hash]:
                    if text[i:i+m] == p:
                        results[p].append(i)

            if i < n - m:
                text_hash = ((text_hash - ord(text[i]) * h_pow) * base
                            + ord(text[i + m])) % mod

    return results

# ä½¿ç”¨ä¾‹
text = "she sells seashells by the seashore"
patterns = ["she", "sea", "sell", "shore"]
result = rabin_karp_multi(text, patterns)
# {'she': [0, 15], 'sea': [10, 27], 'sell': [4], 'shore': [30]}
```

### 1.4 Boyer-Mooreæ³•

```python
def boyer_moore(text, pattern):
    """Boyer-Mooreæ³•: ãƒ‘ã‚¿ãƒ¼ãƒ³æœ«å°¾ã‹ã‚‰æ¯”è¼ƒã—ã€ä¸ä¸€è‡´æ™‚ã«å¤§ããã‚¹ã‚­ãƒƒãƒ—"""
    n, m = len(text), len(pattern)
    if m > n:
        return []

    # Bad Character ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹ç¯‰
    bad_char = {}
    for i in range(m):
        bad_char[pattern[i]] = i  # å„æ–‡å­—ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å†…ã§ã®æœ€å¾Œã®å‡ºç¾ä½ç½®

    positions = []
    i = 0  # ãƒ†ã‚­ã‚¹ãƒˆä¸Šã®ä½ç½®

    while i <= n - m:
        j = m - 1  # ãƒ‘ã‚¿ãƒ¼ãƒ³æœ«å°¾ã‹ã‚‰æ¯”è¼ƒ

        while j >= 0 and pattern[j] == text[i + j]:
            j -= 1

        if j < 0:
            # ãƒãƒƒãƒï¼
            positions.append(i)
            # æ¬¡ã®ãƒãƒƒãƒã‚’æ¢ã™
            i += (m - bad_char.get(text[i + m], -1) if i + m < n else 1)
        else:
            # Bad Characterãƒ«ãƒ¼ãƒ«
            bad_pos = bad_char.get(text[i + j], -1)
            shift = max(1, j - bad_pos)
            i += shift

    return positions

# Boyer-Mooreã®ç‰¹å¾´:
# - ãƒ‘ã‚¿ãƒ¼ãƒ³æœ«å°¾ã‹ã‚‰æ¯”è¼ƒï¼ˆå³â†’å·¦ï¼‰
# - ä¸ä¸€è‡´æ–‡å­—ãŒãƒ‘ã‚¿ãƒ¼ãƒ³ã«å«ã¾ã‚Œãªã‘ã‚Œã°ã€ãƒ‘ã‚¿ãƒ¼ãƒ³é•·åˆ†ã‚¹ã‚­ãƒƒãƒ—
# - æœ€å–„ã‚±ãƒ¼ã‚¹: O(n/m) â€” ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé•·ã„ã»ã©é«˜é€Ÿï¼
# - æœ€æ‚ªã‚±ãƒ¼ã‚¹: O(nm) â€” Good Suffixãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ ã™ã‚‹ã¨O(n)ã«æ”¹å–„
# - å®Ÿç”¨ä¸Šæœ€ã‚‚é«˜é€Ÿãªæ–‡å­—åˆ—æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆGNU grepãŒæ¡ç”¨ï¼‰

# Boyer-Mooreã®å‹•ä½œä¾‹:
# text = "HERE IS A SIMPLE EXAMPLE"
# pattern = "EXAMPLE"
#
# ã‚¹ãƒ†ãƒƒãƒ—1: E ã¨ S ã‚’æ¯”è¼ƒ â†’ ä¸ä¸€è‡´ã€Sã¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç„¡ã„ â†’ 7æ–‡å­—ã‚¹ã‚­ãƒƒãƒ—
# ã‚¹ãƒ†ãƒƒãƒ—2: æ¬¡ã®ä½ç½®ã§æ¯”è¼ƒ â†’ ...
# â†’ ãƒŠã‚¤ãƒ¼ãƒ–æ³•ã‚ˆã‚Šå¤§å¹…ã«å°‘ãªã„æ¯”è¼ƒå›æ•°ã§æ¸ˆã‚€
```

#### Good Suffix ãƒ«ãƒ¼ãƒ«ã®å®Ÿè£…

```python
def boyer_moore_full(text, pattern):
    """Boyer-Mooreæ³•ã®ãƒ•ãƒ«å®Ÿè£…ï¼ˆBad Character + Good Suffixï¼‰"""
    n, m = len(text), len(pattern)
    if m > n:
        return []

    # Bad Character ãƒ†ãƒ¼ãƒ–ãƒ«
    bad_char = [-1] * 256
    for i in range(m):
        bad_char[ord(pattern[i])] = i

    # Good Suffix ãƒ†ãƒ¼ãƒ–ãƒ«
    good_suffix = [0] * (m + 1)
    border = [0] * (m + 1)

    # Case 1: ãƒ‘ã‚¿ãƒ¼ãƒ³å†…ã«good suffixã®ä¸€è‡´ãŒã‚ã‚‹å ´åˆ
    i = m
    j = m + 1
    border[i] = j
    while i > 0:
        while j <= m and pattern[i - 1] != pattern[j - 1]:
            if good_suffix[j] == 0:
                good_suffix[j] = j - i
            j = border[j]
        i -= 1
        j -= 1
        border[i] = j

    # Case 2: good suffixã®æ¥é ­è¾ãŒãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¥é ­è¾ã«ä¸€è‡´ã™ã‚‹å ´åˆ
    j = border[0]
    for i in range(m + 1):
        if good_suffix[i] == 0:
            good_suffix[i] = j
        if i == j:
            j = border[j]

    # æ¤œç´¢
    positions = []
    i = 0
    while i <= n - m:
        j = m - 1
        while j >= 0 and pattern[j] == text[i + j]:
            j -= 1

        if j < 0:
            positions.append(i)
            i += good_suffix[0]
        else:
            bc_shift = j - bad_char[ord(text[i + j])]
            gs_shift = good_suffix[j + 1]
            i += max(bc_shift, gs_shift)

    return positions

# è¨ˆç®—é‡: O(n + m) å‰å‡¦ç† + O(n) æ¤œç´¢
# æœ€å–„: O(n/m) â€” ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé•·ã„ã»ã©é«˜é€Ÿ
```

### 1.5 Aho-Corasickæ³•

```python
from collections import deque

class AhoCorasick:
    """Aho-Corasickæ³•: è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åŒæ™‚æ¤œç´¢ï¼ˆç·šå½¢æ™‚é–“ï¼‰"""

    def __init__(self):
        self.goto = [{}]
        self.failure = [0]
        self.output = [[]]

    def add_pattern(self, pattern, idx):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ """
        state = 0
        for char in pattern:
            if char not in self.goto[state]:
                self.goto[state][char] = len(self.goto)
                self.goto.append({})
                self.failure.append(0)
                self.output.append([])
            state = self.goto[state][char]
        self.output[state].append(idx)

    def build(self):
        """å¤±æ•—é–¢æ•°ã‚’æ§‹ç¯‰ï¼ˆBFSï¼‰"""
        queue = deque()

        # æ·±ã•1ã®ãƒãƒ¼ãƒ‰ã®å¤±æ•—é–¢æ•°ã¯0ï¼ˆãƒ«ãƒ¼ãƒˆï¼‰
        for char, state in self.goto[0].items():
            queue.append(state)

        while queue:
            u = queue.popleft()
            for char, v in self.goto[u].items():
                queue.append(v)

                # å¤±æ•—é–¢æ•°ã®è¨ˆç®—
                state = self.failure[u]
                while state != 0 and char not in self.goto[state]:
                    state = self.failure[state]
                self.failure[v] = self.goto[state].get(char, 0)
                if self.failure[v] == v:
                    self.failure[v] = 0

                # å‡ºåŠ›é–¢æ•°ã®æ›´æ–°
                self.output[v] = self.output[v] + self.output[self.failure[v]]

    def search(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å…¨ã¦æ¤œç´¢"""
        results = []
        state = 0

        for i, char in enumerate(text):
            while state != 0 and char not in self.goto[state]:
                state = self.failure[state]
            state = self.goto[state].get(char, 0)

            for pattern_idx in self.output[state]:
                results.append((i, pattern_idx))

        return results

# ä½¿ç”¨ä¾‹
ac = AhoCorasick()
patterns = ["he", "she", "his", "hers"]
for i, p in enumerate(patterns):
    ac.add_pattern(p, i)
ac.build()

text = "ahishers"
results = ac.search(text)
for pos, idx in results:
    p = patterns[idx]
    print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ '{p}' ãŒä½ç½® {pos - len(p) + 1} ã§ç™ºè¦‹")

# è¨ˆç®—é‡: O(n + m + z)
#   n: ãƒ†ã‚­ã‚¹ãƒˆé•·ã€m: å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç·æ–‡å­—æ•°ã€z: ãƒãƒƒãƒæ•°
# ç”¨é€”:
# - ã‚¦ã‚¤ãƒ«ã‚¹ã‚¹ã‚­ãƒ£ãƒŠï¼ˆã‚·ã‚°ãƒãƒãƒ£ã®æ¤œç´¢ï¼‰
# - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯IDSã®ãƒ‘ã‚±ãƒƒãƒˆæ¤œæŸ»
# - ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆNGãƒ¯ãƒ¼ãƒ‰æ¤œå‡ºï¼‰
# - DNAé…åˆ—ã®è¤‡æ•°ãƒ¢ãƒãƒ¼ãƒ•æ¤œç´¢
```

### 1.6 Z-Algorithm

```python
def z_function(s):
    """Zé…åˆ—ã‚’è¨ˆç®—: z[i] = s[i:]ã¨s[0:]ã®æœ€é•·å…±é€šæ¥é ­è¾ã®é•·ã•"""
    n = len(s)
    z = [0] * n
    z[0] = n
    l, r = 0, 0

    for i in range(1, n):
        if i < r:
            z[i] = min(r - i, z[i - l])
        while i + z[i] < n and s[z[i]] == s[i + z[i]]:
            z[i] += 1
        if i + z[i] > r:
            l, r = i, i + z[i]

    return z

def z_search(text, pattern):
    """Z-Algorithmã‚’ä½¿ã£ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
    # ãƒ‘ã‚¿ãƒ¼ãƒ³$ãƒ†ã‚­ã‚¹ãƒˆ ã‚’é€£çµ
    concat = pattern + "$" + text
    z = z_function(concat)
    m = len(pattern)

    positions = []
    for i in range(m + 1, len(concat)):
        if z[i] == m:
            positions.append(i - m - 1)

    return positions

# è¨ˆç®—é‡: O(n + m)
# KMPã¨åŒã˜ç·šå½¢æ™‚é–“ã ãŒã€å®Ÿè£…ãŒç›´æ„Ÿçš„
# Zé…åˆ—ã¯æ–‡å­—åˆ—ã®ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºã«ã‚‚æœ‰ç”¨

# Zé…åˆ—ã®ä½¿ç”¨ä¾‹: æœ€å°å‘¨æœŸã®æ¤œå‡º
def min_period(s):
    """æ–‡å­—åˆ—ã®æœ€å°å‘¨æœŸã‚’æ±‚ã‚ã‚‹"""
    z = z_function(s)
    n = len(s)
    for period in range(1, n + 1):
        if n % period == 0 and z[period] == n - period:
            return period
    return n

# ä¾‹: "abcabc" â†’ æœ€å°å‘¨æœŸ 3 ("abc")
# ä¾‹: "abab" â†’ æœ€å°å‘¨æœŸ 2 ("ab")
# ä¾‹: "abcde" â†’ æœ€å°å‘¨æœŸ 5 (å…¨ä½“)
```

---

## 2. æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

### 2.1 Trieï¼ˆãƒˆãƒ©ã‚¤æœ¨ï¼‰

```python
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_end = False
        self.count = 0  # ã“ã®æ¥é ­è¾ã‚’æŒã¤å˜èªã®æ•°

class Trie:
    """å‰ç½®æœ¨ â€” æ–‡å­—åˆ—ã®é›†åˆã‚’åŠ¹ç‡çš„ã«ç®¡ç†"""
    def __init__(self):
        self.root = TrieNode()

    def insert(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
            node.count += 1
        node.is_end = True

    def search(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.is_end

    def starts_with(self, prefix):
        node = self.root
        for char in prefix:
            if char not in node.children:
                return False
            node = node.children[char]
        return True

    def count_prefix(self, prefix):
        """prefixã§å§‹ã¾ã‚‹å˜èªã®æ•°"""
        node = self.root
        for char in prefix:
            if char not in node.children:
                return 0
            node = node.children[char]
        return node.count

    def autocomplete(self, prefix, limit=10):
        """ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆ: prefixã§å§‹ã¾ã‚‹å˜èªã‚’è¿”ã™"""
        node = self.root
        for char in prefix:
            if char not in node.children:
                return []
            node = node.children[char]

        results = []
        self._collect_words(node, prefix, results, limit)
        return results

    def _collect_words(self, node, current, results, limit):
        if len(results) >= limit:
            return
        if node.is_end:
            results.append(current)
        for char in sorted(node.children):
            self._collect_words(node.children[char], current + char, results, limit)

    def delete(self, word):
        """å˜èªã‚’å‰Šé™¤"""
        def _delete(node, word, depth):
            if depth == len(word):
                if not node.is_end:
                    return False
                node.is_end = False
                return len(node.children) == 0

            char = word[depth]
            if char not in node.children:
                return False

            should_delete = _delete(node.children[char], word, depth + 1)

            if should_delete:
                del node.children[char]
                return not node.is_end and len(node.children) == 0

            node.children[char].count -= 1
            return False

        _delete(self.root, word, 0)

# ç”¨é€”:
# - ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆï¼ˆæ¤œç´¢å€™è£œã®è¡¨ç¤ºï¼‰
# - ã‚¹ãƒšãƒ«ãƒã‚§ãƒƒã‚¯
# - IPãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆæœ€é•·å‰ç½®ä¸€è‡´ï¼‰
# - è¾æ›¸ã®é«˜é€Ÿæ¤œç´¢
# - é›»è©±ç•ªå·æ¤œç´¢
# è¨ˆç®—é‡: æŒ¿å…¥/æ¤œç´¢ O(m) â€” mã¯æ–‡å­—åˆ—é•·
# ç©ºé–“: O(Î£ Ã— N) â€” Î£ã¯ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‚µã‚¤ã‚ºã€Nã¯ãƒãƒ¼ãƒ‰æ•°

# ä½¿ç”¨ä¾‹: ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆ
trie = Trie()
words = ["apple", "application", "apply", "ape", "banana", "band", "bank"]
for w in words:
    trie.insert(w)

print(trie.autocomplete("app"))  # ["apple", "application", "apply"]
print(trie.count_prefix("app"))  # 3
print(trie.count_prefix("ban"))  # 3
```

#### åœ§ç¸®Trieï¼ˆPatricia Trie / Radix Treeï¼‰

```python
class CompressedTrieNode:
    def __init__(self, label=""):
        self.label = label       # è¾ºã®ãƒ©ãƒ™ãƒ«ï¼ˆè¤‡æ•°æ–‡å­—å¯èƒ½ï¼‰
        self.children = {}
        self.is_end = False

class CompressedTrie:
    """åœ§ç¸®Trie: 1æ–‡å­—ãšã¤ã§ãªãã€å…±é€šæ¥é ­è¾ã‚’ã¾ã¨ã‚ã‚‹"""

    def __init__(self):
        self.root = CompressedTrieNode()

    def insert(self, word):
        node = self.root
        i = 0

        while i < len(word):
            first_char = word[i]

            if first_char not in node.children:
                new_node = CompressedTrieNode(word[i:])
                new_node.is_end = True
                node.children[first_char] = new_node
                return

            child = node.children[first_char]
            label = child.label

            # å…±é€šæ¥é ­è¾ã®é•·ã•ã‚’æ±‚ã‚ã‚‹
            j = 0
            while j < len(label) and i + j < len(word) and label[j] == word[i + j]:
                j += 1

            if j == len(label):
                # ãƒ©ãƒ™ãƒ«å…¨ä½“ãŒä¸€è‡´ â†’ å­ãƒãƒ¼ãƒ‰ã¸
                i += j
                node = child
            else:
                # é€”ä¸­ã§åˆ†å² â†’ ãƒãƒ¼ãƒ‰ã‚’åˆ†å‰²
                # å…±é€šéƒ¨åˆ†ã®æ–°ãƒãƒ¼ãƒ‰
                split = CompressedTrieNode(label[:j])
                split.children[label[j]] = child
                child.label = label[j:]

                if i + j < len(word):
                    new_node = CompressedTrieNode(word[i + j:])
                    new_node.is_end = True
                    split.children[word[i + j]] = new_node
                else:
                    split.is_end = True

                node.children[first_char] = split
                return

        node.is_end = True

    def search(self, word):
        node = self.root
        i = 0

        while i < len(word):
            first_char = word[i]
            if first_char not in node.children:
                return False

            child = node.children[first_char]
            label = child.label

            if not word[i:].startswith(label):
                return False

            i += len(label)
            node = child

        return node.is_end

# é€šå¸¸ã®Trie vs åœ§ç¸®Trie:
# é€šå¸¸: "application" â†’ a â†’ p â†’ p â†’ l â†’ i â†’ c â†’ a â†’ t â†’ i â†’ o â†’ n (11ãƒãƒ¼ãƒ‰)
# åœ§ç¸®: "application" â†’ "application" (1ãƒãƒ¼ãƒ‰)
# ç©ºé–“åŠ¹ç‡ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚Œã‚‹ï¼ˆç‰¹ã«ã‚­ãƒ¼ãŒé•·ã„å ´åˆï¼‰
```

### 2.2 ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—

```python
def build_suffix_array(s):
    """ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—ã‚’æ§‹ç¯‰ï¼ˆO(n log^2 n) ç‰ˆï¼‰"""
    n = len(s)
    # å„æ¥å°¾è¾ã‚’(ãƒ©ãƒ³ã‚¯, æ¬¡ã®ãƒ©ãƒ³ã‚¯, ä½ç½®)ã§ã‚½ãƒ¼ãƒˆ
    suffixes = [(ord(s[i]), ord(s[i + 1]) if i + 1 < n else -1, i)
                for i in range(n)]
    suffixes.sort()

    # ãƒ©ãƒ³ã‚¯ã®æ›´æ–°
    rank = [0] * n
    rank[suffixes[0][2]] = 0
    for i in range(1, n):
        rank[suffixes[i][2]] = rank[suffixes[i-1][2]]
        if suffixes[i][:2] != suffixes[i-1][:2]:
            rank[suffixes[i][2]] += 1

    k = 2
    while k < n:
        # (rank[i], rank[i+k]) ã§ã‚½ãƒ¼ãƒˆ
        suffixes = [(rank[i], rank[i + k] if i + k < n else -1, i)
                    for i in range(n)]
        suffixes.sort()

        new_rank = [0] * n
        new_rank[suffixes[0][2]] = 0
        for i in range(1, n):
            new_rank[suffixes[i][2]] = new_rank[suffixes[i-1][2]]
            if suffixes[i][:2] != suffixes[i-1][:2]:
                new_rank[suffixes[i][2]] += 1
        rank = new_rank
        k *= 2

    return [s[2] for s in suffixes]

# ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—ã®ä½¿ç”¨ä¾‹
s = "banana"
sa = build_suffix_array(s)
print(f"ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—: {sa}")
# [5, 3, 1, 0, 4, 2] â†’ a, ana, anana, banana, na, nana

# ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢: äºŒåˆ†æ¢ç´¢ã§ O(m log n)
def search_with_suffix_array(text, sa, pattern):
    """ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—ã§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢"""
    n = len(text)
    m = len(pattern)
    lo, hi = 0, n - 1

    while lo <= hi:
        mid = (lo + hi) // 2
        suffix = text[sa[mid]:sa[mid] + m]

        if suffix == pattern:
            # ãƒãƒƒãƒ â†’ å…¨ã¦ã®å‡ºç¾ã‚’æ¢ã™
            start = end = mid
            while start > 0 and text[sa[start-1]:sa[start-1]+m] == pattern:
                start -= 1
            while end < n - 1 and text[sa[end+1]:sa[end+1]+m] == pattern:
                end += 1
            return [sa[i] for i in range(start, end + 1)]
        elif suffix < pattern:
            lo = mid + 1
        else:
            hi = mid - 1

    return []

# LCPé…åˆ—ï¼ˆLongest Common Prefixï¼‰
def build_lcp_array(text, sa):
    """Kasaiã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ LCP é…åˆ—ã‚’æ§‹ç¯‰ O(n)"""
    n = len(text)
    rank = [0] * n
    lcp = [0] * n

    for i in range(n):
        rank[sa[i]] = i

    k = 0
    for i in range(n):
        if rank[i] == 0:
            k = 0
            continue
        j = sa[rank[i] - 1]
        while i + k < n and j + k < n and text[i + k] == text[j + k]:
            k += 1
        lcp[rank[i]] = k
        if k > 0:
            k -= 1

    return lcp

# LCPé…åˆ—ã®ç”¨é€”:
# - æœ€é•·åå¾©éƒ¨åˆ†æ–‡å­—åˆ—ã®æ¤œå‡º: max(lcp)
# - ç•°ãªã‚‹éƒ¨åˆ†æ–‡å­—åˆ—ã®æ•°: n*(n+1)/2 - sum(lcp)
# - æœ€é•·å…±é€šéƒ¨åˆ†æ–‡å­—åˆ—ï¼ˆ2ã¤ã®æ–‡å­—åˆ—ã‚’çµåˆã—ã¦SA+LCPã‚’æ§‹ç¯‰ï¼‰
```

```
ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—: å…¨ã¦ã®æ¥å°¾è¾ã‚’ã‚½ãƒ¼ãƒˆã—ãŸé…åˆ—

  æ–‡å­—åˆ—: "banana"
  æ¥å°¾è¾:           ã‚½ãƒ¼ãƒˆå¾Œ:
  0: banana         5: a
  1: anana          3: ana
  2: nana           1: anana
  3: ana            0: banana
  4: na             4: na
  5: a              2: nana

  ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—: [5, 3, 1, 0, 4, 2]

  ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢: äºŒåˆ†æ¢ç´¢ã§ O(m log n)
  æ§‹ç¯‰: O(n) (SA-IS ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ )

  ç”¨é€”:
  - å…¨æ–‡æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³
  - DNAé…åˆ—ã®è§£æ
  - ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ï¼ˆBWT: Burrows-Wheelerå¤‰æ›ï¼‰
  - æœ€é•·åå¾©éƒ¨åˆ†æ–‡å­—åˆ—
  - ç•°ãªã‚‹éƒ¨åˆ†æ–‡å­—åˆ—ã®ã‚«ã‚¦ãƒ³ãƒˆ
```

### 2.3 ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ„ãƒªãƒ¼ï¼ˆæ¦‚è¦ï¼‰

```
ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ„ãƒªãƒ¼: å…¨ã¦ã®æ¥å°¾è¾ã‚’åœ§ç¸®Trieã«æ ¼ç´

  æ–‡å­—åˆ— "banana$" ã®ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ„ãƒªãƒ¼:

  root
  â”œâ”€â”€ "a" â”€â”€ "na" â”€â”€ "na$"
  â”‚          â””â”€â”€ "$"
  â”œâ”€â”€ "banana$"
  â”œâ”€â”€ "na" â”€â”€ "na$"
  â”‚          â””â”€â”€ "$"
  â””â”€â”€ "$"

  ç‰¹å¾´:
  - æ§‹ç¯‰: O(n) (Ukkonenã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ )
  - ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢: O(m)
  - æœ€é•·åå¾©éƒ¨åˆ†æ–‡å­—åˆ—: O(n)
  - æœ€é•·å…±é€šéƒ¨åˆ†æ–‡å­—åˆ—: O(n + m)

  ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ„ãƒªãƒ¼ vs ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ç‰¹æ€§           â”‚ ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ„ãƒªãƒ¼ â”‚ ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ— â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ ç©ºé–“           â”‚ O(n) ã ãŒå®šæ•°å¤§   â”‚ O(n) ã§åŠ¹ç‡çš„    â”‚
  â”‚ æ§‹ç¯‰           â”‚ O(n) Ukkonen     â”‚ O(n) SA-IS      â”‚
  â”‚ ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢   â”‚ O(m)             â”‚ O(m log n)       â”‚
  â”‚ å®Ÿè£…ã®è¤‡é›‘ã•   â”‚ é«˜ã„              â”‚ ä¸­ç¨‹åº¦           â”‚
  â”‚ ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ â”‚ ä½ã„              â”‚ é«˜ã„             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  å®Ÿå‹™çš„ã«ã¯ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ— + LCPé…åˆ—ãŒä¸»æµ
```

---

## 3. æ­£è¦è¡¨ç¾ã¨ã‚ªãƒ¼ãƒˆãƒãƒˆãƒ³

### 3.1 æ­£è¦è¡¨ç¾ã®å†…éƒ¨

```
æ­£è¦è¡¨ç¾ã®å®Ÿè¡Œæ–¹å¼:

  1. NFAï¼ˆéæ±ºå®šæ€§æœ‰é™ã‚ªãƒ¼ãƒˆãƒãƒˆãƒ³ï¼‰æ–¹å¼
     â†’ æ­£è¦è¡¨ç¾ â†’ NFA â†’ æ–‡å­—åˆ—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
     â†’ è¨ˆç®—é‡: O(n Ã— m) â€” nã¯æ–‡å­—åˆ—é•·ã€mã¯æ­£è¦è¡¨ç¾é•·
     â†’ Go, Rust, RE2 ãŒæ¡ç”¨

  2. ãƒãƒƒã‚¯ãƒˆãƒ©ãƒƒã‚¯æ–¹å¼
     â†’ æ­£è¦è¡¨ç¾ã‚’ãƒãƒƒã‚¯ãƒˆãƒ©ãƒƒã‚¯ã§ç›´æ¥å®Ÿè¡Œ
     â†’ è¨ˆç®—é‡: O(2^n) æœ€æ‚ªï¼ˆæŒ‡æ•°æ™‚é–“ï¼ï¼‰
     â†’ Python, JavaScript, Java, Ruby, Perl ãŒæ¡ç”¨
     â†’ ã€Œå£Šæ»…çš„ãƒãƒƒã‚¯ãƒˆãƒ©ãƒƒã‚¯ã€ã®å±é™º

  å£Šæ»…çš„ãƒãƒƒã‚¯ãƒˆãƒ©ãƒƒã‚¯ (ReDoS) ã®ä¾‹:
  ãƒ‘ã‚¿ãƒ¼ãƒ³: (a+)+b
  å…¥åŠ›: "aaaaaaaaaaaaaaaaaaaac"
  â†’ ãƒãƒƒã‚¯ãƒˆãƒ©ãƒƒã‚¯æ–¹å¼: æŒ‡æ•°æ™‚é–“ã§å¿œç­”ãªã—ï¼
  â†’ NFAæ–¹å¼: ç·šå½¢æ™‚é–“ã§ä¸ä¸€è‡´ã‚’åˆ¤å®š

  å¯¾ç­–:
  - æ­£è¦è¡¨ç¾ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
  - RE2/Goæ­£è¦è¡¨ç¾ã‚¨ãƒ³ã‚¸ãƒ³ã®ä½¿ç”¨
  - æ­£è¦è¡¨ç¾ã®é™çš„è§£æãƒ„ãƒ¼ãƒ«
```

### 3.2 æœ‰é™ã‚ªãƒ¼ãƒˆãƒãƒˆãƒ³ã®åŸºç¤

```python
# DFAï¼ˆæ±ºå®šæ€§æœ‰é™ã‚ªãƒ¼ãƒˆãƒãƒˆãƒ³ï¼‰ã®å®Ÿè£…
class DFA:
    """æ±ºå®šæ€§æœ‰é™ã‚ªãƒ¼ãƒˆãƒãƒˆãƒ³"""
    def __init__(self, states, alphabet, transitions, start, accepts):
        self.states = states
        self.alphabet = alphabet
        self.transitions = transitions  # {(state, char): next_state}
        self.start = start
        self.accepts = accepts

    def accepts_string(self, s):
        state = self.start
        for char in s:
            key = (state, char)
            if key not in self.transitions:
                return False
            state = self.transitions[key]
        return state in self.accepts

# ä¾‹: "ab" ã‚’å«ã‚€æ–‡å­—åˆ—ã‚’å—ç†ã™ã‚‹DFA
dfa = DFA(
    states={0, 1, 2},
    alphabet={'a', 'b'},
    transitions={
        (0, 'a'): 1, (0, 'b'): 0,
        (1, 'a'): 1, (1, 'b'): 2,
        (2, 'a'): 2, (2, 'b'): 2,
    },
    start=0,
    accepts={2}
)

print(dfa.accepts_string("aab"))     # True
print(dfa.accepts_string("ba"))      # False
print(dfa.accepts_string("bab"))     # True

# NFA â†’ DFAå¤‰æ›ï¼ˆéƒ¨åˆ†é›†åˆæ§‹æˆæ³•ï¼‰
# ç†è«–çš„ã«ã¯çŠ¶æ…‹æ•°ãŒæŒ‡æ•°çš„ã«å¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
# å®Ÿç”¨çš„ã«ã¯ã»ã¨ã‚“ã©ã®å ´åˆã€ç®¡ç†å¯èƒ½ãªã‚µã‚¤ã‚º
```

### 3.3 ç°¡æ˜“æ­£è¦è¡¨ç¾ã‚¨ãƒ³ã‚¸ãƒ³

```python
class SimpleRegex:
    """ç°¡æ˜“æ­£è¦è¡¨ç¾ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆThompson NFAæ§‹æˆæ³•ï¼‰
    ã‚µãƒãƒ¼ãƒˆ: ãƒªãƒ†ãƒ©ãƒ«æ–‡å­—, '.', '*', '+', '?', '|', '(', ')'
    """

    class State:
        def __init__(self):
            self.transitions = {}  # char -> [State]
            self.epsilon = []      # epsiloné·ç§»
            self.is_accept = False

    class NFA:
        def __init__(self, start, accept):
            self.start = start
            self.accept = accept

    @staticmethod
    def char_nfa(c):
        """å˜ä¸€æ–‡å­—ã®NFA"""
        start = SimpleRegex.State()
        accept = SimpleRegex.State()
        accept.is_accept = True
        start.transitions[c] = [accept]
        return SimpleRegex.NFA(start, accept)

    @staticmethod
    def concat(nfa1, nfa2):
        """é€£çµ"""
        nfa1.accept.is_accept = False
        nfa1.accept.epsilon.append(nfa2.start)
        return SimpleRegex.NFA(nfa1.start, nfa2.accept)

    @staticmethod
    def union(nfa1, nfa2):
        """é¸æŠï¼ˆ|ï¼‰"""
        start = SimpleRegex.State()
        accept = SimpleRegex.State()
        accept.is_accept = True

        start.epsilon.extend([nfa1.start, nfa2.start])
        nfa1.accept.is_accept = False
        nfa2.accept.is_accept = False
        nfa1.accept.epsilon.append(accept)
        nfa2.accept.epsilon.append(accept)

        return SimpleRegex.NFA(start, accept)

    @staticmethod
    def star(nfa):
        """ã‚¯ãƒªãƒ¼ãƒé–‰åŒ…ï¼ˆ*ï¼‰"""
        start = SimpleRegex.State()
        accept = SimpleRegex.State()
        accept.is_accept = True

        start.epsilon.extend([nfa.start, accept])
        nfa.accept.is_accept = False
        nfa.accept.epsilon.extend([nfa.start, accept])

        return SimpleRegex.NFA(start, accept)

    @staticmethod
    def match(nfa, text):
        """NFAã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§æ–‡å­—åˆ—ã‚’ãƒãƒƒãƒ"""
        def epsilon_closure(states):
            stack = list(states)
            closure = set(states)
            while stack:
                state = stack.pop()
                for next_state in state.epsilon:
                    if next_state not in closure:
                        closure.add(next_state)
                        stack.append(next_state)
            return closure

        current = epsilon_closure({nfa.start})

        for char in text:
            next_states = set()
            for state in current:
                if char in state.transitions:
                    next_states.update(state.transitions[char])
                if '.' in state.transitions:  # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰
                    next_states.update(state.transitions['.'])
            current = epsilon_closure(next_states)

        return any(state.is_accept for state in current)

# Thompson NFA ã®åˆ©ç‚¹:
# - å¸¸ã« O(n Ã— m) ã®è¨ˆç®—é‡ï¼ˆãƒãƒƒã‚¯ãƒˆãƒ©ãƒƒã‚¯ãªã—ï¼‰
# - å£Šæ»…çš„ãƒãƒƒã‚¯ãƒˆãƒ©ãƒƒã‚¯ãŒèµ·ããªã„
# - RE2, Go ã®æ­£è¦è¡¨ç¾ã‚¨ãƒ³ã‚¸ãƒ³ãŒæ¡ç”¨
```

### 3.4 å±é™ºãªæ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆReDoSï¼‰

```python
import re
import time

# ReDoSï¼ˆæ­£è¦è¡¨ç¾ã‚µãƒ¼ãƒ“ã‚¹æ‹’å¦æ”»æ’ƒï¼‰ã®ä¾‹

# å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³: ãƒã‚¹ãƒˆã—ãŸé‡è©
dangerous_patterns = [
    r"(a+)+b",           # ãƒã‚¹ãƒˆã—ãŸç¹°ã‚Šè¿”ã—
    r"(a|aa)+b",         # é¸æŠ + ç¹°ã‚Šè¿”ã—
    r"(.*a){x}",         # .* + ç¹°ã‚Šè¿”ã—
    r"([a-zA-Z]+)*@",    # æ–‡å­—ã‚¯ãƒ©ã‚¹ + ãƒã‚¹ãƒˆ
]

# å®‰å…¨ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¸ã®æ›¸ãæ›ãˆä¾‹:
# å±é™º: (a+)+b   â†’ å®‰å…¨: a+b
# å±é™º: (a|aa)+b â†’ å®‰å…¨: a+b
# å±é™º: ([a-zA-Z]+)*@ â†’ å®‰å…¨: [a-zA-Z]+@

# ReDoSã®æ¤œå‡ºãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ:
# 1. ãƒã‚¹ãƒˆã—ãŸé‡è© (X+)+ ã‚„ (X*)*
# 2. é‡ãªã‚Šã®ã‚ã‚‹é¸æŠè‚¢ (a|a)+ ã‚„ (a|ab)+
# 3. é‡è©ã®å¾Œã«å¤±æ•—ã—ã‚„ã™ã„ãƒ‘ã‚¿ãƒ¼ãƒ³
# 4. ãƒãƒƒã‚¯ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã®å¤šç”¨

# å¯¾ç­–:
# 1. æ­£è¦è¡¨ç¾ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
# 2. å…¥åŠ›ã®é•·ã•åˆ¶é™
# 3. RE2 ã‚„ Go ã®æ­£è¦è¡¨ç¾ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
# 4. æ­£è¦è¡¨ç¾ã®é™çš„è§£æãƒ„ãƒ¼ãƒ«ï¼ˆredos-checkerç­‰ï¼‰
# 5. å¯èƒ½ã§ã‚ã‚Œã°æ­£è¦è¡¨ç¾ã‚’ä½¿ã‚ãªã„æ–‡å­—åˆ—å‡¦ç†ã«ç½®ãæ›ãˆ

# Pythonã§ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãæ­£è¦è¡¨ç¾
import signal

def regex_with_timeout(pattern, text, timeout=1):
    """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãæ­£è¦è¡¨ç¾ãƒãƒƒãƒï¼ˆUnixç³»ã®ã¿ï¼‰"""
    def handler(signum, frame):
        raise TimeoutError("æ­£è¦è¡¨ç¾ã®å®Ÿè¡ŒãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")

    signal.signal(signal.SIGALRM, handler)
    signal.alarm(timeout)

    try:
        result = re.match(pattern, text)
        signal.alarm(0)
        return result
    except TimeoutError:
        return None
```

### 3.5 å®Ÿå‹™ã§ã®æ–‡å­—åˆ—å‡¦ç†

```python
# å®Ÿå‹™ã§ã‚ˆãä½¿ã†æ–‡å­—åˆ—æ“ä½œã®è¨ˆç®—é‡

s = "Hello, World!"

# O(n) æ“ä½œ:
s.find("World")      # ç·šå½¢æ¤œç´¢
s.replace("o", "0")  # å…¨ç½®æ›
s.split(",")          # åˆ†å‰²
"".join(parts)        # çµåˆ

# æ³¨æ„: æ–‡å­—åˆ—ã®é€£çµ
# âŒ O(n^2): ãƒ«ãƒ¼ãƒ—å†…ã§æ–‡å­—åˆ—é€£çµ
result = ""
for word in words:
    result += word  # æ¯å›æ–°ã—ã„æ–‡å­—åˆ—ã‚’ç”Ÿæˆ â†’ O(n^2)

# âœ… O(n): joinã‚’ä½¿ç”¨
result = "".join(words)

# âœ… O(n): io.StringIO
from io import StringIO
buf = StringIO()
for word in words:
    buf.write(word)
result = buf.getvalue()
```

#### æ–‡å­—åˆ—å‡¦ç†ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

```python
import re

# 1. æ­£è¦è¡¨ç¾ã®ãƒ—ãƒªã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
# âŒ æ¯å›ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
for line in lines:
    if re.match(r"\d{3}-\d{4}", line):
        pass

# âœ… ãƒ—ãƒªã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
pattern = re.compile(r"\d{3}-\d{4}")
for line in lines:
    if pattern.match(line):
        pass

# 2. æ–‡å­—åˆ—ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
name = "World"
# âŒ é…ã„
result = "Hello, " + name + "!"
# âœ… f-stringï¼ˆPython 3.6+ã€æœ€ã‚‚é«˜é€Ÿï¼‰
result = f"Hello, {name}!"

# 3. å¤§é‡ã®æ–‡å­—åˆ—æ“ä½œ
# âŒ æ–‡å­—åˆ—ã®ç¹°ã‚Šè¿”ã—é€£çµ
s = ""
for i in range(10000):
    s += str(i)

# âœ… ãƒªã‚¹ãƒˆã«æºœã‚ã¦join
parts = []
for i in range(10000):
    parts.append(str(i))
s = "".join(parts)

# âœ… ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿å¼
s = "".join(str(i) for i in range(10000))

# 4. æ–‡å­—åˆ—ã®ã‚¹ãƒ©ã‚¤ã‚¹ã¯æ–°ã—ã„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã™ã‚‹
# Python: s[i:j] ã¯ã‚³ãƒ”ãƒ¼ O(j-i)
# å¤§é‡ã®ã‚¹ãƒ©ã‚¤ã‚¹ã‚’é¿ã‘ã‚‹ â†’ memoryview ã‚„ index ã§å¯¾å‡¦

# 5. å„è¨€èªã§ã®æ–‡å­—åˆ—ãƒ“ãƒ«ãƒ€ãƒ¼
# Python: "".join(list) or io.StringIO
# Java: StringBuilder
# C#: StringBuilder
# Go: strings.Builder
# Rust: String::push_str or format!
# JavaScript: Array.join() or template literals
```

#### Unicodeæ–‡å­—åˆ—ã®æ‰±ã„

```python
# Unicodeã®æ³¨æ„ç‚¹

# 1. æ–‡å­—æ•° vs ãƒã‚¤ãƒˆæ•°
s = "ã“ã‚“ã«ã¡ã¯"
print(len(s))                    # 5ï¼ˆæ–‡å­—æ•°ï¼‰
print(len(s.encode('utf-8')))    # 15ï¼ˆUTF-8ãƒã‚¤ãƒˆæ•°ï¼‰
print(len(s.encode('utf-16')))   # 12ï¼ˆUTF-16ãƒã‚¤ãƒˆæ•°ã€BOMå«ã‚€ï¼‰

# 2. ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒšã‚¢
emoji = "ğŸ˜€"
print(len(emoji))                        # 1ï¼ˆPythonï¼‰
# JavaScriptã§ã¯: "ğŸ˜€".length = 2ï¼ˆUTF-16ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒšã‚¢ï¼‰

# 3. çµåˆæ–‡å­—ï¼ˆCombining Charactersï¼‰
# "ãŒ" = "ã‹" + "ã‚›" ã®å ´åˆãŒã‚ã‚‹
import unicodedata
s1 = "ãŒ"                              # 1æ–‡å­—
s2 = "ã‹\u3099"                        # ã‹ + æ¿ç‚¹ = ãŒ
print(s1 == s2)                         # False!
print(unicodedata.normalize('NFC', s1) ==
      unicodedata.normalize('NFC', s2)) # True

# 4. æ–‡å­—åˆ—ã®æ¯”è¼ƒã¯æ­£è¦åŒ–å¾Œã«è¡Œã†
def safe_compare(s1, s2):
    return unicodedata.normalize('NFC', s1) == unicodedata.normalize('NFC', s2)

# 5. æ›¸è¨˜ç´ ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆGrapheme Clusterï¼‰
# "ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦" ã¯1ã¤ã®çµµæ–‡å­—ã ãŒã€å†…éƒ¨çš„ã«ã¯è¤‡æ•°ã®ã‚³ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
family = "ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦"
print(len(family))  # 7ï¼ˆPythonï¼‰â€” ZWJ (Zero Width Joiner) ã‚’å«ã‚€
# æ­£ã—ã1æ–‡å­—ã¨ã—ã¦æ‰±ã†ã«ã¯ grapheme ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦
```

---

## 4. é«˜åº¦ãªæ–‡å­—åˆ—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

### 4.1 Manacheræ³•ï¼ˆæœ€é•·å›æ–‡éƒ¨åˆ†æ–‡å­—åˆ—ï¼‰

```python
def manacher(s):
    """å…¨ã¦ã®ä½ç½®ã‚’ä¸­å¿ƒã¨ã™ã‚‹æœ€é•·å›æ–‡ã‚’ O(n) ã§æ±‚ã‚ã‚‹"""
    # æ–‡å­—é–“ã«'#'ã‚’æŒ¿å…¥: "abc" â†’ "#a#b#c#"
    t = '#' + '#'.join(s) + '#'
    n = len(t)
    p = [0] * n  # p[i] = ä½ç½®iã‚’ä¸­å¿ƒã¨ã™ã‚‹å›æ–‡ã®åŠå¾„

    center = right = 0  # æœ€ã‚‚å³ã¾ã§å±Šã„ã¦ã„ã‚‹å›æ–‡
    for i in range(n):
        # ãƒŸãƒ©ãƒ¼ã‚’åˆ©ç”¨
        mirror = 2 * center - i
        if i < right:
            p[i] = min(right - i, p[mirror])

        # æ‹¡å¼µã‚’è©¦ã¿ã‚‹
        while (i + p[i] + 1 < n and i - p[i] - 1 >= 0 and
               t[i + p[i] + 1] == t[i - p[i] - 1]):
            p[i] += 1

        # å³ç«¯ã‚’æ›´æ–°
        if i + p[i] > right:
            center = i
            right = i + p[i]

    # æœ€é•·å›æ–‡ã‚’è¦‹ã¤ã‘ã‚‹
    max_len = max(p)
    max_center = p.index(max_len)
    start = (max_center - max_len) // 2

    return s[start:start + max_len]

# ä¾‹:
print(manacher("babad"))    # "bab" or "aba"
print(manacher("cbbd"))     # "bb"
print(manacher("racecar"))  # "racecar"

# è¨ˆç®—é‡: O(n) â€” ãƒŠã‚¤ãƒ¼ãƒ–æ³•ã® O(n^2) ã‹ã‚‰å¤§å¹…æ”¹å–„
# ãƒŸãƒ©ãƒ¼åˆ©ç”¨ãŒè¨ˆç®—é‡å‰Šæ¸›ã®éµ
```

### 4.2 Burrows-Wheelerå¤‰æ›ï¼ˆBWTï¼‰

```python
def bwt_encode(s):
    """Burrows-Wheelerå¤‰æ›ï¼ˆåœ§ç¸®ã®ãŸã‚ã®å‰å‡¦ç†ï¼‰"""
    s = s + '$'  # çµ‚ç«¯æ–‡å­—ã‚’è¿½åŠ 
    n = len(s)

    # å…¨ã¦ã®å·¡å›ã‚·ãƒ•ãƒˆã‚’ã‚½ãƒ¼ãƒˆ
    rotations = sorted(range(n), key=lambda i: s[i:] + s[:i])

    # æœ€å¾Œã®åˆ—ã‚’å–å¾—
    bwt = ''.join(s[(i - 1) % n] for i in rotations)
    # å…ƒã®æ–‡å­—åˆ—ã®ä½ç½®
    original_idx = rotations.index(0)

    return bwt, original_idx

def bwt_decode(bwt, idx):
    """BWTé€†å¤‰æ›"""
    n = len(bwt)
    table = [''] * n

    for _ in range(n):
        table = sorted(bwt[i] + table[i] for i in range(n))

    for row in table:
        if row.endswith('$'):
            return row[:-1]

# ä½¿ç”¨ä¾‹
text = "banana"
encoded, idx = bwt_encode(text)
print(f"BWT: {encoded}")  # "annb$aa"
decoded = bwt_decode(encoded, idx)
print(f"å¾©å…ƒ: {decoded}")  # "banana"

# BWTã®ç”¨é€”:
# - bzip2 åœ§ç¸®ã®æ ¸å¿ƒæŠ€è¡“
# - FM-Indexï¼ˆå…¨æ–‡æ¤œç´¢ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
# - DNAé…åˆ—ã®ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆï¼ˆBWA, Bowtieï¼‰
# BWTã¯åŒã˜æ–‡å­—ã‚’è¿‘ãã«é›†ã‚ã‚‹åŠ¹æœãŒã‚ã‚‹ â†’ Run-Length Encoding ã§åŠ¹ç‡çš„ã«åœ§ç¸®
```

### 4.3 æ–‡å­—åˆ—ãƒãƒƒã‚·ãƒ¥ã®å¿œç”¨

```python
class RollingHash:
    """ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒãƒƒã‚·ãƒ¥: éƒ¨åˆ†æ–‡å­—åˆ—ã®ãƒãƒƒã‚·ãƒ¥ã‚’ O(1) ã§è¨ˆç®—"""

    def __init__(self, s, base=131, mod=10**18 + 9):
        self.n = len(s)
        self.base = base
        self.mod = mod

        # å‰å‡¦ç†: ãƒãƒƒã‚·ãƒ¥å€¤ã¨åŸºæ•°ã®ã¹ãä¹—ã‚’è¨ˆç®—
        self.hash = [0] * (self.n + 1)
        self.power = [1] * (self.n + 1)

        for i in range(self.n):
            self.hash[i + 1] = (self.hash[i] * base + ord(s[i])) % mod
            self.power[i + 1] = (self.power[i] * base) % mod

    def get_hash(self, l, r):
        """éƒ¨åˆ†æ–‡å­—åˆ— s[l:r] ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ O(1) ã§è¿”ã™"""
        return (self.hash[r] - self.hash[l] * self.power[r - l]) % self.mod

    def is_equal(self, l1, r1, l2, r2):
        """s[l1:r1] == s[l2:r2] ã‚’ O(1) ã§åˆ¤å®šï¼ˆç¢ºç‡çš„ï¼‰"""
        return self.get_hash(l1, r1) == self.get_hash(l2, r2)

# ä½¿ç”¨ä¾‹: æœ€é•·åå¾©éƒ¨åˆ†æ–‡å­—åˆ—
def longest_repeated_substring(s):
    """äºŒåˆ†æ¢ç´¢ + ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒãƒƒã‚·ãƒ¥ã§æœ€é•·åå¾©éƒ¨åˆ†æ–‡å­—åˆ—ã‚’æ±‚ã‚ã‚‹"""
    rh = RollingHash(s)
    n = len(s)

    def has_repeat(length):
        """é•·ã•lengthã®åå¾©éƒ¨åˆ†æ–‡å­—åˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹"""
        seen = set()
        for i in range(n - length + 1):
            h = rh.get_hash(i, i + length)
            if h in seen:
                return i
            seen.add(h)
        return -1

    lo, hi = 0, n - 1
    best = ""

    while lo <= hi:
        mid = (lo + hi) // 2
        pos = has_repeat(mid)
        if pos != -1:
            best = s[pos:pos + mid]
            lo = mid + 1
        else:
            hi = mid - 1

    return best

# è¨ˆç®—é‡: O(n log n) â€” æœŸå¾…å€¤
# ä½¿ç”¨ä¾‹
print(longest_repeated_substring("banana"))  # "ana"
print(longest_repeated_substring("abcabc"))  # "abc"
```

---

## 5. å®Ÿå‹™ã§ã®æ–‡å­—åˆ—å‡¦ç†

### 5.1 ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
# å®Ÿå‹™çš„ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¾‹

import re
from collections import Counter

def text_processing_pipeline(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†ã—ã¦å˜èªé »åº¦ã‚’è¨ˆç®—"""
    # 1. æ­£è¦åŒ–
    text = text.lower()

    # 2. ç‰¹æ®Šæ–‡å­—ã®é™¤å»
    text = re.sub(r'[^\w\s]', '', text)

    # 3. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    words = text.split()

    # 4. ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®é™¤å»
    stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were',
                  'in', 'on', 'at', 'to', 'of', 'and', 'or', 'for'}
    words = [w for w in words if w not in stop_words]

    # 5. é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
    freq = Counter(words)

    return freq.most_common(10)

# ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ï¼ˆã‚ã„ã¾ã„æ¤œç´¢ï¼‰
def fuzzy_search(query, candidates, max_distance=2):
    """ç·¨é›†è·é›¢ãŒmax_distanceä»¥å†…ã®å€™è£œã‚’è¿”ã™"""
    results = []

    for candidate in candidates:
        dist = edit_distance(query, candidate)
        if dist <= max_distance:
            results.append((candidate, dist))

    return sorted(results, key=lambda x: x[1])

def edit_distance(s1, s2):
    m, n = len(s1), len(s2)
    dp = list(range(n + 1))

    for i in range(1, m + 1):
        prev = dp[0]
        dp[0] = i
        for j in range(1, n + 1):
            temp = dp[j]
            if s1[i-1] == s2[j-1]:
                dp[j] = prev
            else:
                dp[j] = 1 + min(prev, dp[j], dp[j-1])
            prev = temp

    return dp[n]

# ä½¿ç”¨ä¾‹
words = ["apple", "application", "apply", "maple", "ape",
         "banana", "bandana", "band"]
results = fuzzy_search("aple", words, max_distance=2)
# [('apple', 1), ('ape', 2), ('maple', 2)]
```

### 5.2 å…¨æ–‡æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®ä»•çµ„ã¿

```python
# è»¢ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆInverted Indexï¼‰ã®ç°¡æ˜“å®Ÿè£…

class InvertedIndex:
    """å…¨æ–‡æ¤œç´¢ã®ãŸã‚ã®è»¢ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""

    def __init__(self):
        self.index = {}        # å˜èª â†’ [(doc_id, position), ...]
        self.documents = {}    # doc_id â†’ åŸæ–‡

    def add_document(self, doc_id, text):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ """
        self.documents[doc_id] = text
        words = text.lower().split()

        for pos, word in enumerate(words):
            # ã‚¹ãƒ†ãƒŸãƒ³ã‚°ï¼ˆç°¡æ˜“ç‰ˆ: æœ«å°¾ã®sã‚’é™¤å»ï¼‰
            word = word.strip('.,!?;:')
            if word not in self.index:
                self.index[word] = []
            self.index[word].append((doc_id, pos))

    def search(self, query):
        """å˜èªã‚’å«ã‚€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢"""
        query = query.lower()
        if query not in self.index:
            return []

        # TF-IDFçš„ãªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        import math
        total_docs = len(self.documents)
        results = {}

        postings = self.index[query]
        df = len(set(doc_id for doc_id, _ in postings))
        idf = math.log(total_docs / df) if df > 0 else 0

        for doc_id, pos in postings:
            if doc_id not in results:
                results[doc_id] = 0
            results[doc_id] += 1  # TF

        scored = [(doc_id, tf * idf) for doc_id, tf in results.items()]
        return sorted(scored, key=lambda x: x[1], reverse=True)

    def phrase_search(self, phrase):
        """ãƒ•ãƒ¬ãƒ¼ã‚ºæ¤œç´¢ï¼ˆé€£ç¶šã™ã‚‹å˜èªã®æ¤œç´¢ï¼‰"""
        words = phrase.lower().split()
        if not words or words[0] not in self.index:
            return []

        # æœ€åˆã®å˜èªã®å‡ºç¾ä½ç½®
        candidates = {}
        for doc_id, pos in self.index[words[0]]:
            if doc_id not in candidates:
                candidates[doc_id] = []
            candidates[doc_id].append(pos)

        # å¾Œç¶šã®å˜èªãŒé€£ç¶šã—ã¦ã„ã‚‹ã‹ç¢ºèª
        for i, word in enumerate(words[1:], 1):
            if word not in self.index:
                return []

            next_positions = {}
            for doc_id, pos in self.index[word]:
                if doc_id in candidates:
                    next_positions[doc_id] = next_positions.get(doc_id, [])
                    next_positions[doc_id].append(pos)

            new_candidates = {}
            for doc_id, starts in candidates.items():
                if doc_id in next_positions:
                    valid = [s for s in starts
                            if s + i in next_positions[doc_id]]
                    if valid:
                        new_candidates[doc_id] = valid

            candidates = new_candidates

        return list(candidates.keys())

# ä½¿ç”¨ä¾‹
idx = InvertedIndex()
idx.add_document(1, "The quick brown fox jumps over the lazy dog")
idx.add_document(2, "A quick brown dog outpaces the fox")
idx.add_document(3, "The fox and the dog are friends")

print(idx.search("fox"))           # [(1, ...), (2, ...), (3, ...)]
print(idx.phrase_search("quick brown"))  # [1, 2]
```

---

## 6. å®Ÿè·µæ¼”ç¿’

### æ¼”ç¿’1: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆåŸºç¤ï¼‰
KMPæ³•ã®å¤±æ•—é–¢æ•°ã‚’æ‰‹è¨ˆç®—ã§æ±‚ã‚ã‚ˆï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³: "ABABC"ï¼‰ã€‚

### æ¼”ç¿’2: Trieï¼ˆå¿œç”¨ï¼‰
Trieã‚’ä½¿ã£ã¦ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆæ©Ÿèƒ½ã‚’å®Ÿè£…ã›ã‚ˆã€‚å…¥åŠ›ã•ã‚ŒãŸæ¥é ­è¾ã«å¯¾ã—ã¦ã€è¾æ›¸å†…ã®å€™è£œã‚’æœ€å¤§10å€‹è¿”ã™ã‚ˆã†ã«ã›ã‚ˆã€‚

### æ¼”ç¿’3: Boyer-Mooreï¼ˆå¿œç”¨ï¼‰
Boyer-Mooreæ³•ã® Bad Character ãƒ«ãƒ¼ãƒ«ã‚’å®Ÿè£…ã—ã€ãƒŠã‚¤ãƒ¼ãƒ–æ³•ã¨ã®æ¯”è¼ƒå›æ•°ã®å·®ã‚’è¨ˆæ¸¬ã›ã‚ˆã€‚

### æ¼”ç¿’4: å…¨æ–‡æ¤œç´¢ï¼ˆå¿œç”¨ï¼‰
è»¢ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å®Ÿè£…ã—ã€AND/ORæ¤œç´¢ã«å¯¾å¿œã•ã›ã‚ˆã€‚TF-IDFã«ã‚ˆã‚‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚‚å®Ÿè£…ã›ã‚ˆã€‚

### æ¼”ç¿’5: æ­£è¦è¡¨ç¾ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆç™ºå±•ï¼‰
NFAæ–¹å¼ã®ç°¡æ˜“æ­£è¦è¡¨ç¾ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆ., *, | ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼‰ã‚’å®Ÿè£…ã›ã‚ˆã€‚

### æ¼”ç¿’6: å›æ–‡æ¤œå‡ºï¼ˆç™ºå±•ï¼‰
Manacheræ³•ã‚’å®Ÿè£…ã—ã€æ–‡å­—åˆ—ä¸­ã®å…¨ã¦ã®å›æ–‡éƒ¨åˆ†æ–‡å­—åˆ—ã‚’åˆ—æŒ™ã›ã‚ˆã€‚

### æ¼”ç¿’7: ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—ï¼ˆç™ºå±•ï¼‰
ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ— + LCPé…åˆ—ã‚’æ§‹ç¯‰ã—ã€ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®æœ€é•·åå¾©éƒ¨åˆ†æ–‡å­—åˆ—ã‚’è¦‹ã¤ã‘ã‚ˆã€‚

### æ¼”ç¿’8: æ–‡å­—åˆ—ãƒãƒƒã‚·ãƒ¥ï¼ˆç™ºå±•ï¼‰
ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒãƒƒã‚·ãƒ¥ã‚’ä½¿ã£ã¦ã€2ã¤ã®æ–‡å­—åˆ—ã®æœ€é•·å…±é€šéƒ¨åˆ†æ–‡å­—åˆ—ã‚’ O(n log n) ã§æ±‚ã‚ã‚ˆã€‚

---

## FAQ

### Q1: grep ã¯å†…éƒ¨çš„ã«ä½•ã‚’ä½¿ã£ã¦ã„ã¾ã™ã‹ï¼Ÿ
**A**: GNU grep ã¯ Boyer-Moore ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒãƒ™ãƒ¼ã‚¹ã€‚é•·ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¼·ãã€ãƒ‘ã‚¿ãƒ¼ãƒ³æœ«å°¾ã‹ã‚‰æ¯”è¼ƒã—ã¦ã‚¹ã‚­ãƒƒãƒ—é‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚æ­£è¦è¡¨ç¾ãƒ¢ãƒ¼ãƒ‰ã§ã¯ DFA/NFA ã‚’ä½¿ç”¨ã€‚fgrepï¼ˆå›ºå®šæ–‡å­—åˆ—æ¤œç´¢ï¼‰ã§ã¯ Aho-Corasick ãŒä½¿ã‚ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚

### Q2: å…¨æ–‡æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆElasticsearchç­‰ï¼‰ã®å†…éƒ¨ã¯ï¼Ÿ
**A**: è»¢ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ + BM25ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã€‚å„å˜èªã®å‡ºç¾ä½ç½®ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã€æ¤œç´¢æ™‚ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒ¼ã‚¸ã§å€™è£œã‚’çµã‚‹ã€‚æ—¥æœ¬èªã¯MeCab/kuromojiç­‰ã§å½¢æ…‹ç´ è§£æã—ã¦ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã€‚N-gramã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½µç”¨ã™ã‚‹å ´åˆã‚‚ã‚ã‚‹ã€‚

### Q3: æ–‡å­—åˆ—å‡¦ç†ã§æœ€ã‚‚é‡è¦ãªæœ€é©åŒ–ã¯ï¼Ÿ
**A**: (1)æ–‡å­—åˆ—é€£çµã®å›é¿ï¼ˆjoinã‚’ä½¿ã†ï¼‰ (2)æ­£è¦è¡¨ç¾ã®ãƒ—ãƒªã‚³ãƒ³ãƒ‘ã‚¤ãƒ«(re.compile) (3)ä¸è¦ãªã‚³ãƒ”ãƒ¼ã®å›é¿ï¼ˆã‚¹ãƒ©ã‚¤ã‚¹ã¯æ–°ã—ã„æ–‡å­—åˆ—ã‚’ç”Ÿæˆã™ã‚‹ï¼‰ (4)é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®é¸æŠï¼ˆTrieã€ãƒãƒƒã‚·ãƒ¥ãƒãƒƒãƒ—ç­‰ï¼‰ (5)Unicodeæ­£è¦åŒ–ã®ä¸€è²«æ€§

### Q4: KMPæ³•ã¨Boyer-Mooreæ³•ã¯ã©ã¡ã‚‰ãŒé€Ÿã„ã§ã™ã‹ï¼Ÿ
**A**: ä¸€èˆ¬çš„ã«Boyer-Mooreæ³•ã®æ–¹ãŒå®Ÿç”¨ä¸Šé«˜é€Ÿã€‚ç‰¹ã«ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆãŒå¤§ããï¼ˆASCIIç­‰ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé•·ã„å ´åˆã«æœ‰åˆ©ã€‚æœ€å–„ã‚±ãƒ¼ã‚¹ã§ã¯ O(n/m) ã¨ã€ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’èª­ã¾ãšã«æ¸ˆã‚€ã€‚ä¸€æ–¹ã€KMPæ³•ã¯æœ€æ‚ªã‚±ãƒ¼ã‚¹ã§ã‚‚ O(n+m) ãŒä¿è¨¼ã•ã‚Œã€å®Ÿè£…ã‚‚æ¯”è¼ƒçš„å˜ç´”ã€‚ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚„å°ã•ã„ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆï¼ˆDNAç­‰ï¼‰ã§ã¯KMPæ³•ãŒæœ‰åˆ©ãªå ´åˆã‚‚ã‚ã‚‹ã€‚

### Q5: Aho-Corasickæ³•ã¯ã©ã†ã„ã†å ´é¢ã§ä½¿ã„ã¾ã™ã‹ï¼Ÿ
**A**: è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŒæ™‚ã«æ¤œç´¢ã™ã‚‹å ´é¢ã€‚ä¾‹: ã‚¦ã‚¤ãƒ«ã‚¹ã‚¹ã‚­ãƒ£ãƒŠï¼ˆæ•°ä¸‡ã®ã‚·ã‚°ãƒãƒãƒ£ã‚’åŒæ™‚æ¤œç´¢ï¼‰ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯IDSã€ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆNGãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼‰ã€è¾æ›¸ãƒ™ãƒ¼ã‚¹ã®å½¢æ…‹ç´ è§£æã€‚å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³ãªã‚‰KMPã‚„Boyer-Mooreã®æ–¹ãŒåŠ¹ç‡çš„ã€‚

### Q6: ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—ã¨ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ„ãƒªãƒ¼ã®ä½¿ã„åˆ†ã‘ã¯ï¼Ÿ
**A**: å®Ÿå‹™çš„ã«ã¯ã»ã¼ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ— + LCPé…åˆ—ã§ååˆ†ã€‚ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ„ãƒªãƒ¼ã¯ç†è«–çš„ã«ã¯å¼·åŠ›ã ãŒã€ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ãŒå¤§ããï¼ˆ20n ãƒã‚¤ãƒˆç¨‹åº¦ï¼‰ã€å®Ÿè£…ãŒè¤‡é›‘ã€‚ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—ã¯ 5n ãƒã‚¤ãƒˆç¨‹åº¦ã§ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã‚‚è‰¯ã„ã€‚ãŸã ã—ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æ–‡å­—ã‚’è¿½åŠ ã—ãªãŒã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã¯ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ„ãƒªãƒ¼ï¼ˆUkkonenæ³•ï¼‰ãŒé©ã—ã¦ã„ã‚‹ã€‚

---

## ã¾ã¨ã‚

| ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | è¨ˆç®—é‡ | ç”¨é€” |
|------------|--------|------|
| ãƒŠã‚¤ãƒ¼ãƒ– | O(nm) | çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆ |
| KMP | O(n+m) | å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ |
| Boyer-Moore | O(n/m)æœ€å–„, O(n+m) | å®Ÿç”¨æœ€é€Ÿã®å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ |
| Rabin-Karp | O(n+m)æœŸå¾… | è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ |
| Aho-Corasick | O(n+m+z) | è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³åŒæ™‚æ¤œç´¢ |
| Z-Algorithm | O(n+m) | ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ã€å‘¨æœŸæ¤œå‡º |
| Trie | O(m) | è¾æ›¸ã€ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆ |
| ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ— | O(m log n) | å…¨æ–‡æ¤œç´¢ |
| Manacher | O(n) | å›æ–‡æ¤œå‡º |
| ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒãƒƒã‚·ãƒ¥ | O(1) per query | éƒ¨åˆ†æ–‡å­—åˆ—ã®æ¯”è¼ƒ |

---

## æ¬¡ã«èª­ã‚€ã¹ãã‚¬ã‚¤ãƒ‰
â†’ [[../04-data-structures/00-arrays-and-strings.md]] -- é…åˆ—ã¨æ–‡å­—åˆ—

---

## å‚è€ƒæ–‡çŒ®
1. Cormen, T. H. et al. "Introduction to Algorithms." Chapter 32: String Matching.
2. Knuth, D. E., Morris, J. H., Pratt, V. R. "Fast Pattern Matching in Strings." 1977.
3. Cox, R. "Regular Expression Matching Can Be Simple And Fast." 2007.
4. Gusfield, D. "Algorithms on Strings, Trees, and Sequences." Cambridge University Press, 1997.
5. Aho, A. V., Corasick, M. J. "Efficient string matching: an aid to bibliographic search." 1975.
6. Crochemore, M., Rytter, W. "Jewels of Stringology." World Scientific, 2002.
