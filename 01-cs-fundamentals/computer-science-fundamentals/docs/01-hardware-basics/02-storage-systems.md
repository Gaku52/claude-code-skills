# ストレージシステム

> データの永続化はコンピューティングの根幹であり、ストレージ技術の進化がデジタル社会を支えている。

## この章で学ぶこと

- [ ] HDD/SSD/NVMeの内部構造と動作原理を説明できる
- [ ] ファイルシステムの役割と主要な実装を理解する
- [ ] RAIDレベルの違いと使い分けを説明できる

## 前提知識

- メモリ階層の基礎 → 参照: [[01-memory-hierarchy.md]]

---

## 1. HDD（ハードディスクドライブ）

### 1.1 内部構造

```
HDD の内部構造:

  ┌─────────────────────────────────────┐
  │                                     │
  │     ┌───────────────────┐          │
  │     │   プラッタ (磁気円盤) │        │
  │     │   ┌─────────────┐ │          │
  │     │   │  ─────────  │ │          │
  │     │   │ /  トラック  \│ │         │
  │     │   │|  ┌──────┐  |│ │         │
  │     │   │| │スピンドル│ |│ │        │
  │     │   │|  └──────┘  |│ │         │
  │     │   │ \   セクタ  / │ │         │
  │     │   │  ─────────  │ │          │
  │     │   └─────────────┘ │          │
  │     └───────────────────┘          │
  │                                     │
  │     ┌───────────────────────┐      │
  │     │   アーム + ヘッド      │      │
  │     │   ←────────● ヘッド    │      │
  │     └───────────────────────┘      │
  │                                     │
  └─────────────────────────────────────┘

  コンポーネント:
  - プラッタ: 磁性体を塗布したアルミ/ガラス円盤（両面使用）
  - スピンドル: プラッタを回転させるモーター（5400/7200/10000/15000 RPM）
  - ヘッド: 磁気を読み書きする超小型電磁石（プラッタ表面から10nm浮上）
  - アクチュエータアーム: ヘッドを目的のトラックに移動
```

### 1.2 アクセス時間の構成

```
HDDの読み取り時間 = シーク時間 + 回転待ち + 転送時間

  シーク時間（Seek Time）:
    ヘッドを目的のトラックに移動する時間
    平均: 3-10ms（フルストローク: 15-20ms）

  回転待ち（Rotational Latency）:
    目的のセクタがヘッドの下に来るまで待つ時間
    7200RPMの場合: 平均 4.17ms（半回転分）
    計算: 60秒 / 7200回転 / 2 = 4.17ms

  転送時間（Transfer Time）:
    データを実際に読み書きする時間
    100-200 MB/s → 1MBの読み取りに5-10μs

  典型的なランダム読み取り:
    シーク(5ms) + 回転待ち(4ms) + 転送(0.01ms) ≈ 9ms
    → 1秒間に約110回のランダムI/O (110 IOPS)
```

---

## 2. SSD（ソリッドステートドライブ）

### 2.1 NAND フラッシュの仕組み

```
SSD の内部構造:

  ┌─────────────────────────────────────────┐
  │  SSD コントローラ                        │
  │  ┌──────┐ ┌──────┐ ┌──────┐ ┌───────┐ │
  │  │ FTL  │ │ ECC  │ │ WL   │ │ GC    │ │
  │  │      │ │      │ │      │ │       │ │
  │  └──────┘ └──────┘ └──────┘ └───────┘ │
  │                                         │
  │  ┌──────────────────────────────────┐   │
  │  │    NAND フラッシュチップ          │   │
  │  │  ┌────┐ ┌────┐ ┌────┐ ┌────┐   │   │
  │  │  │Die0│ │Die1│ │Die2│ │Die3│   │   │
  │  │  └────┘ └────┘ └────┘ └────┘   │   │
  │  │  各Dieの中:                      │   │
  │  │  ┌─────────────────────────┐    │   │
  │  │  │ Block 0                 │    │   │
  │  │  │ ┌─────┬─────┬─────┐    │    │   │
  │  │  │ │Page0│Page1│Page2│... │    │   │
  │  │  │ └─────┴─────┴─────┘    │    │   │
  │  │  │ Block 1                 │    │   │
  │  │  │ ...                     │    │   │
  │  │  └─────────────────────────┘    │   │
  │  └──────────────────────────────────┘   │
  │                                         │
  │  ┌──────────────┐                       │
  │  │ DRAM キャッシュ│ (マッピングテーブル)  │
  │  └──────────────┘                       │
  └─────────────────────────────────────────┘

  FTL: Flash Translation Layer（論理→物理アドレス変換）
  ECC: Error Correcting Code（エラー訂正）
  WL: Wear Leveling（書き込み均等化）
  GC: Garbage Collection（不要ブロックの回収）
```

### 2.2 SSD特有の制約

| 操作 | 可能な単位 | 速度 |
|------|----------|------|
| 読み取り | ページ単位 (4-16KB) | 〜25μs |
| 書き込み | ページ単位 (4-16KB) | 〜250μs |
| 消去 | **ブロック単位** (256-512ページ) | 〜2ms |

**重要**: SSDは「上書き」ができない。書き込み前に必ずブロック単位で消去が必要。

→ **書き込み増幅（Write Amplification）**: 1ページを更新するために、ブロック全体を読み→消去→書き戻す必要がある。

→ **TRIM**: OSがSSDに「このブロックはもう使わない」と通知し、GCの効率を向上させる。

### 2.3 SSD vs HDD

| 項目 | HDD | SATA SSD | NVMe SSD |
|------|-----|----------|----------|
| 順次読み取り | 100-200 MB/s | 500 MB/s | 3,500-14,000 MB/s |
| 順次書き込み | 100-200 MB/s | 450 MB/s | 3,000-12,000 MB/s |
| ランダムIOPS | 100-200 | 50,000-100,000 | 500,000-2,000,000 |
| レイテンシ | 3-10 ms | 50-100 μs | 10-20 μs |
| 消費電力 | 6-8W | 2-3W | 5-8W |
| 寿命 | 〜5年（機械摩耗） | 3-5年（書込制限） | 3-5年（書込制限） |
| 耐衝撃性 | 低い（ヘッドクラッシュ） | 高い | 高い |
| 価格/TB | 〜$15 | 〜$50 | 〜$60-100 |

---

## 3. NVMe/PCIe

### 3.1 プロトコルスタック

```
ストレージI/Oプロトコルの進化:

  SATA (2003):
    CPU → AHCI → SATA → SSD
    ■ 1つのコマンドキュー、キュー深度32
    ■ 最大帯域: 600 MB/s (SATA III)
    ■ レガシーなHDD向けプロトコルの延長

  NVMe over PCIe (2011):
    CPU → NVMe → PCIe → SSD
    ■ 65,535個のコマンドキュー、各キュー深度65,536
    ■ 最大帯域: 32 GB/s (PCIe 5.0 x4)
    ■ SSD向けに一から設計されたプロトコル
    ■ CPU使用率も低い（割り込み削減）

  比較:
  │ 項目          │ AHCI/SATA  │ NVMe/PCIe    │
  │───────────────│────────────│──────────────│
  │ キュー数      │ 1          │ 65,535       │
  │ キュー深度    │ 32         │ 65,536       │
  │ 帯域幅        │ 600 MB/s   │ 32+ GB/s     │
  │ レイテンシ    │ 〜100 μs   │ 〜10 μs      │
  │ CPU効率       │ 低い       │ 高い          │
```

---

## 4. ファイルシステム

### 4.1 主要ファイルシステム比較

| FS | OS | 最大ファイル | 最大ボリューム | ジャーナリング | COW | 特徴 |
|----|-----|-----------|-------------|-------------|-----|------|
| **ext4** | Linux | 16TB | 1EB | あり | なし | Linux標準、安定 |
| **XFS** | Linux | 8EB | 8EB | あり | なし | 大規模ファイル向け |
| **Btrfs** | Linux | 16EB | 16EB | なし | あり | スナップショット、圧縮 |
| **ZFS** | FreeBSD/Linux | 16EB | 256ZB | なし | あり | データ整合性最強 |
| **NTFS** | Windows | 16EB | 256TB | あり | なし | Windows標準 |
| **APFS** | macOS/iOS | 8EB | — | なし | あり | Apple専用、暗号化 |
| **F2FS** | Android | 3.94TB | 16TB | なし | なし | SSD/eMMC最適化 |

### 4.2 ジャーナリングの仕組み

```
ジャーナリング（ext4の場合）:

  通常の書き込み（ジャーナリングなし）:
    1. メタデータ更新
    2. データ書き込み
    → 途中で電源断 → データ不整合（FSの破損）

  ジャーナリング:
    1. ジャーナル領域に「これから何をするか」を書く
    2. 実際のメタデータ/データを更新
    3. ジャーナルのトランザクションを「完了」にマーク

    電源断が発生した場合:
    → 起動時にジャーナルを読み、未完了のトランザクションを
       ロールバックまたはリプレイ
    → FSの一貫性が保証される

  ┌────────┐    ┌────────┐    ┌────────┐
  │ 1.記録  │───→│ 2.実行  │───→│ 3.完了  │
  │ Journal │    │ Data   │    │ Commit │
  └────────┘    └────────┘    └────────┘
       ↑                            │
       └───── 電源断時はここから再開 ─┘
```

### 4.3 Copy-on-Write（COW）

```
COW（Btrfs, ZFS, APFSの場合）:

  従来のFS:
    データブロック → 直接上書き
    → 書き込み中の電源断でデータ破損

  COW:
    1. 新しいブロックにデータを書く
    2. メタデータを新ブロックを指すように更新
    3. 古いブロックを解放
    → 書き込み中の電源断でも古いデータが残る

  利点:
  - アトミックな書き込み（壊れない）
  - スナップショットが O(1) で作成可能（メタデータのコピーのみ）
  - 圧縮・重複排除が容易

  欠点:
  - フラグメンテーションが発生しやすい
  - ランダム書き込みのオーバーヘッド
```

---

## 5. RAID

### 5.1 RAIDレベル比較

```
RAID 0（ストライピング）:
  ┌──────┐ ┌──────┐
  │Disk 0│ │Disk 1│
  │ A1   │ │ A2   │  ← データを交互に分散
  │ A3   │ │ A4   │
  └──────┘ └──────┘
  性能: 読み書き2倍  冗長性: なし（1台死亡=全損）

RAID 1（ミラーリング）:
  ┌──────┐ ┌──────┐
  │Disk 0│ │Disk 1│
  │ A1   │ │ A1   │  ← 同じデータを複製
  │ A2   │ │ A2   │
  └──────┘ └──────┘
  性能: 読み2倍、書き1倍  冗長性: 1台障害OK  容量: 50%

RAID 5（パリティ分散）:
  ┌──────┐ ┌──────┐ ┌──────┐
  │Disk 0│ │Disk 1│ │Disk 2│
  │ A1   │ │ A2   │ │ Ap   │  ← パリティを分散配置
  │ B1   │ │ Bp   │ │ B2   │
  │ Cp   │ │ C1   │ │ C2   │
  └──────┘ └──────┘ └──────┘
  性能: 読み(N-1)倍、書き遅い  冗長性: 1台障害OK  容量: (N-1)/N

RAID 6（二重パリティ）:
  RAID 5 + パリティ2つ → 2台同時障害OK

RAID 10（1+0: ミラー+ストライプ）:
  ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐
  │Disk 0│ │Disk 1│ │Disk 2│ │Disk 3│
  │ A1   │ │ A1   │ │ A2   │ │ A2   │
  └──────┘ └──────┘ └──────┘ └──────┘
    ミラー対1          ミラー対2
  性能: 読み4倍、書き2倍  冗長性: 各対1台OK  容量: 50%
```

### 5.2 RAID 選択ガイド

| RAID | 容量効率 | 読取性能 | 書込性能 | 耐障害性 | 用途 |
|------|---------|---------|---------|---------|------|
| 0 | 100% | 最高 | 最高 | なし | テンポラリ/キャッシュ |
| 1 | 50% | 良 | 普通 | 1台 | OS、ブート |
| 5 | (N-1)/N | 良 | 遅い | 1台 | ファイルサーバー |
| 6 | (N-2)/N | 良 | 最遅 | 2台 | 大規模ストレージ |
| 10 | 50% | 最高 | 良 | 各対1台 | DB、高パフォーマンス |

---

## 6. I/Oスケジューラ

### 6.1 主要スケジューラ

| スケジューラ | 方式 | 適用 |
|------------|------|------|
| **NOOP/None** | FIFO（先入先出） | SSD（ハードウェアが最適化） |
| **Deadline** | デッドラインベース | DB、リアルタイム |
| **CFQ** | 完全公平キューイング | デスクトップ（旧デフォルト） |
| **BFQ** | Budget Fair Queuing | デスクトップ（低レイテンシ） |
| **mq-deadline** | マルチキュー版 | NVMe SSD |
| **kyber** | 2レベルキュー | 高性能NVMe |

---

## 7. ストレージの未来

| 技術 | 特徴 | 状況 |
|------|------|------|
| **CXL** | CPU-メモリ間の新プロトコル、メモリプーリング | 2024年実用化開始 |
| **Intel Optane** | DRAMとSSDの中間の特性（終了） | 生産終了、技術は他社へ |
| **PLC NAND** | 5ビット/セル、大容量低コスト | 量産開始 |
| **DNA Storage** | 1グラムで215PBのデータを保存 | 研究段階 |
| **ガラスストレージ** | 1000年以上の耐久性 | Microsoft Project Silica |

---

## 8. 実践演習

### 演習1: I/O性能の計算（基礎）

7200RPMのHDDで、以下の操作にかかる時間を計算せよ:
1. ランダムに1つの4KBブロックを読み取る
2. 連続した1GBのファイルを読み取る
3. ランダムに1000個の4KBブロックを読み取る

### 演習2: ストレージ選定（応用）

以下のワークロードに最適なストレージとRAIDレベルを選定し、理由を述べよ:
1. PostgreSQLデータベース（読み取り多、低レイテンシ必須）
2. 動画配信サービスの元データ保管（大容量、シーケンシャル読み取り）
3. Webアプリのログ収集（書き込み多、順次追記）

### 演習3: ベンチマーク（発展）

`fio` または `dd` コマンドを使って自分のマシンのストレージ性能を測定せよ:
- 順次読み取り/書き込みの帯域幅
- ランダム読み取り/書き込みのIOPS
- 測定結果を理論値と比較

---

## FAQ

### Q1: SSDの寿命はどのくらいですか？

**A**: TBW（Total Bytes Written）で表される。一般的なコンシューマSSD:
- 500GB SSD: 〜300 TBW（1日100GB書き込みで約8年）
- 一般的な使用では寿命前にPC自体を買い替える
- サーバー用SSDはさらに高耐久（〜10x PBW）

### Q2: データベースはHDDとSSDどちらに置くべきですか？

**A**: ランダムI/Oが多いDBは**SSDが圧倒的に有利**:
- HDD: 100 IOPS → SSD: 100,000+ IOPS（1000倍）
- 特にインデックス検索、ランダムなJOIN操作で差が顕著
- コールドデータ（アーカイブ）はHDD/S3で十分

### Q3: ZFSとext4の選び方は？

**A**:
- **ext4**: シンプル、安定、Linux標準。一般的なWebサーバーに最適
- **ZFS**: データ整合性が最重要な場面（NAS、バックアップ、DB）。メモリ使用量が多い（1TBあたり1GBのRAM推奨）

---

## まとめ

| 概念 | ポイント |
|------|---------|
| HDD | 機械式、ランダムI/O遅い(〜100 IOPS)、大容量・安価 |
| SSD | NAND、ランダムI/O速い(〜100K IOPS)、書き込み回数制限あり |
| NVMe | PCIe直結、AHCI→NVMeで並列度65,535倍 |
| FS | ジャーナリング(ext4)かCOW(ZFS/Btrfs)で整合性確保 |
| RAID | 0=速度、1=安全、5=バランス、10=性能+安全 |

---

## 次に読むべきガイド

→ [[03-motherboard-and-bus.md]] — マザーボードとバスアーキテクチャ

---

## 参考文献

1. Arpaci-Dusseau, R. H. & Arpaci-Dusseau, A. C. "Operating Systems: Three Easy Pieces." Chapter on Hard Disk Drives and Flash-based SSDs.
2. Cornwell, M. "Anatomy of a Solid-State Drive." ACM Queue, 2012.
3. Agrawal, N. et al. "Design Tradeoffs for SSD Performance." USENIX ATC, 2008.
4. Bonwick, J. & Moore, B. "ZFS: The Last Word in File Systems." Sun Microsystems, 2004.
5. Love, R. "Linux Kernel Development." 3rd Edition, Addison-Wesley, 2010.
