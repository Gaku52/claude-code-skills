# メモリ階層

> 「メモリは速いが小さく、ストレージは大きいが遅い」— この制約がコンピュータアーキテクチャの全てを支配する。

## この章で学ぶこと

- [ ] メモリ階層の各レベルの特性（速度・容量・コスト）を説明できる
- [ ] キャッシュの仕組みと局所性の原理を理解する
- [ ] 仮想メモリとページングの動作を説明できる

## 前提知識

- CPUの基本構造 → 参照: [[00-cpu-architecture.md]]

---

## 1. メモリ階層ピラミッド

### 1.1 全体像

```
メモリ階層ピラミッド:

  速い・高い・小さい
  ▲
  │  ┌───────────┐
  │  │ レジスタ   │ ← 〜1ns, 数KB, CPU内部
  │  │  (Reg)     │
  │  ├───────────┤
  │  │ L1キャッシュ│ ← 〜1-2ns, 64KB×2, CPU内部
  │  │  (L1i/L1d) │    (命令/データ分離)
  │  ├───────────┤
  │  │ L2キャッシュ│ ← 〜3-10ns, 256KB-1MB, コアごと
  │  │  (L2)      │
  │  ├───────────┤
  │  │ L3キャッシュ│ ← 〜10-30ns, 8-64MB, 全コア共有
  │  │  (L3/LLC)  │
  │  ├───────────┤
  │  │ メインメモリ│ ← 〜50-100ns, 8-128GB, DRAM
  │  │  (RAM)     │
  │  ├───────────┤
  │  │ SSD        │ ← 〜10-100μs, 256GB-4TB
  │  │ (NVMe)     │
  │  ├───────────┤
  │  │ HDD        │ ← 〜3-10ms, 1-20TB
  │  │            │
  │  ├───────────┤
  │  │ テープ     │ ← 〜秒-分, PB級, アーカイブ
  │  │ (Tape)     │
  │  └───────────┘
  ▼
  遅い・安い・大きい
```

### 1.2 レイテンシ比較（Jeff Dean の数値 — 2023年版）

| 操作 | レイテンシ | 人間スケール換算 |
|------|----------|----------------|
| L1キャッシュ参照 | 1 ns | 1秒 |
| 分岐予測ミス | 3 ns | 3秒 |
| L2キャッシュ参照 | 4 ns | 4秒 |
| L3キャッシュ参照 | 12 ns | 12秒 |
| ミューテックスロック/アンロック | 17 ns | 17秒 |
| メインメモリ参照 | 100 ns | 1分40秒 |
| 1KB のデータを1Gbpsネットワークで送信 | 10 μs | 2.8時間 |
| SSD からランダム読み出し | 16 μs | 4.4時間 |
| SSD から1MB読み出し | 49 μs | 13.6時間 |
| HDD シーク | 2 ms | 23日 |
| HDD から1MB読み出し | 825 μs | 9.5日 |
| TCP パケット往復（同一DC内） | 500 μs | 5.8日 |
| TCP パケット往復（東京→米西海岸） | 150 ms | 4.8年 |

→ **メインメモリがL1キャッシュの100倍遅い**。HDDに至ってはL1の200万倍遅い。

---

## 2. キャッシュの仕組み

### 2.1 なぜキャッシュが必要か

```
CPUとメモリの速度ギャップ（メモリウォール問題）:

  性能
  │
  │   CPU速度      ／
  │              ／    ← 年50-60%向上（ムーアの法則）
  │            ／
  │          ／        ←── ギャップが拡大
  │        ／
  │      ／    メモリ速度
  │    ／  ─────────────  ← 年7-10%向上
  │  ／
  │／
  └──────────────────── 年

  1980年: CPU 1サイクル = メモリ 1サイクル
  2025年: CPU 1サイクル = メモリ 200-300サイクル
  → キャッシュなしではCPUが99%の時間メモリ待ち
```

### 2.2 キャッシュの方式

```
3つのキャッシュマッピング方式:

1. ダイレクトマップ（Direct-Mapped）:
   メモリアドレスから一意にキャッシュラインが決まる
   ┌────┐
   │ 0  │ ← アドレス 0, 8, 16, 24... がここに入る
   │ 1  │ ← アドレス 1, 9, 17, 25...
   │ 2  │ ← アドレス 2, 10, 18, 26...
   │ ...│
   │ 7  │ ← アドレス 7, 15, 23, 31...
   └────┘
   利点: 高速（インデックス計算が単純）
   欠点: 衝突が多い（同じラインに複数のアドレスがマッピング）

2. フルアソシアティブ（Fully Associative）:
   どのアドレスもどのキャッシュラインにも入れる
   ┌────┐
   │    │ ← 任意のアドレスが入れる
   │    │ ← 任意のアドレスが入れる
   │ ...│
   └────┘
   利点: 衝突なし
   欠点: 全ラインを検索する必要がある（低速/回路が大きい）

3. セットアソシアティブ（Set-Associative）:★現代の標準
   n本のキャッシュラインをセットとしてグループ化
   ┌──────────────────┐
   │ Set 0: [Way0][Way1]│ ← 2-Way: 各セットに2本のライン
   │ Set 1: [Way0][Way1]│
   │ Set 2: [Way0][Way1]│
   │ ...                │
   └──────────────────┘
   利点: ダイレクトマップの速さ + フルアソシアの柔軟性
   現代CPU: L1は8-Way、L2は4-8-Way、L3は16-Way
```

### 2.3 キャッシュライン

```
キャッシュラインの構造（典型: 64バイト）:

  ┌──────┬───────────────────────────────────────────┐
  │ Tag  │              Data (64 bytes)               │
  │      │ byte0 byte1 byte2 ... byte63               │
  └──────┴───────────────────────────────────────────┘

  メモリアドレスの分解:
  ┌──────────┬──────────┬──────────┐
  │   Tag    │  Index   │  Offset  │
  │ (上位)   │ (中間)   │ (下位6bit)│
  └──────────┴──────────┴──────────┘

  64バイトのキャッシュラインは、
  int[16] 配列の16要素を一度にキャッシュにロードできる。
  → 連続アクセスなら1回のキャッシュミスで16要素分のデータが手に入る
  → これが「空間的局所性」を活かす仕組み
```

---

## 3. 局所性の原理

### 3.1 時間的局所性（Temporal Locality）

「最近アクセスしたデータは、近い将来また使われる可能性が高い」

```python
# 時間的局所性の例
total = 0
for i in range(1000000):
    total += data[i]  # 'total' は100万回アクセスされる
                       # → totalはレジスタまたはL1キャッシュに保持される
```

### 3.2 空間的局所性（Spatial Locality）

「あるアドレスにアクセスしたら、近くのアドレスも使われる可能性が高い」

```python
# 空間的局所性の例
for i in range(len(array)):
    process(array[i])  # array[0], array[1], array[2]...は
                       # メモリ上で連続
                       # → 1回のキャッシュラインロードで複数要素をカバー
```

### 3.3 キャッシュミスの種類

| 種類 | 英語名 | 原因 | 対策 |
|------|--------|------|------|
| **義務ミス** | Compulsory (Cold) | 初めてのアクセス | プリフェッチ |
| **容量ミス** | Capacity | キャッシュが小さすぎ | ワーキングセットを小さく |
| **競合ミス** | Conflict | 同じセットに集中 | アソシアティビティ向上 |

---

## 4. 仮想メモリ

### 4.1 概要

仮想メモリは、各プロセスに独立したアドレス空間を提供し、物理メモリを効率的に管理する仕組み。

```
仮想メモリの概念:

  プロセスA の仮想アドレス空間      物理メモリ (RAM)
  ┌──────────────────┐             ┌──────────────┐
  │ 0x0000: コード   │───────────→│ 0x5000       │
  │ 0x1000: ヒープ   │───────────→│ 0x8000       │
  │ 0x2000: (未使用) │             │              │
  │ 0x3000: スタック  │───────────→│ 0xA000       │
  └──────────────────┘             │              │
                                   │              │
  プロセスB の仮想アドレス空間      │              │
  ┌──────────────────┐             │              │
  │ 0x0000: コード   │───────────→│ 0x2000       │
  │ 0x1000: ヒープ   │───────────→│ 0xC000       │
  │ 0x2000: スタック  │───────────→│ 0x1000       │
  └──────────────────┘             └──────────────┘

  ポイント:
  - 両プロセスとも 0x0000 から始まるが、物理的には別の場所
  - プロセス間のメモリは完全に分離（セキュリティ）
  - 物理メモリが不足したらSSD/HDDにスワップアウト
```

### 4.2 ページテーブルとTLB

```
アドレス変換の仕組み:

  仮想アドレス
  ┌──────────────┬────────────┐
  │ ページ番号   │ オフセット │
  │ (VPN)        │            │
  └──────┬───────┴────────────┘
         │
         ▼
  ┌─────────────┐    ミス    ┌────────────────┐
  │    TLB      │ ────────→ │ ページテーブル  │
  │ (Translation│            │ (メインメモリ)  │
  │  Lookaside  │            │                │
  │  Buffer)    │            │ VPN→PPN変換    │
  │ ヒット率:   │            │                │
  │  95-99%     │            └────────┬───────┘
  └──────┬──────┘                     │
         │ ヒット                     │
         ▼                           ▼
  物理アドレス
  ┌──────────────┬────────────┐
  │ 物理ページ番号│ オフセット │
  │ (PPN)        │            │
  └──────────────┴────────────┘

  TLBミスのコスト: 〜100ns（メモリアクセス）
  TLBヒットのコスト: 〜1ns
  → TLBは「ページテーブルのキャッシュ」
```

### 4.3 ページフォルト

```
ページフォルトの処理フロー:

  1. CPUが仮想アドレスにアクセス
  2. TLBミス → ページテーブル参照
  3. ページテーブルに「物理メモリにない」フラグ
  4. ★ ページフォルト例外発生
  5. OSがディスク(SSD/HDD)から該当ページを読み込む
  6. 物理メモリにロード、ページテーブル更新
  7. 命令を再実行

  コスト: 〜1ms (SSD) 〜10ms (HDD)
  → 通常のメモリアクセスの10,000〜100,000倍遅い
  → スラッシング（頻繁なページフォルト）は致命的
```

---

## 5. メモリ帯域とレイテンシ

### 5.1 各レベルの詳細比較

| レベル | レイテンシ | 帯域幅 | 容量 | 1GBあたりコスト |
|--------|----------|--------|------|---------------|
| レジスタ | 0.3ns (1 cycle) | ∞ (CPU内部) | 〜1KB | — |
| L1 Cache | 1ns (3-4 cycles) | 〜1TB/s | 64KB×2 | — |
| L2 Cache | 3-10ns | 〜500GB/s | 256KB-1MB | — |
| L3 Cache | 10-30ns | 〜200GB/s | 8-64MB | — |
| DRAM (DDR5) | 50-100ns | 50-100GB/s | 8-128GB | 〜$3 |
| NVMe SSD | 10-100μs | 3-14GB/s | 256GB-8TB | 〜$0.08 |
| SATA SSD | 50-100μs | 500MB/s | 256GB-4TB | 〜$0.05 |
| HDD | 3-10ms | 100-200MB/s | 1-20TB | 〜$0.02 |
| テープ | 秒-分 | 300MB/s | PB級 | 〜$0.005 |

### 5.2 Back-of-the-envelope計算

```python
# メモリ帯域の実際の測定例

import time
import array

# 1GBの配列を作成
size = 250_000_000  # 250M × 4bytes = 1GB
data = array.array('i', range(size))

# 順次アクセス（空間的局所性あり）
start = time.time()
total = 0
for i in range(size):
    total += data[i]
elapsed = time.time() - start

print(f"Sequential: {elapsed:.2f}s, Bandwidth: {size * 4 / elapsed / 1e9:.1f} GB/s")
# 典型的な結果: 8-15 GB/s（メモリ帯域に近い）

# ランダムアクセス（局所性なし）
import random
indices = list(range(size))
random.shuffle(indices)

start = time.time()
total = 0
for i in indices[:1_000_000]:  # 100万回のランダムアクセス
    total += data[i]
elapsed = time.time() - start

print(f"Random: {elapsed:.2f}s")
# 典型的な結果: 順次アクセスの10-100倍遅い
```

---

## 6. キャッシュフレンドリーなプログラミング

### 6.1 構造体配列 vs 配列構造体

```c
// AoS (Array of Structures) — キャッシュ非効率な場合がある
struct Particle {
    float x, y, z;        // 位置（よく使う）
    float vx, vy, vz;     // 速度（よく使う）
    char name[32];         // 名前（たまにしか使わない）
    int id;                // ID（たまにしか使わない）
};
struct Particle particles[10000];
// 全位置を処理する場合、nameやidもキャッシュラインに乗り、無駄

// SoA (Structure of Arrays) — キャッシュ効率的
struct ParticleSystem {
    float x[10000], y[10000], z[10000];        // 位置
    float vx[10000], vy[10000], vz[10000];     // 速度
    char names[10000][32];                       // 名前
    int ids[10000];                              // ID
};
// 位置だけを処理する場合、x[], y[], z[]が密にキャッシュに乗る
// → ゲームエンジン、物理シミュレーション、GPUプログラミングで重要
```

### 6.2 ループのブロッキング（タイリング）

```c
// 行列乗算のキャッシュ最適化

// ❌ 単純な実装（キャッシュミス多発）
void matmul_naive(int N, float *A, float *B, float *C) {
    for (int i = 0; i < N; i++)
        for (int j = 0; j < N; j++)
            for (int k = 0; k < N; k++)
                C[i*N+j] += A[i*N+k] * B[k*N+j];
    // B[k*N+j] がkのループで列方向にアクセス → キャッシュミス
}

// ✅ ブロッキング（タイリング）: キャッシュに収まる小ブロック単位で計算
void matmul_blocked(int N, float *A, float *B, float *C) {
    int BLOCK = 64;  // L1キャッシュに収まるサイズ
    for (int ii = 0; ii < N; ii += BLOCK)
        for (int jj = 0; jj < N; jj += BLOCK)
            for (int kk = 0; kk < N; kk += BLOCK)
                for (int i = ii; i < ii+BLOCK && i < N; i++)
                    for (int j = jj; j < jj+BLOCK && j < N; j++)
                        for (int k = kk; k < kk+BLOCK && k < N; k++)
                            C[i*N+j] += A[i*N+k] * B[k*N+j];
    // 小ブロック内ではデータがキャッシュに収まり、ミスが激減
    // N=1024 で 3-5倍高速化
}
```

---

## 7. NUMA（Non-Uniform Memory Access）

```
NUMAアーキテクチャ（マルチソケットサーバー）:

  ┌─────────────────────┐    ┌─────────────────────┐
  │     Socket 0        │    │     Socket 1        │
  │  ┌──────┐ ┌──────┐ │    │ ┌──────┐ ┌──────┐  │
  │  │Core0 │ │Core1 │ │    │ │Core4 │ │Core5 │  │
  │  └──────┘ └──────┘ │    │ └──────┘ └──────┘  │
  │  ┌──────┐ ┌──────┐ │    │ ┌──────┐ ┌──────┐  │
  │  │Core2 │ │Core3 │ │    │ │Core6 │ │Core7 │  │
  │  └──────┘ └──────┘ │    │ └──────┘ └──────┘  │
  │                     │    │                     │
  │  ┌────────────────┐ │    │ ┌────────────────┐  │
  │  │ Local Memory   │ │    │ │ Local Memory   │  │
  │  │ 64GB (速い)    │ │    │ │ 64GB (速い)    │  │
  │  └────────┬───────┘ │    │ └────────┬───────┘  │
  └───────────┼─────────┘    └──────────┼──────────┘
              │    QPI/UPI リンク       │
              └────────────────────────┘
                  (遅い: ローカルの1.5-2倍)

  Core0がSocket0のメモリにアクセス: 〜80ns（ローカル）
  Core0がSocket1のメモリにアクセス: 〜120-160ns（リモート）
  → メモリ配置を意識しないと性能が50%以上低下
```

---

## 8. 実践演習

### 演習1: レイテンシの直感（基礎）

Jeff Deanの数値を使い、以下の処理の概算時間を計算せよ:
1. 1000要素の配列をL1キャッシュ内で線形探索
2. 100万要素のソート済み配列を二分探索（L3キャッシュに収まる場合）
3. SSDから100MBのファイルを読み出し

### 演習2: キャッシュ効率の測定（応用）

好きな言語で以下を実装し、実行時間を比較せよ:
1. N×N行列の行優先合計 vs 列優先合計（N=5000, 10000）
2. 連続メモリの配列 vs 連結リストの走査速度

### 演習3: メモリ使用量の分析（発展）

自分が開発しているアプリケーションについて:
1. ワーキングセットサイズを推定
2. L3キャッシュに収まるか判定
3. ページフォルトが発生しうる場面を特定

---

## FAQ

### Q1: GC（ガベージコレクション）はキャッシュにどう影響しますか？

**A**: GCはキャッシュの大敵になりうる:
- **マーク&スイープ**: 全オブジェクトを走査 → キャッシュ内容が全て入れ替わる
- **世代別GC**: 若い世代のみ回収すればキャッシュへの影響は限定的
- **コンパクション**: オブジェクトを移動させると参照の局所性が向上する

Java の G1GC や ZGC はこれらのトレードオフを考慮して設計されている。

### Q2: 仮想メモリはパフォーマンスに悪影響ですか？

**A**: TLBが十分に大きく、ワーキングセットが物理メモリに収まっていればオーバーヘッドは最小限。問題になるのは:
- **Huge Pages未使用時**: 4KBページだとTLBエントリが不足しやすい → 2MBのHuge Pages使用で改善
- **スラッシング**: 物理メモリ不足でページフォルトが頻発 → メモリ増設が唯一の解

### Q3: Apple Silicon の統合メモリはなぜ速いのですか？

**A**: 従来のPCではCPUとGPUが別チップで、データ転送にPCIeバス（〜32GB/s）を使う。Apple Siliconでは:
- CPU/GPU/NPUが同じメモリを直接参照（コピー不要）
- LPDDR5の帯域を全コンポーネントが共有（〜400GB/s）
- CPUの計算結果をGPUがゼロコピーで利用可能
- 特にAI推論（LLM等）で効果的（モデル全体をメモリに保持）

---

## まとめ

| 概念 | ポイント |
|------|---------|
| メモリ階層 | レジスタ→L1→L2→L3→RAM→SSD→HDD（速度と容量のトレードオフ） |
| キャッシュ | セットアソシアティブ方式が現代の標準 |
| 局所性の原理 | 時間的（最近使ったデータ）+ 空間的（近くのデータ） |
| 仮想メモリ | プロセス分離 + 物理メモリの効率的管理 |
| TLB | ページテーブルのキャッシュ（ヒット率95-99%） |
| プログラマーの責務 | キャッシュフレンドリーなデータ構造とアクセスパターン |

---

## 次に読むべきガイド

→ [[02-storage-systems.md]] — ストレージシステムの仕組み

---

## 参考文献

1. Bryant, R. E. & O'Hallaron, D. R. "Computer Systems: A Programmer's Perspective." 3rd Edition, Pearson, 2015.
2. Drepper, U. "What Every Programmer Should Know About Memory." 2007. https://people.freebsd.org/~lstewart/articles/cpumemory.pdf
3. Dean, J. "Numbers Everyone Should Know." (Various presentations at Google)
4. Hennessy, J. L. & Patterson, D. A. "Computer Architecture: A Quantitative Approach." 6th Edition, 2017.
5. Intel. "Intel 64 and IA-32 Architectures Optimization Reference Manual."
