# GPUと並列計算

> GPUは「一つの命令を数千のコアで同時実行する」— CPU的な逐次処理とは根本的に異なるアプローチである。

## この章で学ぶこと

- [ ] GPUの内部アーキテクチャとCPUとの違いを説明できる
- [ ] GPGPU（汎用GPU計算）の仕組みを理解する
- [ ] AI/MLにおけるGPUの役割を説明できる

## 前提知識

- CPUアーキテクチャの基礎 → 参照: [[00-cpu-architecture.md]]

---

## 1. GPU vs CPU

### 1.1 設計思想の違い

```
CPU: レイテンシ最適化（1タスクを最速で）
  ┌──────────────────────────────────────┐
  │ ┌────────────────────┐ ┌──────────┐ │
  │ │ 大きなキャッシュ    │ │ 分岐予測 │ │  ← 複雑な制御ロジック
  │ │ (L1: 64KB×2)       │ │ 器       │ │  ← 少数の強力なコア
  │ │ (L2: 1MB)          │ └──────────┘ │
  │ │ (L3: 32MB)         │              │
  │ └────────────────────┘              │
  │ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐│
  │ │Core 0│ │Core 1│ │Core 2│ │Core 3││  ← 4-16コア
  │ │(超強力)│ │(超強力)│ │(超強力)│ │(超強力)│
  │ └──────┘ └──────┘ └──────┘ └──────┘│
  └──────────────────────────────────────┘

GPU: スループット最適化（大量タスクを並列で）
  ┌──────────────────────────────────────┐
  │ ┌────┐                   小キャッシュ │
  │ │制御│                               │  ← 単純な制御ロジック
  │ └────┘                               │  ← 数千の小さなコア
  │ ┌─┐┌─┐┌─┐┌─┐┌─┐┌─┐┌─┐┌─┐┌─┐┌─┐  │
  │ │c││c││c││c││c││c││c││c││c││c│  │  ← SM(ストリーミング
  │ └─┘└─┘└─┘└─┘└─┘└─┘└─┘└─┘└─┘└─┘  │    マルチプロセッサ)内
  │ ┌─┐┌─┐┌─┐┌─┐┌─┐┌─┐┌─┐┌─┐┌─┐┌─┐  │    に32-128コア
  │ │c││c││c││c││c││c││c││c││c││c│  │
  │ └─┘└─┘└─┘└─┘└─┘└─┘└─┘└─┘└─┘└─┘  │  ← SMが数十〜百個
  │ ... (数千コア)                       │
  └──────────────────────────────────────┘
```

### 1.2 数値比較

| 指標 | CPU (Ryzen 9 7950X) | GPU (RTX 4090) |
|------|---------------------|----------------|
| コア数 | 16 | 16,384 (CUDA) |
| クロック | 5.7 GHz | 2.5 GHz |
| FP32性能 | 〜1.5 TFLOPS | 82.6 TFLOPS |
| メモリ帯域 | 83 GB/s (DDR5) | 1,008 GB/s (GDDR6X) |
| TDP | 170W | 450W |
| 得意分野 | 逐次処理、分岐多い処理 | 大量データの並列処理 |
| 苦手分野 | 大規模並列 | 分岐多い処理、逐次処理 |

---

## 2. GPUアーキテクチャ

### 2.1 NVIDIA GPU の構造

```
NVIDIA GPU（Ada Lovelace アーキテクチャ）:

  ┌─────────────────────────────────────────────┐
  │                  GPU チップ                   │
  │                                               │
  │  ┌─────────────────────────────────────────┐ │
  │  │  GPC (Graphics Processing Cluster) × 12 │ │
  │  │  ┌──────────────────────────────────┐   │ │
  │  │  │  TPC (Texture Processing Cluster) │   │ │
  │  │  │  ┌────────────────────────────┐  │   │ │
  │  │  │  │  SM (Streaming Multiprocessor)│ │   │ │
  │  │  │  │  ┌─────────────────────────┐│ │   │ │
  │  │  │  │  │ CUDA Core × 128        ││ │   │ │
  │  │  │  │  │ Tensor Core × 4        ││ │   │ │
  │  │  │  │  │ RT Core × 1            ││ │   │ │
  │  │  │  │  │ 共有メモリ: 128KB      ││ │   │ │
  │  │  │  │  │ レジスタファイル: 256KB ││ │   │ │
  │  │  │  │  │ L1 Cache: 128KB        ││ │   │ │
  │  │  │  │  └─────────────────────────┘│ │   │ │
  │  │  │  └────────────────────────────┘  │   │ │
  │  │  └──────────────────────────────────┘   │ │
  │  └─────────────────────────────────────────┘ │
  │                                               │
  │  ┌─────────────────┐  ┌────────────────────┐│
  │  │  L2 Cache: 72MB  │  │ メモリコントローラ  ││
  │  └─────────────────┘  │ GDDR6X 24GB       ││
  │                        │ 384-bit バス      ││
  │                        └────────────────────┘│
  └─────────────────────────────────────────────┘

  RTX 4090: 128SM × 128 CUDA Core = 16,384 CUDA Core
```

### 2.2 ワープ（Warp）

```
NVIDIA GPU のスレッド階層:

  Grid（全体）
  ├── Block 0
  │   ├── Warp 0: Thread 0-31   ← 32スレッドが完全同期実行
  │   ├── Warp 1: Thread 32-63
  │   └── ...
  ├── Block 1
  │   ├── Warp 0: Thread 0-31
  │   └── ...
  └── ...

  ワープ内の全スレッドは同じ命令を同時実行（SIMT: Single Instruction, Multiple Threads）

  → 分岐があると「ワープダイバージェンス」が発生:
  if (threadIdx.x < 16) {
      // 前半16スレッドがここを実行
      // 後半16スレッドは待機（無駄）
  } else {
      // 後半16スレッドがここを実行
      // 前半16スレッドは待機（無駄）
  }
  → GPU で分岐の多いコードが遅い理由
```

---

## 3. CUDAプログラミング

### 3.1 基本構造

```cuda
// CUDA: ベクトル加算の例
// C = A + B (各要素を並列に計算)

// GPU上で実行されるカーネル関数
__global__ void vectorAdd(float *A, float *B, float *C, int N) {
    // 各スレッドが1要素を担当
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        C[idx] = A[idx] + B[idx];  // 超シンプル: 1スレッド1加算
    }
}

int main() {
    int N = 1000000;  // 100万要素
    float *d_A, *d_B, *d_C;

    // GPUメモリ確保
    cudaMalloc(&d_A, N * sizeof(float));
    cudaMalloc(&d_B, N * sizeof(float));
    cudaMalloc(&d_C, N * sizeof(float));

    // CPU→GPUにデータ転送
    cudaMemcpy(d_A, h_A, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, N * sizeof(float), cudaMemcpyHostToDevice);

    // カーネル起動: 3907ブロック × 256スレッド = 〜100万スレッド
    int blockSize = 256;
    int numBlocks = (N + blockSize - 1) / blockSize;
    vectorAdd<<<numBlocks, blockSize>>>(d_A, d_B, d_C, N);

    // GPU→CPUにデータ転送
    cudaMemcpy(h_C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);

    // GPUメモリ解放
    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
    return 0;
}

// CPU: 100万回のループ → 〜1ms
// GPU: 100万スレッド並列 → 〜0.01ms + メモリ転送コスト
```

### 3.2 GPU計算が有利な条件

```
GPU が有利:
  ■ データ並列性が高い（同じ処理を大量データに適用）
  ■ 演算密度が高い（メモリアクセスより計算が多い）
  ■ 分岐が少ない（条件分岐でスレッドが分散しない）

  例: 行列乗算、画像処理、物理シミュレーション、AI学習

GPU が不利:
  □ 逐次処理（前の結果に依存する連続計算）
  □ 分岐が多い（if/else が複雑）
  □ データが小さい（並列化のオーバーヘッドの方が大きい）
  □ メモリアクセスがランダム（合体メモリアクセスが効かない）

  例: ファイル処理、テキスト解析、ツリー探索
```

---

## 4. GPU世代の進化

| 世代 | 年 | アーキテクチャ | CUDA Core | 特徴 |
|------|-----|-------------|-----------|------|
| Tesla | 2006 | G80 | 128 | CUDA登場 |
| Fermi | 2010 | GF100 | 512 | ECC、L1/L2キャッシュ |
| Kepler | 2012 | GK110 | 2,880 | Dynamic Parallelism |
| Maxwell | 2014 | GM200 | 3,072 | 電力効率2倍 |
| Pascal | 2016 | GP100 | 3,840 | FP16、NVLink |
| Volta | 2017 | GV100 | 5,120 | **Tensor Core登場** |
| Turing | 2018 | TU102 | 4,608 | RT Core（レイトレ） |
| Ampere | 2020 | GA102 | 10,752 | TF32、構造化スパース |
| Hopper | 2022 | GH100 | 16,896 | Transformer Engine |
| Ada | 2022 | AD102 | 16,384 | DLSS 3 |
| Blackwell | 2024 | GB202 | 〜21,760 | FP4、第5世代Tensor |

---

## 5. AI学習エンジンとしてのGPU

### 5.1 Tensor Core

```
通常のCUDA Core: スカラ演算（1クロックで1演算）
  a = b × c + d  → 1クロック

Tensor Core: 行列演算（1クロックで行列乗算）
  ┌─────┐   ┌─────┐   ┌─────┐
  │ 4×4 │ × │ 4×4 │ = │ 4×4 │  → 1クロック
  │行列A│   │行列B│   │行列C│
  └─────┘   └─────┘   └─────┘
  = 64回のFMA（積和演算）を1クロックで実行

  → AI学習の本質は巨大な行列乗算
  → Tensor Core がAI学習を10-100倍高速化
```

### 5.2 精度の使い分け

| 精度 | ビット数 | 用途 | Tensor Core性能 |
|------|---------|------|----------------|
| FP64 | 64 | 科学計算 | 基準 |
| FP32 | 32 | 一般的な計算 | 2倍 |
| TF32 | 19 | AI学習 | 8倍 |
| FP16 | 16 | AI学習/推論 | 16倍 |
| BF16 | 16 | AI学習（範囲重視） | 16倍 |
| INT8 | 8 | AI推論 | 32倍 |
| FP8 | 8 | AI学習（Hopper〜） | 64倍 |
| FP4 | 4 | AI推論（Blackwell〜） | 128倍 |

→ 精度を下げるほど高速。AI学習では混合精度（Mixed Precision）で品質を維持しつつ高速化。

---

## 6. GPU以外のアクセラレータ

| デバイス | 開発元 | 特徴 | 用途 |
|---------|--------|------|------|
| **TPU** | Google | 行列演算専用、大規模学習 | Google内部AI |
| **NPU** | Apple/Qualcomm | 低消費電力AI推論 | スマホ/PC |
| **FPGA** | Intel/AMD | プログラマブル回路 | カスタム処理 |
| **IPU** | Graphcore | グラフ構造に最適化 | GNN、AI学習 |

---

## 7. 並列計算の理論

### 7.1 アムダールの法則

```
アムダールの法則:

  高速化 = 1 / ((1-P) + P/N)

  P = 並列化可能な割合
  N = プロセッサ数

  例: P=0.95（95%が並列化可能）、N=1024コア
  高速化 = 1 / (0.05 + 0.95/1024) = 1 / 0.0509 ≈ 19.6倍

  ★ 95%が並列化可能でも、最大19.6倍にしかならない
  ★ 5%の逐次部分がボトルネック

  並列化率と最大高速化:
  P=50%  → 最大 2倍   （コアを増やしても意味なし）
  P=90%  → 最大 10倍
  P=95%  → 最大 20倍
  P=99%  → 最大 100倍
  P=99.9%→ 最大 1000倍 ← AI学習はこの領域
```

### 7.2 グスタフソンの法則

```
グスタフソンの法則:

  アムダールの法則の「問題サイズ固定」を修正。
  「コアが増えれば、より大きな問題を同じ時間で解ける」

  高速化 = N - (1-P) × (N-1)

  → 問題サイズをスケールすれば、並列化の効果はほぼ線形
  → これがGPU計算の真の価値:
     「同じ時間で、より大きなモデルを学習できる」
```

---

## 8. 実践演習

### 演習1: CPU vs GPU の判定（基礎）

以下のタスクについて、CPUとGPUのどちらが適しているか判定し理由を述べよ:
1. 100万枚の画像に同じフィルタを適用
2. JSONファイルの解析と変換
3. ニューラルネットワークの学習
4. Gitの差分計算
5. 動画のエンコード

### 演習2: アムダールの法則（応用）

あるプログラムの実行時間の内訳が以下の場合:
- 入力処理: 5% （逐次）
- メイン計算: 80% （並列化可能）
- 結果集約: 10% （部分的に並列化可能、50%）
- 出力処理: 5% （逐次）

16コアCPUと1024コアGPUを使った場合の最大高速化率を計算せよ。

### 演習3: GPU活用（発展）

PyTorchを使って、CPU vs GPUの行列乗算速度を比較するベンチマークを作成せよ:
```python
import torch
import time

# 行列サイズを変えて測定: 100, 500, 1000, 5000, 10000
for N in [100, 500, 1000, 5000, 10000]:
    # CPU
    a_cpu = torch.randn(N, N)
    b_cpu = torch.randn(N, N)
    start = time.time()
    c_cpu = torch.mm(a_cpu, b_cpu)
    cpu_time = time.time() - start

    # GPU (CUDA or MPS)
    # ... 実装して比較
```

---

## FAQ

### Q1: ゲーム用GPUとAI学習用GPUの違いは？

**A**: 同じGPUチップでも、用途によってメモリとTensor Coreが異なる:
- **ゲーム用（RTX 4090）**: VRAM 24GB、RT Core重視、消費者向け
- **AI学習用（A100/H100）**: VRAM 80GB、Tensor Core重視、ECC付き
- **AI推論用（L4/L40S）**: 低消費電力、INT8/FP8最適化

### Q2: Apple Silicon のGPUはNVIDIAに勝てますか？

**A**: 用途による:
- **メモリ帯域**: M4 Max (546GB/s) vs RTX 4090 (1,008GB/s) → NVIDIA優位
- **統合メモリ**: M4 Maxの128GBをGPUが直接使える → 大規模LLM推論で有利
- **電力効率**: Apple Siliconが圧倒的に優れる
- **CUDA エコシステム**: AI/MLライブラリの大半がCUDA前提 → NVIDIA優位

### Q3: GPUプログラミングを学ぶべきですか？

**A**: AI/ML、ゲーム開発、HPC（高性能計算）に関わるなら必須。Web開発者は直接は不要だが、「なぜAI学習にGPUが必要か」を理解することは重要。PyTorch/TensorFlowはGPUの詳細を抽象化してくれるため、CUDAを直接書く必要は少ない。

---

## まとめ

| 概念 | ポイント |
|------|---------|
| CPU vs GPU | レイテンシ最適化 vs スループット最適化 |
| CUDA Core | 汎用並列計算。数千〜数万コア |
| Tensor Core | 行列演算専用。AI学習を10-100倍高速化 |
| ワープ | 32スレッドの同期実行単位。分岐でダイバージェンス |
| アムダールの法則 | 逐次部分が並列化の限界を決める |
| 精度 | FP32→FP16→FP8と下げるほど高速（品質トレードオフ） |

---

## 次に読むべきガイド

→ [[05-io-systems.md]] — I/Oシステムと割り込み

---

## 参考文献

1. Kirk, D. B. & Hwu, W. W. "Programming Massively Parallel Processors." 4th Edition, Morgan Kaufmann, 2022.
2. NVIDIA. "CUDA C++ Programming Guide." https://docs.nvidia.com/cuda/
3. Hennessy, J. L. & Patterson, D. A. "Computer Architecture: A Quantitative Approach." 6th Edition.
4. NVIDIA. "NVIDIA H100 Tensor Core GPU Architecture." Whitepaper, 2022.
5. Amdahl, G. M. "Validity of the single processor approach to achieving large scale computing capabilities." AFIPS, 1967.
